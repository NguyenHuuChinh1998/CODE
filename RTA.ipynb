{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import date, timedelta\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import polars as pl\n",
    "import fastexcel\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data(folder_path, sheet_name=None):\n",
    "    file_paths = glob.glob(f\"{folder_path}/*.xlsx\") + glob.glob(f\"{folder_path}/*.csv\")\n",
    "    df_list = []\n",
    "\n",
    "    for file in file_paths:\n",
    "        if file.endswith('.xlsx'):\n",
    "            df = pl.read_excel(file, sheet_name=sheet_name)\n",
    "        elif file.endswith('.csv'):\n",
    "            try:\n",
    "                df = pl.read_csv(file, encoding=\"utf-8\")\n",
    "            except:\n",
    "                df = pl.read_csv(file, encoding=\"ISO-8859-1\", ignore_errors=True)\n",
    "        \n",
    "        df = df.with_columns([\n",
    "            pl.col(col).cast(pl.String) \n",
    "            for col in df.columns\n",
    "        ])\n",
    "        \n",
    "        df_list.append(df)\n",
    "    \n",
    "    merged_df = pl.concat(df_list, how='vertical')\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "today_temp = datetime.today().date()\n",
    "today = today_temp.strftime('%b_%d_%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_glob_1 = \"C:/Users/huuchinh.nguyen\"\n",
    "first_glob_2 = \"C:/Users/ADMIN\"\n",
    "\n",
    "if os.path.exists(first_glob_1):\n",
    "    first_glob = first_glob_1\n",
    "elif os.path.exists(first_glob_2):\n",
    "    first_glob = first_glob_2\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Neither {first_glob_1} nor {first_glob_2} exists.\")\n",
    "\n",
    "folder_paths = {\n",
    "    # \"input_agent_activity\":f'{first_glob}/Concentrix Corporation/WFM-Expedia-HCM - Branding files/Rawdata/STORAGE_INPUT_AGENT_ACTIVITY/2025',\n",
    "    \"input_agent_activity\":f'{first_glob}/Concentrix Corporation/WFM-Expedia-HCM - Branding files/Rawdata/INPUT_AGENT_ACTIVITY_FOR_REPORT',\n",
    "    \"input_iex\":f'{first_glob}/Concentrix Corporation/WFM-Expedia-HCM - Branding files/Rawdata/OUTPUT_AGENT_IEX_FOR_REPORT',\n",
    "    \"input_pull_out\":f'{first_glob}/Concentrix Corporation/WFM-Expedia-HCM - Branding files/Save/EXP Leave Management.xlsx',\n",
    "    \"input_hc_master\":f'{first_glob}/Concentrix Corporation/WFM-Expedia-HCM - Branding files/Headcount/HC Master Database - 2025.xlsx',\n",
    "    \"input_ramco_code\":f'{first_glob}/Concentrix Corporation/WFM-Expedia-HCM - Branding files/Rawdata/INPUT_RAMCO_CODE',\n",
    "    \"input_ramco_ot\":f'{first_glob}/Concentrix Corporation/WFM-Expedia-HCM - Branding files/Rawdata/INPUT_OT_RAMCO_CODE',\n",
    "    \"rta_output\":f'{first_glob}/Concentrix Corporation/WFM-Expedia-HCM - Branding files/Rawdata/STORAGE_OUTPUT_RTA',\n",
    "    \"rta_intervals_output\":f'{first_glob}/Concentrix Corporation/WFM-Expedia-HCM - Branding files/Rawdata/OUTPUT_AGENT_ACTIVITY_INTERVALS',\n",
    "    \"ramco_output\":f'{first_glob}/Concentrix Corporation/WFM-Expedia-HCM - Branding files/Rawdata',\n",
    "    \"hc_extend_by_month\":f'{first_glob}/Concentrix Corporation/WFM-Expedia-HCM - Branding files/Headcount/HC Extend by Month'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huuchinh.nguyen\\AppData\\Local\\Temp\\ipykernel_27960\\3254034840.py:14: ChronoFormatWarning: Detected the pattern `.%f` in the chrono format string. This pattern should not be used to parse values after a decimal point. Use `%.f` instead. See the full specification: https://docs.rs/chrono/latest/chrono/format/strftime\n",
      "  pl.col(['Datetime_Fluctuate_Start_Shift','Datetime_Fluctuate_End_Shift','Datetime_First_Start_Shift','Datetime_First_End_Shift']).str.to_datetime(\"%Y-%m-%d %H:%M:%S.%f\", strict=False),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (8_238, 5)\n",
      "┌────────────┬────────────────────────────────┬─────────────┬───────────────┬──────────────────────┐\n",
      "│ Date       ┆ Email Id                       ┆ Night_Shift ┆ Previous Date ┆ Previous_Night_Shift │\n",
      "│ ---        ┆ ---                            ┆ ---         ┆ ---           ┆ ---                  │\n",
      "│ date       ┆ str                            ┆ f64         ┆ date          ┆ f64                  │\n",
      "╞════════════╪════════════════════════════════╪═════════════╪═══════════════╪══════════════════════╡\n",
      "│ 2025-05-23 ┆ caotan.nguyen@concentrix.com   ┆ 1.0         ┆ 2025-05-22    ┆ 1.0                  │\n",
      "│ 2025-05-07 ┆ thituongvy.nguyen1@concentrix. ┆ 1.0         ┆ 2025-05-06    ┆ 1.0                  │\n",
      "│            ┆ …                              ┆             ┆               ┆                      │\n",
      "│ 2025-05-09 ┆ hatuuyen.nguyen@concentrix.com ┆ 0.0         ┆ 2025-05-08    ┆ 0.0                  │\n",
      "│ 2025-06-11 ┆ laikimthu.nguyen1@concentrix.c ┆ 0.0         ┆ 2025-06-10    ┆ 0.0                  │\n",
      "│            ┆ …                              ┆             ┆               ┆                      │\n",
      "│ 2025-06-05 ┆ maihanhnhi.tran@concentrix.com ┆ 1.0         ┆ 2025-06-04    ┆ 1.0                  │\n",
      "│ …          ┆ …                              ┆ …           ┆ …             ┆ …                    │\n",
      "│ 2025-05-18 ┆ ngocson.nguyen1@concentrix.com ┆ 0.0         ┆ 2025-05-17    ┆ 0.0                  │\n",
      "│ 2025-05-29 ┆ hatuuyen.nguyen@concentrix.com ┆ 0.0         ┆ 2025-05-28    ┆ 0.0                  │\n",
      "│ 2025-05-22 ┆ thanhbo.do@concentrix.com      ┆ 0.0         ┆ 2025-05-21    ┆ 1.0                  │\n",
      "│ 2025-05-18 ┆ quangthuan.huynh@concentrix.co ┆ 0.0         ┆ 2025-05-17    ┆ 0.0                  │\n",
      "│            ┆ …                              ┆             ┆               ┆                      │\n",
      "│ 2025-05-23 ┆ yennhi.bach@concentrix.com     ┆ 0.0         ┆ 2025-05-22    ┆ 0.0                  │\n",
      "└────────────┴────────────────────────────────┴─────────────┴───────────────┴──────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "AGENT_ACTIVITY_INPUT = input_data(folder_paths[\"input_agent_activity\"])\n",
    "AGENT_ACTIVITY_INPUT = AGENT_ACTIVITY_INPUT[['Agent Email', 'Supervisor Email','Agent State','Number of Active Contacts','Productive Aux Flag (Yes / No)','Start Time','End Time','Total Time in seconds','Agent Business Location']]\n",
    "AGENT_ACTIVITY_INPUT = AGENT_ACTIVITY_INPUT.unique()\n",
    "remove_agent_state = [\"End of Shift-IDLE\", \"Log out-IDLE\",\"ENDOFSHIFT-IDLE\"]\n",
    "AGENT_ACTIVITY_INPUT = AGENT_ACTIVITY_INPUT.filter((pl.col('End Time').is_not_null()) & (~pl.col('Agent State').is_in(remove_agent_state)))\n",
    "AGENT_ACTIVITY_INPUT = AGENT_ACTIVITY_INPUT.with_columns(pl.col('Total Time in seconds').str.replace(',', '').cast(pl.Float64).alias('Total Time in seconds'))\n",
    "AGENT_ACTIVITY_INPUT = AGENT_ACTIVITY_INPUT.filter((pl.col('Total Time in seconds') < 3600*10))\n",
    "# AGENT_ACTIVITY_INPUT = AGENT_ACTIVITY_INPUT.filter(~pl.col(\"Agent State\").str.contains(\"Offline-IDLE\"))\n",
    "\n",
    "IEX = input_data(folder_paths[\"input_iex\"])\n",
    "IEX = IEX.unique()\n",
    "IEX = IEX.with_columns([\n",
    "    pl.col(['Date']).str.to_date(\"%Y-%m-%d\", strict=False),\n",
    "    pl.col(['Datetime_Fluctuate_Start_Shift','Datetime_Fluctuate_End_Shift','Datetime_First_Start_Shift','Datetime_First_End_Shift']).str.to_datetime(\"%Y-%m-%d %H:%M:%S.%f\", strict=False),\n",
    "    pl.col(['Night_Shift','Target','Unplanned','Planned','Roster Presented','Roster Scheduled']).cast(pl.Float64)])\n",
    "columns_to_sec = ['Time_Of_Day','Open Time', 'Extra Time', 'Break Time', 'Lunch Time', 'Training', 'NCNS','AL','Target']\n",
    "IEX = IEX.with_columns([(pl.col(col).fill_null(0).cast(pl.Float64) * 3600).alias(col) for col in columns_to_sec])\n",
    "Night_Shift_1 = IEX[['Date','Email Id','Night_Shift']].unique()\n",
    "Night_Shift_2 = Night_Shift_1.with_columns((pl.col('Date') - pl.duration(days=1)).alias('Previous Date'))\n",
    "Night_Shift = Night_Shift_2.join(Night_Shift_1,left_on = ['Previous Date','Email Id'], right_on = ['Date','Email Id'], how = 'left')\n",
    "Night_Shift = Night_Shift.rename({'Night_Shift_right':'Previous_Night_Shift'})\n",
    "print(Night_Shift)\n",
    "Previous_Date = IEX[['Date','Email Id','Datetime_First_Start_Shift','Shift Tracking']].unique()\n",
    "\n",
    "PULL_OUT_DATA = pl.read_excel(folder_paths[\"input_pull_out\"], sheet_name='Pull Out')\n",
    "PULL_OUT_DATA = PULL_OUT_DATA.unique()\n",
    "PULL_OUT_DATA = PULL_OUT_DATA.with_columns(\n",
    "    pl.col('Pull-out duration (mins)').cast(pl.Float64).alias('Duration')\n",
    ")\n",
    "PULL_OUT_DATA = PULL_OUT_DATA.with_columns(pl.col('Duration') / 60)\n",
    "PULL_OUT = PULL_OUT_DATA.group_by(['Pull-out Date','Emp ID']).agg([pl.col('Duration').sum().cast(pl.Float64).alias('total_time_pull_out')])\n",
    "\n",
    "HC_MASTER_DATABASE = input_data(folder_paths[\"hc_extend_by_month\"])\n",
    "HC_MASTER_DATABASE = HC_MASTER_DATABASE.rename({'Date Start Week':'Week_Monday'})\n",
    "HC_MASTER_DATABASE = HC_MASTER_DATABASE.with_columns(pl.col('Date').str.to_date(\"%Y-%m-%d\", strict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huuchinh.nguyen\\AppData\\Local\\Temp\\ipykernel_27960\\514442199.py:40: DeprecationWarning: `DataFrame.melt` is deprecated. Use `unpivot` instead, with `index` instead of `id_vars` and `on` instead of `value_vars`\n",
      "  df = df.melt(id_vars=[\"OU\", \"employee_code\", \"employee_name\", \"Employee Type\"],variable_name=\"Date\",value_name=\"ramco_marked\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (14_727, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>EID</th><th>employee_name</th><th>Employee Type</th><th>Date</th><th>ramco_marked</th></tr><tr><td>str</td><td>str</td><td>str</td><td>date</td><td>str</td></tr></thead><tbody><tr><td>&quot;103110468&quot;</td><td>&quot;THANH DAT&nbsp;&nbsp;HUA&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>2025-04-18</td><td>&quot;WO&quot;</td></tr><tr><td>&quot;103111956&quot;</td><td>&quot;MINH QUAN&nbsp;&nbsp;LE&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>2025-04-21</td><td>&quot;SL&quot;</td></tr><tr><td>&quot;102502718&quot;</td><td>&quot;GIA HAN&nbsp;&nbsp;DINH&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>2025-04-22</td><td>&quot;AB&quot;</td></tr><tr><td>&quot;103124232&quot;</td><td>&quot;PHUONG TIEN&nbsp;&nbsp;DANG&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>2025-04-26</td><td>&quot;WO&quot;</td></tr><tr><td>&quot;103109091&quot;</td><td>&quot;NGOC SON&nbsp;&nbsp;NGUYEN&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>2025-04-27</td><td>&quot;SL&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;103101217&quot;</td><td>&quot;TIEN DAT&nbsp;&nbsp;HA&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>2025-05-05</td><td>&quot;PR&quot;</td></tr><tr><td>&quot;102502737&quot;</td><td>&quot;THANH THUY&nbsp;&nbsp;DANG&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>2025-05-07</td><td>&quot;PR&quot;</td></tr><tr><td>&quot;103169793&quot;</td><td>&quot;THI PHUONG DUNG&nbsp;&nbsp;LE&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>2025-05-20</td><td>&quot;PR&quot;</td></tr><tr><td>&quot;102884064&quot;</td><td>&quot;MIA TUYET MAI&nbsp;&nbsp;NGUYEN&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>2025-05-11</td><td>&quot;WO&quot;</td></tr><tr><td>&quot;102458550&quot;</td><td>&quot;THI THU HA&nbsp;&nbsp;TRAN&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>2025-05-26</td><td>&quot;PR&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (14_727, 5)\n",
       "┌───────────┬───────────────────────┬─────────────────┬────────────┬──────────────┐\n",
       "│ EID       ┆ employee_name         ┆ Employee Type   ┆ Date       ┆ ramco_marked │\n",
       "│ ---       ┆ ---                   ┆ ---             ┆ ---        ┆ ---          │\n",
       "│ str       ┆ str                   ┆ str             ┆ date       ┆ str          │\n",
       "╞═══════════╪═══════════════════════╪═════════════════╪════════════╪══════════════╡\n",
       "│ 103110468 ┆ THANH DAT  HUA        ┆ Fixed Term Hire ┆ 2025-04-18 ┆ WO           │\n",
       "│ 103111956 ┆ MINH QUAN  LE         ┆ Fixed Term Hire ┆ 2025-04-21 ┆ SL           │\n",
       "│ 102502718 ┆ GIA HAN  DINH         ┆ Fixed Term Hire ┆ 2025-04-22 ┆ AB           │\n",
       "│ 103124232 ┆ PHUONG TIEN  DANG     ┆ Fixed Term Hire ┆ 2025-04-26 ┆ WO           │\n",
       "│ 103109091 ┆ NGOC SON  NGUYEN      ┆ Fixed Term Hire ┆ 2025-04-27 ┆ SL           │\n",
       "│ …         ┆ …                     ┆ …               ┆ …          ┆ …            │\n",
       "│ 103101217 ┆ TIEN DAT  HA          ┆ Fixed Term Hire ┆ 2025-05-05 ┆ PR           │\n",
       "│ 102502737 ┆ THANH THUY  DANG      ┆ Fixed Term Hire ┆ 2025-05-07 ┆ PR           │\n",
       "│ 103169793 ┆ THI PHUONG DUNG  LE   ┆ Fixed Term Hire ┆ 2025-05-20 ┆ PR           │\n",
       "│ 102884064 ┆ MIA TUYET MAI  NGUYEN ┆ Fixed Term Hire ┆ 2025-05-11 ┆ WO           │\n",
       "│ 102458550 ┆ THI THU HA  TRAN      ┆ Fixed Term Hire ┆ 2025-05-26 ┆ PR           │\n",
       "└───────────┴───────────────────────┴─────────────────┴────────────┴──────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_ramco(folder_path, HC_MASTER_DATABASE):\n",
    "    import polars as pl\n",
    "    import glob\n",
    "\n",
    "    file_paths = glob.glob(f\"{folder_path}/Attendance Normal Month*.xlsx\") + glob.glob(f\"{folder_path}/Attendance Normal Month*.csv\")\n",
    "    df_list = []\n",
    "\n",
    "    for file in file_paths:\n",
    "        if file.endswith('.xlsx'):\n",
    "            df_raw = pl.read_excel(file, sheet_id=0)\n",
    "        elif file.endswith('.csv'):\n",
    "            try:\n",
    "                df_raw = pl.read_csv(file, encoding=\"utf-8\")\n",
    "            except:\n",
    "                df_raw = pl.read_csv(file, encoding=\"ISO-8859-1\", ignore_errors=True)\n",
    "\n",
    "        columns_to_keep = ['OU', 'employee_code', 'employee_name', 'Employee Type'] + [f'{i}' for i in range(1, 32)]\n",
    "        available_columns = [col for col in columns_to_keep if col in df_raw.columns]\n",
    "        if not available_columns:\n",
    "            continue\n",
    "\n",
    "        df = df_raw.select(available_columns)\n",
    "\n",
    "        if df.height < 2:\n",
    "            continue\n",
    "\n",
    "        header_row = df.row(0)\n",
    "\n",
    "        new_columns = []\n",
    "        for idx, col in enumerate(df.columns):\n",
    "            if col in [str(i) for i in range(1, 32)]:\n",
    "                new_col_name = header_row[idx]\n",
    "                new_col_name_str = str(new_col_name) if new_col_name is not None else col\n",
    "                new_columns.append(new_col_name_str)\n",
    "            else:\n",
    "                new_columns.append(col)\n",
    "        df = df[2:]\n",
    "        df = df.rename({old: new for old, new in zip(df.columns, new_columns)})\n",
    "        df = df.filter(pl.col(\"OU\").str.contains(\"Vietnam\"))\n",
    "        df = df.melt(id_vars=[\"OU\", \"employee_code\", \"employee_name\", \"Employee Type\"],variable_name=\"Date\",value_name=\"ramco_marked\")\n",
    "        df = df.filter((pl.col('Date') != \"\") & pl.col('Date').is_not_null())\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"Date\").str.strptime(pl.Date, \"%d-%b-%Y\", strict=False)\n",
    "        )\n",
    "        HC = HC_MASTER_DATABASE.select(['Date','OracleID', 'Email Id','Status'])\n",
    "        df = df.join(HC, left_on=[\"Date\",\"employee_code\"], right_on=[\"Date\",\"OracleID\"], how='left')\n",
    "        df = df.filter(pl.col('Status')==\"Active\")\n",
    "        df = df.select(pl.all().exclude([\"OU\", \"Email Id\", \"Status\"]))\n",
    "        # df = df.with_columns(pl.col('employee_code').cast(pl.Int32).alias('employee_code'))\n",
    "        df = df.unique()\n",
    "        df_list.append(df)\n",
    "\n",
    "    if df_list:\n",
    "        final_df = pl.concat(df_list, how='vertical')\n",
    "    else:\n",
    "        final_df = pl.DataFrame()\n",
    "\n",
    "    return final_df\n",
    "\n",
    "RAMCO = process_ramco(folder_paths[\"input_ramco_code\"], HC_MASTER_DATABASE)\n",
    "RAMCO = RAMCO.rename({'employee_code':'EID'})\n",
    "RAMCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huuchinh.nguyen\\AppData\\Local\\Temp\\ipykernel_27960\\3655215165.py:48: DeprecationWarning: `DataFrame.melt` is deprecated. Use `unpivot` instead, with `index` instead of `id_vars` and `on` instead of `value_vars`\n",
      "  df = df.melt(id_vars=[\"OU\", \"employee_code\", \"employee_name\", \"Employee Type\", \"OT Type\"],variable_name=\"Date\",value_name=\"Temp\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (22_426, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>employee_code</th><th>employee_name</th><th>Employee Type</th><th>OT Type</th><th>Date</th><th>Temp</th><th>Hours</th><th>Status</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>date</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;103145144&quot;</td><td>&quot;QUANG DUY&nbsp;&nbsp;NGUYEN&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>&quot;OT3.0X&quot;</td><td>2025-04-07</td><td>&quot;0&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;103171084&quot;</td><td>&quot;NHAT HAO&nbsp;&nbsp;TRAN&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>&quot;NSA&quot;</td><td>2025-04-05</td><td>&quot;0&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;103131062&quot;</td><td>&quot;THI THANH PHUONG&nbsp;&nbsp;NGUYEN&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>&quot;OT3.0X&quot;</td><td>2025-04-19</td><td>&quot;0&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;102502737&quot;</td><td>&quot;THANH THUY&nbsp;&nbsp;DANG&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>&quot;NSA&quot;</td><td>2025-04-18</td><td>&quot;0&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;102830326&quot;</td><td>&quot;QUOC KHOI&nbsp;&nbsp;TRAN&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>&quot;OT3.0X&quot;</td><td>2025-04-27</td><td>&quot;0&quot;</td><td>null</td><td>null</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;102502746&quot;</td><td>&quot;HUU THO&nbsp;&nbsp;NGUYEN&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>&quot;OT3.0X&quot;</td><td>2025-05-10</td><td>&quot;0&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;102850964&quot;</td><td>&quot;NGUYEN NHU QUYNH&nbsp;&nbsp;TRAN&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>&quot;OT3.0X&quot;</td><td>2025-05-21</td><td>&quot;0&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;102478278&quot;</td><td>&quot;THANH TIEN&nbsp;&nbsp;VO&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>&quot;NSA&quot;</td><td>2025-05-21</td><td>&quot;0&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;102507584&quot;</td><td>&quot;QUOC VIET PHUONG&nbsp;&nbsp;LE&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>&quot;NSA&quot;</td><td>2025-05-05</td><td>&quot;0&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;103060471&quot;</td><td>&quot;THANH LUAN&nbsp;&nbsp;NGUYEN&quot;</td><td>&quot;Fixed Term Hire&quot;</td><td>&quot;OT3.9X&quot;</td><td>2025-05-08</td><td>&quot;0&quot;</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (22_426, 8)\n",
       "┌───────────────┬─────────────────┬─────────────────┬─────────┬────────────┬──────┬───────┬────────┐\n",
       "│ employee_code ┆ employee_name   ┆ Employee Type   ┆ OT Type ┆ Date       ┆ Temp ┆ Hours ┆ Status │\n",
       "│ ---           ┆ ---             ┆ ---             ┆ ---     ┆ ---        ┆ ---  ┆ ---   ┆ ---    │\n",
       "│ str           ┆ str             ┆ str             ┆ str     ┆ date       ┆ str  ┆ str   ┆ str    │\n",
       "╞═══════════════╪═════════════════╪═════════════════╪═════════╪════════════╪══════╪═══════╪════════╡\n",
       "│ 103145144     ┆ QUANG DUY       ┆ Fixed Term Hire ┆ OT3.0X  ┆ 2025-04-07 ┆ 0    ┆ null  ┆ null   │\n",
       "│               ┆ NGUYEN          ┆                 ┆         ┆            ┆      ┆       ┆        │\n",
       "│ 103171084     ┆ NHAT HAO  TRAN  ┆ Fixed Term Hire ┆ NSA     ┆ 2025-04-05 ┆ 0    ┆ null  ┆ null   │\n",
       "│ 103131062     ┆ THI THANH       ┆ Fixed Term Hire ┆ OT3.0X  ┆ 2025-04-19 ┆ 0    ┆ null  ┆ null   │\n",
       "│               ┆ PHUONG  NGUYEN  ┆                 ┆         ┆            ┆      ┆       ┆        │\n",
       "│ 102502737     ┆ THANH THUY      ┆ Fixed Term Hire ┆ NSA     ┆ 2025-04-18 ┆ 0    ┆ null  ┆ null   │\n",
       "│               ┆ DANG            ┆                 ┆         ┆            ┆      ┆       ┆        │\n",
       "│ 102830326     ┆ QUOC KHOI  TRAN ┆ Fixed Term Hire ┆ OT3.0X  ┆ 2025-04-27 ┆ 0    ┆ null  ┆ null   │\n",
       "│ …             ┆ …               ┆ …               ┆ …       ┆ …          ┆ …    ┆ …     ┆ …      │\n",
       "│ 102502746     ┆ HUU THO  NGUYEN ┆ Fixed Term Hire ┆ OT3.0X  ┆ 2025-05-10 ┆ 0    ┆ null  ┆ null   │\n",
       "│ 102850964     ┆ NGUYEN NHU      ┆ Fixed Term Hire ┆ OT3.0X  ┆ 2025-05-21 ┆ 0    ┆ null  ┆ null   │\n",
       "│               ┆ QUYNH  TRAN     ┆                 ┆         ┆            ┆      ┆       ┆        │\n",
       "│ 102478278     ┆ THANH TIEN  VO  ┆ Fixed Term Hire ┆ NSA     ┆ 2025-05-21 ┆ 0    ┆ null  ┆ null   │\n",
       "│ 102507584     ┆ QUOC VIET       ┆ Fixed Term Hire ┆ NSA     ┆ 2025-05-05 ┆ 0    ┆ null  ┆ null   │\n",
       "│               ┆ PHUONG  LE      ┆                 ┆         ┆            ┆      ┆       ┆        │\n",
       "│ 103060471     ┆ THANH LUAN      ┆ Fixed Term Hire ┆ OT3.9X  ┆ 2025-05-08 ┆ 0    ┆ null  ┆ null   │\n",
       "│               ┆ NGUYEN          ┆                 ┆         ┆            ┆      ┆       ┆        │\n",
       "└───────────────┴─────────────────┴─────────────────┴─────────┴────────────┴──────┴───────┴────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_ramco_ot(folder_path, HC_MASTER_DATABASE):\n",
    "    import polars as pl\n",
    "    import glob\n",
    "\n",
    "    file_paths = glob.glob(f\"{folder_path}/OT Normal Month*.xlsx\") + glob.glob(f\"{folder_path}/OT Normal Month*.csv\")\n",
    "    df_list = []\n",
    "\n",
    "    for file in file_paths:\n",
    "        if file.endswith('.xlsx'):\n",
    "            df_raw = pl.read_excel(file, sheet_id=0)\n",
    "        elif file.endswith('.csv'):\n",
    "            try:\n",
    "                df_raw = pl.read_csv(file, encoding=\"utf-8\")\n",
    "            except:\n",
    "                df_raw = pl.read_csv(file, encoding=\"ISO-8859-1\", ignore_errors=True)\n",
    "\n",
    "        columns_to_keep = ['OU', 'employee_code', 'employee_name', 'Employee Type','OT Type'] + [f'day_{i}' for i in range(1, 32)]\n",
    "        available_columns = [col for col in columns_to_keep if col in df_raw.columns]\n",
    "        if not available_columns:\n",
    "            continue\n",
    "\n",
    "        df = df_raw.select(available_columns)\n",
    "\n",
    "        if df.height < 2:\n",
    "            continue\n",
    "\n",
    "        header_row = df.row(0)\n",
    "\n",
    "        new_columns = []\n",
    "        for idx, col in enumerate(df.columns):\n",
    "            if col.startswith('day_'):\n",
    "                try:\n",
    "                    day_number = int(col.split('_')[1])\n",
    "                except (IndexError, ValueError):\n",
    "                    day_number = None\n",
    "\n",
    "                if day_number is not None and 1 <= day_number <= 31:\n",
    "                    new_col_name = header_row[idx]\n",
    "                    new_col_name_str = str(new_col_name) if new_col_name is not None else col\n",
    "                    new_columns.append(new_col_name_str)\n",
    "                else:\n",
    "                    new_columns.append(col)\n",
    "            else:\n",
    "                new_columns.append(col)\n",
    "        df = df[2:]\n",
    "        df = df.rename({old: new for old, new in zip(df.columns, new_columns)})\n",
    "        df = df.filter(pl.col(\"OU\").str.contains(\"Vietnam\"))\n",
    "        df = df.melt(id_vars=[\"OU\", \"employee_code\", \"employee_name\", \"Employee Type\", \"OT Type\"],variable_name=\"Date\",value_name=\"Temp\")\n",
    "        df = df.filter((pl.col('Date') != \"\") & pl.col('Date').is_not_null())\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"Date\").str.strptime(pl.Date, \"%d-%b-%Y\", strict=False)\n",
    "        )\n",
    "        HC = HC_MASTER_DATABASE.select(['Date','OracleID', 'Email Id','Status'])\n",
    "        df = df.join(HC, left_on=[\"Date\",\"employee_code\"], right_on=[\"Date\",\"OracleID\"], how='left')\n",
    "        df = df.filter(pl.col('Status')==\"Active\")\n",
    "        df = df.select(pl.all().exclude([\"OU\", \"Email Id\", \"Status\"]))\n",
    "        # df = df.with_columns(pl.col('employee_code').cast(pl.Int32).alias('employee_code'))\n",
    "        df = df.unique()\n",
    "        df_list.append(df)\n",
    "\n",
    "    if df_list:\n",
    "        final_df = pl.concat(df_list, how='vertical')\n",
    "    else:\n",
    "        final_df = pl.DataFrame()\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "RAMCO_OT_rawdata = process_ramco_ot(folder_paths['input_ramco_code'], HC_MASTER_DATABASE)\n",
    "\n",
    "def split_temp_column(df):\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"Temp\").str.extract(r\"(\\d+\\.\\d+)\", 1).alias(\"Hours\"),\n",
    "        pl.col(\"Temp\").str.extract(r\"-(\\w+)\", 1).alias(\"Status\")\n",
    "    ])\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        pl.when(pl.col(\"Temp\").is_in([\"0\", None])).then(None).otherwise(pl.col(\"Hours\")).alias(\"Hours\"),\n",
    "        pl.when(pl.col(\"Temp\").is_in([\"0\", None])).then(None).otherwise(pl.col(\"Status\")).alias(\"Status\")\n",
    "    ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "RAMCO_OT_rawdata = split_temp_column(RAMCO_OT_rawdata)\n",
    "\n",
    "RAMCO_OT = RAMCO_OT_rawdata.with_columns(pl.col('Hours').str.to_decimal().cast(pl.Int64).alias(\"Hours\"))\n",
    "RAMCO_OT = RAMCO_OT.with_columns(pl.when(pl.col(\"Hours\") > 0).then(pl.lit(\"(\") + pl.col(\"Hours\").cast(pl.Decimal(10, 1)) + pl.lit(\" hrs\") + pl.lit(\" - \") + pl.col(\"Status\") + pl.lit(\")\")).otherwise(pl.lit(\"\")).alias(\"OT Status\"))\n",
    "RAMCO_OT = RAMCO_OT.group_by([\"employee_code\", \"Date\"]).agg([\n",
    "    pl.col(\"OT Type\").first().alias(\"ot_type\"),\n",
    "    pl.when(pl.col(\"Hours\").is_not_null()).then(pl.col(\"Hours\")).sum().alias(\"hours\"),\n",
    "    pl.when(pl.col(\"OT Type\").str.contains(\"NSA\")).then(pl.col(\"Status\")).first().alias(\"nsa_authorize\"),\n",
    "    pl.when(pl.col(\"OT Type\").str.contains(\"OT\")).then(pl.col(\"Status\")).first().alias(\"ot_authorize\"),\n",
    "    pl.col(\"OT Status\").first().alias(\"ot_status\")\n",
    "]).fill_null(\"\")\n",
    "RAMCO_OT = RAMCO_OT.rename({'employee_code':'EID'})\n",
    "RAMCO_OT = RAMCO_OT.with_columns(pl.col('EID').cast(pl.Utf8))\n",
    "\n",
    "RAMCO_OT_rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1_194_094, 17)\n",
      "┌────────────┬────────────┬────────────┬───────────┬───┬───────────┬───────────┬───────────┬───────┐\n",
      "│ Agent      ┆ Supervisor ┆ Agent      ┆ Number of ┆ … ┆ Previous_ ┆ Night_Shi ┆ Date_Conv ┆ First │\n",
      "│ Email      ┆ Email      ┆ State      ┆ Active    ┆   ┆ Night_Shi ┆ ft_2_Chec ┆ erted     ┆ Shift │\n",
      "│ ---        ┆ ---        ┆ ---        ┆ Contacts  ┆   ┆ ft        ┆ k         ┆ ---       ┆ ---   │\n",
      "│ str        ┆ str        ┆ str        ┆ ---       ┆   ┆ ---       ┆ ---       ┆ date      ┆ str   │\n",
      "│            ┆            ┆            ┆ str       ┆   ┆ f64       ┆ bool      ┆           ┆       │\n",
      "╞════════════╪════════════╪════════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════╡\n",
      "│ lehoaianh. ┆ vuphuonguy ┆ Available  ┆ 1         ┆ … ┆ null      ┆ true      ┆ 2025-04-1 ┆ null  │\n",
      "│ vi@concent ┆ en.nguyen1 ┆ Chat       ┆           ┆   ┆           ┆           ┆ 6         ┆       │\n",
      "│ rix.com    ┆ @concentri ┆            ┆           ┆   ┆           ┆           ┆           ┆       │\n",
      "│            ┆ …          ┆            ┆           ┆   ┆           ┆           ┆           ┆       │\n",
      "│ kimngan.ng ┆ xuanan.ngu ┆ Available  ┆ 0         ┆ … ┆ null      ┆ true      ┆ 2025-02-0 ┆ null  │\n",
      "│ uyen3@conc ┆ yen1@conce ┆ Chat       ┆           ┆   ┆           ┆           ┆ 7         ┆       │\n",
      "│ entrix.com ┆ ntrix.com  ┆            ┆           ┆   ┆           ┆           ┆           ┆       │\n",
      "│ nguyenyenn ┆ xuanan.ngu ┆ Available  ┆ 1         ┆ … ┆ null      ┆ true      ┆ 2025-01-1 ┆ null  │\n",
      "│ hi.trinh@c ┆ yen1@conce ┆ Chat       ┆           ┆   ┆           ┆           ┆ 1         ┆       │\n",
      "│ oncentrix. ┆ ntrix.com  ┆            ┆           ┆   ┆           ┆           ┆           ┆       │\n",
      "│ …          ┆            ┆            ┆           ┆   ┆           ┆           ┆           ┆       │\n",
      "│ sonhai.vu@ ┆ khanh.nguy ┆ Available  ┆ 1         ┆ … ┆ null      ┆ true      ┆ 2025-01-0 ┆ null  │\n",
      "│ concentrix ┆ en1@concen ┆ Chat       ┆           ┆   ┆           ┆           ┆ 1         ┆       │\n",
      "│ .com       ┆ trix.com   ┆            ┆           ┆   ┆           ┆           ┆           ┆       │\n",
      "│ hientruc.t ┆ anhtuan.lu ┆ Available  ┆ 1         ┆ … ┆ null      ┆ true      ┆ 2025-02-0 ┆ null  │\n",
      "│ ran@concen ┆ ong@concen ┆ Chat       ┆           ┆   ┆           ┆           ┆ 6         ┆       │\n",
      "│ trix.com   ┆ trix.com   ┆            ┆           ┆   ┆           ┆           ┆           ┆       │\n",
      "│ …          ┆ …          ┆ …          ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …     │\n",
      "│ vubaokhang ┆ xuanan.ngu ┆ Available  ┆ 0         ┆ … ┆ null      ┆ true      ┆ 2025-02-1 ┆ null  │\n",
      "│ .bui@conce ┆ yen1@conce ┆ Chat       ┆           ┆   ┆           ┆           ┆ 2         ┆       │\n",
      "│ ntrix.com  ┆ ntrix.com  ┆            ┆           ┆   ┆           ┆           ┆           ┆       │\n",
      "│ anhtu.hoan ┆ nguyenthao ┆ Available  ┆ 0         ┆ … ┆ null      ┆ true      ┆ 2025-03-2 ┆ null  │\n",
      "│ g1@concent ┆ nhi.tran@c ┆ Chat       ┆           ┆   ┆           ┆           ┆ 7         ┆       │\n",
      "│ rix.com    ┆ oncentrix. ┆            ┆           ┆   ┆           ┆           ┆           ┆       │\n",
      "│            ┆ …          ┆            ┆           ┆   ┆           ┆           ┆           ┆       │\n",
      "│ tranchanh. ┆ chihuy.von ┆ Training   ┆ 1         ┆ … ┆ null      ┆ true      ┆ 2025-05-0 ┆ null  │\n",
      "│ phan@conce ┆ g@concentr ┆            ┆           ┆   ┆           ┆           ┆ 4         ┆       │\n",
      "│ ntrix.com  ┆ ix.com     ┆            ┆           ┆   ┆           ┆           ┆           ┆       │\n",
      "│ quynhmy.ph ┆ thithuhien ┆ Outbound   ┆ 1         ┆ … ┆ null      ┆ true      ┆ 2025-01-1 ┆ null  │\n",
      "│ an@concent ┆ .nguyen4@c ┆ Call       ┆           ┆   ┆           ┆           ┆ 1         ┆       │\n",
      "│ rix.com    ┆ oncentrix. ┆            ┆           ┆   ┆           ┆           ┆           ┆       │\n",
      "│            ┆ …          ┆            ┆           ┆   ┆           ┆           ┆           ┆       │\n",
      "│ ledinhhoan ┆ xuanan.ngu ┆ Available  ┆ 2         ┆ … ┆ null      ┆ true      ┆ 2025-02-1 ┆ null  │\n",
      "│ .do1@conce ┆ yen1@conce ┆ Chat       ┆           ┆   ┆           ┆           ┆ 5         ┆       │\n",
      "│ ntrix.com  ┆ ntrix.com  ┆            ┆           ┆   ┆           ┆           ┆           ┆       │\n",
      "└────────────┴────────────┴────────────┴───────────┴───┴───────────┴───────────┴───────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "AGENT_ACTIVITY_INPUT_FOR_NS = AGENT_ACTIVITY_INPUT.with_columns(pl.col('Start Time').cast(pl.Utf8).str.strptime(pl.Datetime).alias('Start Time'))\n",
    "AGENT_ACTIVITY_INPUT_FOR_NS = AGENT_ACTIVITY_INPUT_FOR_NS.with_columns(pl.col('End Time').cast(pl.Utf8).str.strptime(pl.Datetime).alias('End Time'))\n",
    "AGENT_ACTIVITY_INPUT_FOR_NS = AGENT_ACTIVITY_INPUT_FOR_NS.with_columns(pl.col('Start Time').dt.date().alias('Start Date'))\n",
    "AGENT_ACTIVITY_INPUT_FOR_NS = AGENT_ACTIVITY_INPUT_FOR_NS.with_columns(pl.col('End Time').dt.date().alias('End Date'))\n",
    "AGENT_ACTIVITY_INPUT_FOR_NS = AGENT_ACTIVITY_INPUT_FOR_NS.with_columns((pl.col('Start Date') + pl.duration(days=1)).alias('Next Date'))\n",
    "AGENT_ACTIVITY_INPUT_FOR_NS = AGENT_ACTIVITY_INPUT_FOR_NS.with_columns((pl.col('Start Date') - pl.duration(days=1)).alias('Previous Date'))\n",
    "\n",
    "ACTIVITY_MERGED_NIGHT_SHIFT = AGENT_ACTIVITY_INPUT_FOR_NS.join(Night_Shift[['Date','Email Id','Night_Shift','Previous_Night_Shift']],\n",
    "                                                                 left_on=['Start Date','Agent Email'],\n",
    "                                                                 right_on=['Date','Email Id'],\n",
    "                                                                 how='left')\n",
    "ACTIVITY_MERGED_NIGHT_SHIFT = ACTIVITY_MERGED_NIGHT_SHIFT.rename({'Start Date':'Date'})\n",
    "\n",
    "def update_night_shift(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    df = df.with_columns(\n",
    "        pl.when(\n",
    "            (pl.col('Night_Shift') == 0) & \n",
    "            (pl.col('Start Time').dt.time() < pl.time(17, 0)) & \n",
    "            (pl.col('Start Time').dt.time() >= pl.time(0, 0)) & \n",
    "            (pl.col('Previous_Night_Shift') == 1)\n",
    "        ).then(False).otherwise(True).alias('Night_Shift_2_Check')\n",
    "    )\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col('Night_Shift_2_Check') == False)\n",
    "        .then(1)\n",
    "        .otherwise(pl.col('Night_Shift'))\n",
    "        .alias('Night_Shift')\n",
    "    )\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.when((pl.col('Start Time').dt.hour() >= 0) & (pl.col('Start Time').dt.hour() < 12) & (pl.col('Previous_Night_Shift') == 1))\n",
    "        .then(pl.col('Date') - pl.duration(days=1))\n",
    "        \n",
    "        .when((pl.col('Start Time').dt.hour() >= 0) & (pl.col('Start Time').dt.hour() < 12) & (pl.col('Night_Shift') == 1))\n",
    "        .then(pl.col('Date') - pl.duration(days=1))\n",
    "        \n",
    "        .when((pl.col('Start Time').dt.hour() >= 0) & (pl.col('Start Time').dt.hour() < 18) & (pl.col('Night_Shift') == 0))\n",
    "        .then(pl.col('Date'))\n",
    "        \n",
    "        .when((pl.col('Start Time').dt.hour() >= 18) & (pl.col('Night_Shift') == 1))\n",
    "        .then(pl.col('Date'))\n",
    "        \n",
    "        .otherwise(pl.col('Date'))\n",
    "        .alias('Date_Converted')\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "ACTIVITY_MERGED_NIGHT_SHIFT = update_night_shift(ACTIVITY_MERGED_NIGHT_SHIFT)\n",
    "\n",
    "Drop_NS = ACTIVITY_MERGED_NIGHT_SHIFT.drop('Night_Shift')\n",
    "Merged_Drop_NS_With_IEX = Drop_NS.join(IEX[['Date','Email Id','First Shift']], left_on=['Date','Agent Email'], right_on=['Date','Email Id'], how='left')\n",
    "\n",
    "Agent_Activity_Include_IEX = Merged_Drop_NS_With_IEX.filter(~pl.col('First Shift').is_null())\n",
    "\n",
    "Agent_Not_IEX = Merged_Drop_NS_With_IEX.filter(pl.col('First Shift').is_null())\n",
    "print(Agent_Not_IEX)\n",
    "Agent_Not_IEX = Agent_Not_IEX.rename({'Date':'Start Date'})\n",
    "Agent_Not_IEX = Agent_Not_IEX[['Agent Email', 'Supervisor Email','Agent State','Number of Active Contacts','Productive Aux Flag (Yes / No)','Start Time','End Time','Total Time in seconds','Agent Business Location', 'Start Date', 'End Date', 'Next Date', 'Previous Date']]\n",
    "def add_tracker_time(Agent_Not_IEX: pl.DataFrame, agent_col: str, date_col: str, time_col: str, new_col: str, agg_func: str, time_threshold: int = 13, time_comparison: str = '>') -> pl.DataFrame:\n",
    "    if time_comparison not in ['>', '<']:\n",
    "        raise ValueError(\"Invalid comparison operator. Please use '>' or '<'.\")\n",
    "    if time_comparison == '>':\n",
    "        filtered_data = Agent_Not_IEX.filter(pl.col(time_col).dt.hour() > time_threshold)\n",
    "    else:\n",
    "        filtered_data = Agent_Not_IEX.filter(pl.col(time_col).dt.hour() < time_threshold)\n",
    "    if agg_func not in ['min', 'max']:\n",
    "        raise ValueError(\"Invalid aggregation function. Please use 'min' or 'max'.\")\n",
    "    aggregation = getattr(pl.col(time_col), agg_func)().alias(new_col)\n",
    "    min_max_time_per_group = filtered_data.group_by([agent_col, date_col]).agg(aggregation)\n",
    "    Agent_Not_IEX = Agent_Not_IEX.join(min_max_time_per_group, on=[agent_col, date_col], how='left')\n",
    "    return Agent_Not_IEX\n",
    "\n",
    "AGENT_ACTIVITY_INPUT_1 = add_tracker_time(Agent_Not_IEX, agent_col=\"Agent Email\", date_col=\"Start Date\", time_col=\"Start Time\", new_col=\"Min Date < 1pm\", agg_func=\"min\", time_comparison='<', time_threshold=13)\n",
    "AGENT_ACTIVITY_INPUT_1 = add_tracker_time(AGENT_ACTIVITY_INPUT_1, agent_col=\"Agent Email\", date_col=\"Start Date\", time_col=\"Start Time\", new_col=\"Max Date < 1pm\", agg_func=\"max\", time_comparison='<', time_threshold=13)\n",
    "AGENT_ACTIVITY_INPUT_1 = add_tracker_time(AGENT_ACTIVITY_INPUT_1, agent_col=\"Agent Email\", date_col=\"Start Date\", time_col=\"Start Time\", new_col=\"Min Date > 1pm\", agg_func=\"min\", time_comparison='>', time_threshold=13)\n",
    "AGENT_ACTIVITY_INPUT_1 = add_tracker_time(AGENT_ACTIVITY_INPUT_1, agent_col=\"Agent Email\", date_col=\"Start Date\", time_col=\"Start Time\", new_col=\"Max Date > 1pm\", agg_func=\"max\", time_comparison='>', time_threshold=13)\n",
    "AGENT_ACTIVITY_INPUT_1 = AGENT_ACTIVITY_INPUT_1[['Agent Email','Start Date','Next Date','Min Date < 1pm','Max Date < 1pm','Min Date > 1pm','Max Date > 1pm']].unique()\n",
    "\n",
    "AGENT_ACTIVITY_INPUT_2 = add_tracker_time(Agent_Not_IEX, agent_col=\"Agent Email\", date_col=\"Start Date\", time_col=\"Start Time\", new_col=\"Min Date+1 < 4am\", agg_func=\"min\", time_comparison='<', time_threshold=4)\n",
    "AGENT_ACTIVITY_INPUT_2 = add_tracker_time(AGENT_ACTIVITY_INPUT_2, agent_col=\"Agent Email\", date_col=\"Start Date\", time_col=\"Start Time\", new_col=\"Max Date+1 < 1pm\", agg_func=\"max\", time_comparison='<', time_threshold=13)\n",
    "AGENT_ACTIVITY_INPUT_2 = AGENT_ACTIVITY_INPUT_2[['Agent Email', 'Start Date','Min Date+1 < 4am','Max Date+1 < 1pm']].unique()\n",
    "\n",
    "AGENT_ACTIVITY_INPUT_FINAL = AGENT_ACTIVITY_INPUT_1.join(AGENT_ACTIVITY_INPUT_2, left_on=['Agent Email', 'Start Date'],right_on=['Agent Email', 'Start Date'], how=\"left\")\n",
    "\n",
    "night_shift_condition = (\n",
    "    ((pl.col('Min Date+1 < 4am').dt.hour() < 4) & (pl.col('Max Date+1 < 1pm').dt.hour() < 13))\n",
    ")\n",
    "\n",
    "AddedCustomNight_Shift_Check = AGENT_ACTIVITY_INPUT_FINAL.with_columns(\n",
    "    pl.when(night_shift_condition).then(1).otherwise(0).alias('Night_Shift')\n",
    ")\n",
    "AddedCustomNight_Shift_Check = AddedCustomNight_Shift_Check[['Agent Email','Start Date','Next Date','Min Date < 1pm','Max Date < 1pm','Min Date > 1pm','Max Date > 1pm','Min Date+1 < 4am','Max Date+1 < 1pm','Night_Shift']]\n",
    "AddedCustomNight_Shift_Check = AddedCustomNight_Shift_Check.unique()\n",
    "AddedCustomNight_Shift_Check = AddedCustomNight_Shift_Check.rename({'Start Date':'Date'})\n",
    "AddedCustomNight_Shift_Check = AddedCustomNight_Shift_Check.sort('Date')\n",
    "\n",
    "Night_Shift_Tracker_1 = AddedCustomNight_Shift_Check[['Agent Email','Date','Night_Shift']]\n",
    "Night_Shift_Tracker_2 = Night_Shift_Tracker_1.with_columns((pl.col('Date') - pl.duration(days=1)).alias('Previous Date'))\n",
    "Night_Shift_Tracker = Night_Shift_Tracker_2.join(Night_Shift_Tracker_1[['Date','Agent Email','Night_Shift']],\n",
    "                                                                 left_on=['Previous Date','Agent Email'],\n",
    "                                                                 right_on=['Date','Agent Email'],\n",
    "                                                                 how='left')\n",
    "Night_Shift_Tracker = Night_Shift_Tracker.rename({'Night_Shift_right':'Previous_Night_Shift'})\n",
    "\n",
    "ACTIVITY_MERGED_NIGHT_SHIFT_1 = Agent_Not_IEX.join(Night_Shift_Tracker[['Date','Agent Email','Night_Shift','Previous_Night_Shift']],\n",
    "                                                                 left_on=['Start Date','Agent Email'],\n",
    "                                                                 right_on=['Date','Agent Email'],\n",
    "                                                                 how='left')\n",
    "ACTIVITY_MERGED_NIGHT_SHIFT_1 = ACTIVITY_MERGED_NIGHT_SHIFT_1.rename({'Start Date':'Date'})\n",
    "\n",
    "\n",
    "ACTIVITY_MERGED_NIGHT_SHIFT_1 = update_night_shift(ACTIVITY_MERGED_NIGHT_SHIFT_1)\n",
    "Agent_Activity_Exclude_IEX = ACTIVITY_MERGED_NIGHT_SHIFT_1.drop('Night_Shift')\n",
    "\n",
    "Agent_Activity_Include_IEX = Agent_Activity_Include_IEX.with_columns(\n",
    "    pl.col('Previous_Night_Shift').cast(pl.Int32)\n",
    ")\n",
    "\n",
    "Agent_Activity_Exclude_IEX = Agent_Activity_Exclude_IEX.with_columns(\n",
    "    pl.col('Previous_Night_Shift').cast(pl.Int32)\n",
    ")\n",
    "\n",
    "# AGENT_ACTIVITY_COMBINE = pl.concat([Agent_Activity_Include_IEX, Agent_Activity_Exclude_IEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date_Converted', 'Email Id', 'Target', 'Datetime_Fluctuate_Start_Shift', 'Datetime_Fluctuate_End_Shift', 'Fluctuate Shift', 'Datetime_First_Start_Shift', 'Datetime_First_End_Shift', 'First Shift', 'Time_Of_Day', 'Open Time', 'Extra Time', 'Break Time', 'Lunch Time', 'Training', 'NCNS', 'AL', 'Unplanned', 'Planned', 'Roster Presented', 'Roster Scheduled', 'Night_Shift', 'Shift Tracking', 'OT Type', 'Combined OT Range', 'OT PreShift', 'OT PostShift', 'Supervisor Email', 'Agent State', 'Number of Active Contacts', 'Productive Aux Flag (Yes / No)', 'Start Time', 'End Time', 'Total Time in seconds', 'Agent Business Location', 'Date', 'End Date', 'Next Date', 'Previous Date', 'Previous_Night_Shift', 'Night_Shift_2_Check', 'First Shift_right', 'Year', 'Month', 'Week_Monday', 'OracleID', 'IEX ID', 'Employee Name', 'LOB', 'LOB_2', 'LOB_3', 'Alias', 'Supervisor Name', 'Wave', 'Detail Status', 'Status']\n"
     ]
    }
   ],
   "source": [
    "HC = HC_MASTER_DATABASE[['Year', 'Month', 'Week_Monday', 'Date','OracleID','IEX ID','Employee Name', 'Email Id','LOB','LOB_2','LOB_3',\n",
    "                   'Alias', 'Supervisor Name', 'Wave', 'Detail Status', 'Status']].unique()\n",
    "HC = HC.rename({'Date':'Date_Converted'})\n",
    "\n",
    "COMPACT_IEX = IEX[['Date','Email Id','Target', 'Datetime_Fluctuate_Start_Shift', 'Datetime_Fluctuate_End_Shift', \n",
    "                   'Fluctuate Shift', 'Datetime_First_Start_Shift', 'Datetime_First_End_Shift', \n",
    "                   'First Shift','Time_Of_Day','Open Time', 'Extra Time', 'Break Time', 'Lunch Time', 'Training', \n",
    "                   'NCNS','AL','Unplanned','Planned','Roster Presented','Roster Scheduled', 'Night_Shift','Shift Tracking','OT Type','Combined OT Range', 'OT PreShift', 'OT PostShift']].unique()\n",
    "COMPACT_IEX = COMPACT_IEX.rename({'Date':'Date_Converted'})\n",
    "\n",
    "Agent_Activity_Include_IEX = Agent_Activity_Include_IEX.join(HC,left_on=['Date_Converted','Agent Email'],right_on=['Date_Converted','Email Id'],how='left')\n",
    "Agent_Activity_Include_IEX = COMPACT_IEX.join(Agent_Activity_Include_IEX,left_on=['Date_Converted','Email Id'],right_on=['Date_Converted','Agent Email'],how='left')\n",
    "\n",
    "Agent_Activity_Include_IEX_1 = Agent_Activity_Include_IEX.filter(pl.col(\"Week_Monday\").is_not_null())\n",
    "Agent_Activity_Include_IEX_2 = Agent_Activity_Include_IEX.filter((pl.col(\"First Shift\").is_not_null()) & (pl.col(\"Week_Monday\").is_null()))\n",
    "\n",
    "standard_columns_1 = Agent_Activity_Include_IEX_1.columns[:Agent_Activity_Include_IEX_1.columns.index('Year')]\n",
    "Agent_Activity_Include_IEX_2 = Agent_Activity_Include_IEX_2.select(standard_columns_1)\n",
    "Agent_Activity_Include_IEX_2 = Agent_Activity_Include_IEX_2.join(HC,left_on=['Date_Converted','Email Id'],right_on=['Date_Converted','Email Id'],how='left')\n",
    "\n",
    "Agent_Activity_Include_IEX = pl.concat([Agent_Activity_Include_IEX_1, Agent_Activity_Include_IEX_2]).unique()\n",
    "\n",
    "Agent_Activity_Exclude_IEX = Agent_Activity_Exclude_IEX.join(HC,left_on=['Date_Converted','Agent Email'],right_on=['Date_Converted','Email Id'],how='left')\n",
    "Agent_Activity_Exclude_IEX = Agent_Activity_Exclude_IEX.rename({'Agent Email': 'Email Id'})\n",
    "\n",
    "columns_1 = set(Agent_Activity_Include_IEX.columns)\n",
    "columns_2 = set(Agent_Activity_Exclude_IEX.columns)\n",
    "\n",
    "missing_in_1 = columns_2 - columns_1\n",
    "missing_in_2 = columns_1 - columns_2\n",
    "\n",
    "for col in missing_in_1:\n",
    "    Agent_Activity_Include_IEX = Agent_Activity_Include_IEX.with_columns(pl.lit(None).alias(col))\n",
    "\n",
    "for col in missing_in_2:\n",
    "    Agent_Activity_Exclude_IEX = Agent_Activity_Exclude_IEX.with_columns(pl.lit(None).alias(col))\n",
    "\n",
    "standard_columns_2 = Agent_Activity_Include_IEX.columns\n",
    "\n",
    "Agent_Activity_Exclude_IEX = Agent_Activity_Exclude_IEX.select(standard_columns_2)\n",
    "\n",
    "ACTIVITY_DATE_CONVERTED_COMBINED = pl.concat([Agent_Activity_Include_IEX, Agent_Activity_Exclude_IEX]).unique()\n",
    "print(ACTIVITY_DATE_CONVERTED_COMBINED.columns)\n",
    "\n",
    "ACTIVITY_DATE_CONVERTED_COMBINED = ACTIVITY_DATE_CONVERTED_COMBINED.with_columns([\n",
    "    pl.col(\"Year\").cast(pl.Utf8),\n",
    "    pl.col(\"OracleID\").cast(pl.Utf8),\n",
    "    pl.col(\"IEX ID\").cast(pl.Utf8),\n",
    "])\n",
    "\n",
    "ACTIVITY_DATE_CONVERTED = ACTIVITY_DATE_CONVERTED_COMBINED.sort('Start Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1_479_385,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Productive Aux Flag (Yes / No)</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;Yes&quot;</td></tr><tr><td>&quot;Yes&quot;</td></tr><tr><td>&quot;Yes&quot;</td></tr><tr><td>&quot;Yes&quot;</td></tr><tr><td>&quot;Yes&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1_479_385,)\n",
       "Series: 'Productive Aux Flag (Yes / No)' [str]\n",
       "[\n",
       "\tnull\n",
       "\tnull\n",
       "\tnull\n",
       "\tnull\n",
       "\tnull\n",
       "\t…\n",
       "\t\"Yes\"\n",
       "\t\"Yes\"\n",
       "\t\"Yes\"\n",
       "\t\"Yes\"\n",
       "\t\"Yes\"\n",
       "]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACTIVITY_DATE_CONVERTED['Productive Aux Flag (Yes / No)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (65, 3)\n",
      "┌────────────────┬─────────────────────────────────┬────────────────┐\n",
      "│ Date_Converted ┆ Email Id                        ┆ Shift Tracking │\n",
      "│ ---            ┆ ---                             ┆ ---            │\n",
      "│ date           ┆ str                             ┆ str            │\n",
      "╞════════════════╪═════════════════════════════════╪════════════════╡\n",
      "│ 2025-05-12     ┆ quockhang.lam@concentrix.com    ┆ Off            │\n",
      "│ 2025-05-12     ┆ thaitrung.tang@concentrix.com   ┆ Off            │\n",
      "│ 2025-05-12     ┆ huuhoang.nguyen@concentrix.com  ┆ Off            │\n",
      "│ 2025-05-12     ┆ phanbaotuan.vo@concentrix.com   ┆ Off            │\n",
      "│ 2025-05-12     ┆ daothanhtam.pham2@concentrix.c… ┆ Off            │\n",
      "│ …              ┆ …                               ┆ …              │\n",
      "│ 2025-05-12     ┆ thituongvy.tran@concentrix.com  ┆ Off            │\n",
      "│ 2025-05-12     ┆ anny.doan@concentrix.com        ┆ Off            │\n",
      "│ 2025-05-12     ┆ maihanhnhi.tran@concentrix.com  ┆ Off            │\n",
      "│ 2025-05-12     ┆ nuhuynhnhi.ton@concentrix.com   ┆ Off            │\n",
      "│ 2025-05-12     ┆ vanminhduy.ngo@concentrix.com   ┆ Off            │\n",
      "└────────────────┴─────────────────────────────────┴────────────────┘\n"
     ]
    }
   ],
   "source": [
    "GROUP_AND_SUM = ACTIVITY_DATE_CONVERTED.group_by([\n",
    "    'Year','Month','Week_Monday','Date_Converted','Employee Name','Email Id','OracleID','IEX ID','Target',\n",
    "    'Datetime_Fluctuate_Start_Shift','Datetime_Fluctuate_End_Shift','Fluctuate Shift',\n",
    "    'Datetime_First_Start_Shift', 'Datetime_First_End_Shift', 'First Shift', 'Alias', 'LOB', 'LOB_2','LOB_3',\n",
    "    'Supervisor Name','Wave','Detail Status','Status','Time_Of_Day','Open Time','Extra Time','Break Time',\n",
    "    'Lunch Time','Training', 'NCNS','AL','Unplanned','Planned','Roster Presented','Roster Scheduled',\n",
    "    'Night_Shift','Shift Tracking','OT Type','Combined OT Range','OT PreShift','OT PostShift'\n",
    "]).agg([\n",
    "    pl.col(\"Start Time\").min().alias(\"start\"),\n",
    "    pl.col(\"End Time\").max().alias(\"end\"),\n",
    "    pl.col(\"Total Time in seconds\").sum().cast(pl.Float64).alias(\"duration\"),\n",
    "\n",
    "    pl.when(pl.col(\"Number of Active Contacts\").is_not_null()).then(pl.col(\"Total Time in seconds\")).sum().cast(pl.Float64).alias(\"total_time_chat_handle\"),\n",
    "    pl.when(pl.col(\"Productive Aux Flag (Yes / No)\") == \"Yes\").then(pl.col(\"Total Time in seconds\")).sum().cast(pl.Float64).alias(\"sum_productive\"),\n",
    "\n",
    "    # BREAK (idle, Productive Aux Flag = No)\n",
    "    pl.when(\n",
    "        (pl.col(\"Agent State\").is_in([\"Break-IDLE\", \"Break\"])) &\n",
    "        (pl.col(\"Productive Aux Flag (Yes / No)\") == \"No\")\n",
    "    ).then(pl.col(\"Total Time in seconds\")).sum().cast(pl.Float64).alias(\"break\"),\n",
    "\n",
    "    # LUNCH (idle, Productive Aux Flag = No)\n",
    "    pl.when(\n",
    "        (pl.col(\"Agent State\").is_in([\"Lunch-IDLE\", \"Lunch\"])) &\n",
    "        (pl.col(\"Productive Aux Flag (Yes / No)\") == \"No\")\n",
    "    ).then(pl.col(\"Total Time in seconds\")).sum().cast(pl.Float64).alias(\"lunch\"),\n",
    "\n",
    "    # COACHING (idle, Productive Aux Flag = No)\n",
    "    pl.when(\n",
    "        (pl.col(\"Agent State\").is_in([\"Coaching-IDLE\", \"Coaching\"])) &\n",
    "        (pl.col(\"Productive Aux Flag (Yes / No)\") == \"No\")\n",
    "    ).then(pl.col(\"Total Time in seconds\")).sum().cast(pl.Float64).alias(\"coaching-idle\"),\n",
    "\n",
    "    # TRAINING (idle, Productive Aux Flag = No)\n",
    "    pl.when(\n",
    "        (pl.col(\"Agent State\").is_in([\"Training-IDLE\", \"Training\"])) &\n",
    "        (pl.col(\"Productive Aux Flag (Yes / No)\") == \"No\")\n",
    "    ).then(pl.col(\"Total Time in seconds\")).sum().cast(pl.Float64).alias(\"training-idle\"),\n",
    "\n",
    "    # OUTBOUND (idle, Productive Aux Flag = No)\n",
    "    pl.when(\n",
    "        (pl.col(\"Agent State\").is_in([\"Outbound Call-IDLE\"])) &\n",
    "        (pl.col(\"Productive Aux Flag (Yes / No)\") == \"No\")\n",
    "    ).then(pl.col(\"Total Time in seconds\")).sum().cast(pl.Float64).alias(\"outbound-idle\"),\n",
    "\n",
    "    # BREAK COUNT (idle, Productive Aux Flag = No)\n",
    "    pl.when(\n",
    "        (pl.col(\"Agent State\").is_in([\"Break-IDLE\"])) & \n",
    "        (pl.col(\"Total Time in seconds\") > 60) &\n",
    "        (pl.col(\"Productive Aux Flag (Yes / No)\") == \"No\")\n",
    "    ).then(pl.col(\"Total Time in seconds\")).count().cast(pl.Float64).alias(\"break_count\"),\n",
    "\n",
    "    # OTHER STATUS (idle, Productive Aux Flag = No, loại trừ các trạng thái trên)\n",
    "    pl.when(\n",
    "        (pl.col(\"Productive Aux Flag (Yes / No)\") == \"No\") &\n",
    "        (~pl.col(\"Agent State\").is_in([\n",
    "            \"Break-IDLE\", \"Break\", \"Lunch-IDLE\", \"Lunch\",\n",
    "            \"Coaching-IDLE\", \"Coaching\", \"Training-IDLE\", \"Training\",\n",
    "            \"Outbound Call-IDLE\"\n",
    "        ]))\n",
    "    ).then(pl.col(\"Total Time in seconds\")).sum().cast(pl.Float64).alias(\"other_status\"),\n",
    "])\n",
    "\n",
    "\n",
    "Added_Over_Break = GROUP_AND_SUM.with_columns(\n",
    "    pl.when(pl.col(\"break\").is_null()).then(0)\n",
    "    .when(pl.col(\"Detail Status\").str.contains(\"Production\"))\n",
    "        .then(pl.when((pl.col(\"break\") - 30 * 60) < 0).then(0).otherwise(pl.col(\"break\") - 30 * 60))\n",
    "    .when(pl.col(\"Detail Status\").str.contains(\"Nesting\"))\n",
    "        .then(pl.when((pl.col(\"break\") - 15 * 60) < 0).then(0).otherwise(pl.col(\"break\") - 15 * 60))\n",
    "    .otherwise(0)\n",
    "    .alias(\"over_break\")\n",
    ")\n",
    "\n",
    "Added_Over_Lunch = Added_Over_Break.with_columns(\n",
    "    pl.when(pl.col(\"lunch\").is_null()).then(0)\n",
    "    .when((pl.col(\"lunch\") - 60 * 60) < 0).then(0)\n",
    "    .otherwise(pl.col(\"lunch\") - 60 * 60)\n",
    "    .alias(\"over_lunch\")\n",
    ")\n",
    "\n",
    "Added_Exceed_Break = Added_Over_Lunch.with_columns(\n",
    "    pl.when(pl.col(\"break_count\") == 0)\n",
    "        .then(0)\n",
    "    .when(pl.col(\"Detail Status\").str.contains(\"Production\"))\n",
    "        .then(pl.when(pl.col(\"break_count\")  <= 2).then(0).otherwise(pl.col(\"break_count\") - 2))\n",
    "    .when(pl.col(\"Detail Status\").str.contains(\"Nesting\"))\n",
    "        .then(pl.when(pl.col(\"break_count\") <= 1).then(0).otherwise(pl.col(\"break_count\") - 1))\n",
    "    .otherwise(0)\n",
    "    .alias(\"exceed_break\")\n",
    ")\n",
    "\n",
    "arrange_duration = Added_Exceed_Break.select([\n",
    "    *[col for col in Added_Exceed_Break.columns if col != 'duration'],\n",
    "    'duration'\n",
    "])\n",
    "\n",
    "added_HC_Actual = arrange_duration.with_columns(\n",
    "    pl.when(pl.col(\"duration\").is_null()).then(0)\n",
    "    .otherwise(\n",
    "        pl.when(pl.col(\"Detail Status\").str.contains(\"Nesting\"))\n",
    "        .then(\n",
    "            pl.when(pl.col(\"duration\") >= 3*3600).then(1)\n",
    "            .when((pl.col(\"duration\") >= 2*3600) & (pl.col(\"duration\") <= 3*3600)).then(0.5)\n",
    "            .otherwise(0)\n",
    "        )\n",
    "        .otherwise(\n",
    "            pl.when(pl.col(\"duration\") >= 7*3600).then(1)\n",
    "            .when((pl.col(\"duration\") >= 3*3600) & (pl.col(\"duration\") <= 6*3600)).then(0.5)\n",
    "            .otherwise(0)\n",
    "        )\n",
    "    ).alias(\"hc_actual\")\n",
    ")\n",
    "\n",
    "added_HC_Schedule = added_HC_Actual.with_columns(pl.when(\n",
    "        (pl.col(\"First Shift\").str.contains(\"-\")) & (pl.col(\"Time_Of_Day\") > 0) & (pl.col(\"Time_Of_Day\") < 5*3600)).then(0.5).when(\n",
    "        (pl.col(\"First Shift\").str.contains(\"-\")) & (pl.col(\"Time_Of_Day\") >= 5*3600)).then(1)\n",
    "    .when(pl.col(\"First Shift\") == \"AL\").then(1).otherwise(0).cast(pl.Float64).alias(\"hc_schedule\"))\n",
    "\n",
    "added_Time_Late = added_HC_Schedule.with_columns(pl.when((pl.col(\"start\") - pl.col(\"Datetime_Fluctuate_Start_Shift\")).dt.total_seconds() > 180).then((pl.col(\"start\") - pl.col(\"Datetime_Fluctuate_Start_Shift\")).dt.total_seconds()).otherwise(0).cast(pl.Float64).alias(\"time_late\"))\n",
    "added_Time_Leave = added_Time_Late.with_columns(pl.when((pl.col(\"Datetime_Fluctuate_End_Shift\") - pl.col(\"end\")).dt.total_seconds() > 0).then((pl.col(\"Datetime_Fluctuate_End_Shift\") - pl.col(\"end\")).dt.total_seconds()).otherwise(0).cast(pl.Float64).alias(\"time_leave\"))\n",
    "addded_Lateness = added_Time_Leave.with_columns(pl.when(pl.col(\"start\").is_not_null()).then(pl.when((pl.col(\"time_late\").is_not_null()) & (pl.col(\"time_late\") != 0)).then(1).otherwise(0)).otherwise(None).cast(pl.Float64).alias(\"lateness\"))\n",
    "addAdherenceTime = addded_Lateness.with_columns((pl.col(\"Target\")- pl.col(\"time_late\")-pl.col(\"over_break\")-pl.col(\"over_lunch\")).alias(\"adherence_time\"))\n",
    "merged_pull_out = addAdherenceTime.join(PULL_OUT[['Pull-out Date','Emp ID', 'total_time_pull_out']], left_on=['Date_Converted','OracleID'],right_on=['Pull-out Date','Emp ID'], how='left')\n",
    "\n",
    "sec_to_hr = [\n",
    "    'Target', 'Time_Of_Day', 'Open Time', 'Extra Time', 'Break Time', 'Lunch Time', \n",
    "    'Training', 'NCNS', 'AL', 'total_time_chat_handle', 'sum_productive', 'break', \n",
    "    'lunch', 'outbound-idle','coaching-idle','training-idle', 'over_break', 'over_lunch', 'duration', 'time_late', \n",
    "    'time_leave', 'adherence_time', 'total_time_pull_out', 'other_status'\n",
    "]\n",
    "\n",
    "converted_time = merged_pull_out.with_columns([(pl.col(col).fill_null(0).cast(pl.Float64) / 3600).alias(col) for col in sec_to_hr])\n",
    "merged_ramco = converted_time.join(RAMCO[['EID','Date','ramco_marked']], left_on=['Date_Converted','OracleID'],right_on=['Date','EID'], how='left')\n",
    "merged_ramco_ot = merged_ramco.join(RAMCO_OT[['EID','Date',\"nsa_authorize\",\"ot_authorize\",\"hours\",\"ot_status\",\"ot_type\"]], left_on=['Date_Converted','OracleID'],right_on=['Date','EID'], how='left')\n",
    "\n",
    "from datetime import datetime\n",
    "today = datetime.now().date()\n",
    "added_today = merged_ramco_ot.with_columns(pl.lit(today).cast(pl.Date).alias(\"Today\"))\n",
    "\n",
    "added_days_count = added_today.with_columns([\n",
    "    ((pl.col(\"Today\") - pl.col(\"Date_Converted\")).cast(pl.Float64) / (24 * 60 * 60 * 1000)).alias(\"days_count\")\n",
    "])\n",
    "\n",
    "nm_group = added_days_count.with_columns([\n",
    "    pl.when((pl.col(\"ramco_marked\") == \"NM\"))\n",
    "    .then(\n",
    "        pl.when(pl.col(\"days_count\").is_between(0, 2)).then(pl.lit(\"00-02 Days\"))\n",
    "        .when(pl.col(\"days_count\").is_between(3, 5)).then(pl.lit(\"03-05 Days\"))\n",
    "        .when(pl.col(\"days_count\").is_between(6, 10)).then(pl.lit(\"06-10 Days\"))\n",
    "        .when(pl.col(\"days_count\").is_between(11, 30)).then(pl.lit(\">10 Days\"))\n",
    "        .when(pl.col(\"days_count\")> 30).then(pl.lit(\">30 Days Unapproved\"))\n",
    "        .otherwise(None)\n",
    "    )\n",
    "    .otherwise(None).alias(\"nm_group\")\n",
    "])\n",
    "\n",
    "nsa_group = nm_group.with_columns([\n",
    "    pl.when((pl.col(\"ot_type\").str.contains(\"NSA\")) & (pl.col(\"nsa_authorize\") == \"Pending\"))\n",
    "    .then(\n",
    "        pl.when(pl.col(\"days_count\").is_between(0, 2)).then(pl.lit(\"00-02 Days\"))\n",
    "        .when(pl.col(\"days_count\").is_between(3, 5)).then(pl.lit(\"03-05 Days\"))\n",
    "        .when(pl.col(\"days_count\").is_between(6, 10)).then(pl.lit(\"06-10 Days\"))\n",
    "        .when(pl.col(\"days_count\").is_between(11, 30)).then(pl.lit(\">10 Days\"))\n",
    "        .when(pl.col(\"days_count\")> 30).then(pl.lit(\">30 Days Unapproved\"))\n",
    "        .otherwise(None)\n",
    "    )\n",
    "    .otherwise(None).alias(\"nsa_group\")\n",
    "])\n",
    "\n",
    "ot_group = nsa_group.with_columns([\n",
    "    pl.when((pl.col(\"ot_type\").str.contains(\"OT\")) & (pl.col(\"ot_authorize\") == \"Pending\"))\n",
    "    .then(\n",
    "        pl.when(pl.col(\"days_count\").is_between(0, 2)).then(pl.lit(\"00-02 Days\"))\n",
    "        .when(pl.col(\"days_count\").is_between(3, 5)).then(pl.lit(\"03-05 Days\"))\n",
    "        .when(pl.col(\"days_count\").is_between(6, 10)).then(pl.lit(\"06-10 Days\"))\n",
    "        .when(pl.col(\"days_count\").is_between(11, 30)).then(pl.lit(\">10 Days\"))\n",
    "        .when(pl.col(\"days_count\")> 30).then(pl.lit(\">30 Days Unapproved\"))\n",
    "        .otherwise(None)\n",
    "    )\n",
    "    .otherwise(None).alias(\"ot_group\")\n",
    "])\n",
    "\n",
    "RTA_REPORT = ot_group\n",
    "RTA_REPORT = RTA_REPORT.sort(['Date_Converted', 'start'], descending=[False, True])\n",
    "RTA_REPORT = RTA_REPORT.unique()\n",
    "\n",
    "print(RTA_REPORT.filter(\n",
    "    (pl.col('Date_Converted').cast(pl.Date) == pl.lit('2025-05-12').str.strptime(pl.Date, \"%Y-%m-%d\")) & \n",
    "    (pl.col('Shift Tracking') == 'Off')\n",
    ").select(['Date_Converted', 'Email Id', 'Shift Tracking']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huuchinh.nguyen\\AppData\\Local\\Temp\\ipykernel_11912\\348970879.py:27: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  (pl.arange(0, pl.count()).over([\"Date_Converted\", \"IEX ID\", \"Unique_Count\"]) + 1).alias(\"Custom\")).with_columns(pl.col('Custom')- 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (665_975, 19)\n",
      "┌──────┬────────┬─────────────┬─────────────┬───┬─────────────┬────────────┬────────────┬──────────┐\n",
      "│ Year ┆ Month  ┆ Week_Monday ┆ Date_Conver ┆ … ┆ Agent State ┆ Datetime_S ┆ Datetime_E ┆ Duration │\n",
      "│ ---  ┆ ---    ┆ ---         ┆ ted         ┆   ┆ ---         ┆ tart_Time  ┆ nd_Time    ┆ ---      │\n",
      "│ str  ┆ str    ┆ str         ┆ ---         ┆   ┆ str         ┆ ---        ┆ ---        ┆ f64      │\n",
      "│      ┆        ┆             ┆ date        ┆   ┆             ┆ datetime[μ ┆ datetime[μ ┆          │\n",
      "│      ┆        ┆             ┆             ┆   ┆             ┆ s]         ┆ s]         ┆          │\n",
      "╞══════╪════════╪═════════════╪═════════════╪═══╪═════════════╪════════════╪════════════╪══════════╡\n",
      "│ 2025 ┆ May-25 ┆ 2025-05-12  ┆ 2025-05-15  ┆ … ┆ Available   ┆ 2025-05-15 ┆ 2025-05-15 ┆ 0.5      │\n",
      "│      ┆        ┆             ┆             ┆   ┆ Chat        ┆ 07:00:00   ┆ 07:30:00   ┆          │\n",
      "│ 2025 ┆ Apr-25 ┆ 2025-04-07  ┆ 2025-04-08  ┆ … ┆ Offline     ┆ 2025-04-08 ┆ 2025-04-08 ┆ 0.230278 │\n",
      "│      ┆        ┆             ┆             ┆   ┆             ┆ 13:05:59   ┆ 13:19:48   ┆          │\n",
      "│ 2025 ┆ Apr-25 ┆ 2025-04-21  ┆ 2025-04-27  ┆ … ┆ Available   ┆ 2025-04-27 ┆ 2025-04-27 ┆ 0.460278 │\n",
      "│      ┆        ┆             ┆             ┆   ┆ Chat        ┆ 18:02:23   ┆ 18:30:00   ┆          │\n",
      "│ 2025 ┆ Mar-25 ┆ 2025-03-24  ┆ 2025-03-29  ┆ … ┆ Available   ┆ 2025-03-29 ┆ 2025-03-29 ┆ 0.18     │\n",
      "│      ┆        ┆             ┆             ┆   ┆ Chat        ┆ 14:49:12   ┆ 15:00:00   ┆          │\n",
      "│ 2025 ┆ Apr-25 ┆ 2025-03-31  ┆ 2025-04-05  ┆ … ┆ Available   ┆ 2025-04-05 ┆ 2025-04-05 ┆ 0.06     │\n",
      "│      ┆        ┆             ┆             ┆   ┆ Chat        ┆ 06:26:24   ┆ 06:30:00   ┆          │\n",
      "│ …    ┆ …      ┆ …           ┆ …           ┆ … ┆ …           ┆ …          ┆ …          ┆ …        │\n",
      "│ 2025 ┆ Mar-25 ┆ 2025-03-03  ┆ 2025-03-04  ┆ … ┆ Outbound    ┆ 2025-03-04 ┆ 2025-03-04 ┆ 0.050278 │\n",
      "│      ┆        ┆             ┆             ┆   ┆ Call        ┆ 08:56:59   ┆ 09:00:00   ┆          │\n",
      "│ 2025 ┆ Mar-25 ┆ 2025-03-24  ┆ 2025-03-29  ┆ … ┆ Available   ┆ 2025-03-29 ┆ 2025-03-29 ┆ 0.460278 │\n",
      "│      ┆        ┆             ┆             ┆   ┆ Chat        ┆ 13:31:47   ┆ 13:59:24   ┆          │\n",
      "│ 2025 ┆ Mar-25 ┆ 2025-03-24  ┆ 2025-03-30  ┆ … ┆ Available   ┆ 2025-03-30 ┆ 2025-03-30 ┆ 0.410278 │\n",
      "│      ┆        ┆             ┆             ┆   ┆ Chat        ┆ 12:05:23   ┆ 12:30:00   ┆          │\n",
      "│ 2025 ┆ May-25 ┆ 2025-05-05  ┆ 2025-05-10  ┆ … ┆ Available   ┆ 2025-05-10 ┆ 2025-05-10 ┆ 0.31     │\n",
      "│      ┆        ┆             ┆             ┆   ┆ Chat        ┆ 21:41:24   ┆ 22:00:00   ┆          │\n",
      "│ 2025 ┆ Jun-25 ┆ 2025-06-02  ┆ 2025-06-03  ┆ … ┆ End of      ┆ 2025-06-04 ┆ 2025-06-04 ┆ 0.01     │\n",
      "│      ┆        ┆             ┆             ┆   ┆ Shift       ┆ 07:01:48   ┆ 07:02:24   ┆          │\n",
      "└──────┴────────┴─────────────┴─────────────┴───┴─────────────┴────────────┴────────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "AGENT_ACTIVITY_SPLIT = ACTIVITY_DATE_CONVERTED.with_columns(pl.col('Start Time').dt.date().alias('Start Date'))\n",
    "AGENT_ACTIVITY_SPLIT = AGENT_ACTIVITY_SPLIT.with_columns(pl.col('End Time').dt.date().alias('End Date'))\n",
    "split_table = AGENT_ACTIVITY_SPLIT[['Year','Month','Week_Monday','Date_Converted','Date','Employee Name','Email Id','OracleID','IEX ID','LOB','Wave','Datetime_Fluctuate_Start_Shift','Datetime_Fluctuate_End_Shift','Fluctuate Shift','Start Date','End Date','Agent State','Start Time','End Time']]\n",
    "split_table = split_table.rename({'Start Time': 'Datetime_Start_Action', 'End Time': 'Datetime_End_Action'})\n",
    "split_table = split_table.with_columns([((pl.col(f'Datetime_{col}_Action').dt.hour() + pl.col(f'Datetime_{col}_Action').dt.minute() / 60 + pl.col(f'Datetime_{col}_Action').dt.second() / 3600).round(2)).alias(f'{col}_Time_Hours') for col in ['Start', 'End']])\n",
    "split_table = split_table.with_columns(pl.when(pl.col('End_Time_Hours') < pl.col('Start_Time_Hours')).then(pl.col('End_Time_Hours') + 24).otherwise(pl.col('End_Time_Hours')).alias('End_Time_Hours'))\n",
    "for col in ['Start', 'End']:\n",
    "    split_table = split_table.with_columns(\n",
    "        pl.when(pl.col(f\"{col}_Time_Hours\") * 2 % 1 == 0)\n",
    "          .then(pl.col(f\"{col}_Time_Hours\").round(1))\n",
    "          .otherwise((pl.col(f\"{col}_Time_Hours\") * 2).floor() / 2)\n",
    "          .alias(f\"{col}_Time_Half_Rounding\")\n",
    "    )\n",
    "split_table = split_table.with_columns(\n",
    "    pl.when(\n",
    "        pl.col('End_Time_Half_Rounding').is_not_nan() & \n",
    "        pl.col('Start_Time_Half_Rounding').is_not_nan()\n",
    ").then((pl.col('End_Time_Half_Rounding') - pl.col('Start_Time_Half_Rounding')) * 2).otherwise(None).cast(pl.Int64).alias('Number_Split'))\n",
    "split_table = split_table.with_columns(pl.col('Start_Time_Half_Rounding').alias('Time_1'))\n",
    "split_table = split_table.with_columns((pl.col('Time_1') + 0.5).alias('Time_2') )\n",
    "split_table = split_table.with_columns(\n",
    "    pl.col('Number_Split').cast(pl.Int32).repeat_by(pl.col('Number_Split')).alias('replicate')\n",
    ")\n",
    "split_table = split_table.explode('replicate')\n",
    "split_table = split_table.with_columns(\n",
    "    pl.concat_str([pl.col(\"Date_Converted\").cast(pl.Utf8), pl.col(\"IEX ID\").cast(pl.Utf8), pl.col(\"Datetime_Start_Action\").cast(pl.Utf8)], separator=\" - \").alias(\"Unique_Count\")).with_columns(\n",
    "        (pl.arange(0, pl.count()).over([\"Date_Converted\", \"IEX ID\", \"Unique_Count\"]) + 1).alias(\"Custom\")).with_columns(pl.col('Custom')- 1)\n",
    "split_table = split_table.with_columns((pl.col('Time_1') + pl.col('Custom') * 0.5).alias('Time_1_New'))\n",
    "split_table = split_table.with_columns((pl.col('Time_2') + pl.col('Custom') * 0.5).alias('Time_2_New'))\n",
    "split_table = split_table.with_columns((((pl.col('End_Time_Hours') * 2).floor() + 1) / 2).cast(pl.Float32).alias('Rounded_End_Time_Hours'))\n",
    "split_table = split_table.with_columns(\n",
    "    pl.when(pl.col(\"Time_1_New\") > pl.col(\"Start_Time_Hours\")).then(pl.col(\"Time_1_New\")).otherwise(pl.col(\"Start_Time_Hours\")).alias(\"Start_Time_Number\"),\n",
    "    pl.when(pl.col(\"Time_2_New\") < pl.col(\"End_Time_Hours\")).then(pl.col(\"Time_2_New\")).otherwise(pl.col(\"End_Time_Hours\")).alias(\"End_Time_Number\")\n",
    ")\n",
    "split_table = split_table.with_columns(\n",
    "    pl.when(pl.col('Start Date') == pl.col('End Date')).then(pl.col('Start Date')).otherwise(\n",
    "        pl.when((pl.col('Start_Time_Number') >= 24) & (pl.col('Start Date') != pl.col('End Date'))).then(pl.col('Start Date') + pl.duration(days=1)).otherwise(pl.col('Start Date'))\n",
    "    ).alias('New_Day_Start_Time')\n",
    ")\n",
    "split_table = split_table.with_columns(pl.when(pl.col('End_Time_Number') < pl.col('Start_Time_Number')).then(pl.col('End Date') - pl.duration(days=1)).otherwise(pl.col('End Date')).alias('New_Day_End_Time'))\n",
    "for col in ['Start', 'End']:\n",
    "    split_table = split_table.with_columns(\n",
    "        ((pl.col(f'{col}_Time_Number') - 24 * (pl.col(f'{col}_Time_Number') >= 24).cast(pl.Int32)) % 24).alias(f'{col}_Time_Number')\n",
    "    )\n",
    "def number_to_time(number):\n",
    "    if number is None:\n",
    "        return None\n",
    "    hours = int(np.floor(number))\n",
    "    minutes = int(np.floor((number - hours) * 60))\n",
    "    seconds = int(np.floor((number - hours - minutes / 60) * 3600))\n",
    "    return f\"{hours:02}:{minutes:02}:{seconds:02}\"\n",
    "def number_to_time(df, col):\n",
    "    hours = (df[col] // 1).cast(pl.Int32)\n",
    "    minutes = ((df[col] % 1) * 60).cast(pl.Int32)\n",
    "    seconds = (((df[col] % 1) * 60) % 1 * 60).cast(pl.Int32)\n",
    "    return (pl.concat_str([hours.cast(pl.Utf8).str.zfill(2),minutes.cast(pl.Utf8).str.zfill(2),seconds.cast(pl.Utf8).str.zfill(2)], separator=':'))\n",
    "for col in ['Start', 'End']:\n",
    "    split_table = split_table.with_columns(\n",
    "        number_to_time(split_table, f'{col}_Time_Number').alias(f'{col}_Time')\n",
    "    )\n",
    "split_table = split_table.with_columns(pl.when(pl.col('Time_1_New') >= 24).then(pl.col('Time_1_New')-24).otherwise(pl.col('Time_1_New')).alias('Intervals'))\n",
    "split_table = split_table.with_columns(number_to_time(split_table, 'Intervals'))\n",
    "\n",
    "def hms_to_duration(df: pl.DataFrame, column: str) -> pl.DataFrame:\n",
    "    return df.with_columns([\n",
    "        pl.col(column).str.split(\":\").alias(f\"{column}_parts\")\n",
    "    ]).with_columns([\n",
    "        (pl.col(f\"{column}_parts\").list.get(0).cast(pl.Int64) * pl.duration(hours=1) +\n",
    "         pl.col(f\"{column}_parts\").list.get(1).cast(pl.Int64) * pl.duration(minutes=1) +\n",
    "         pl.col(f\"{column}_parts\").list.get(2).cast(pl.Int64) * pl.duration(seconds=1)\n",
    "        ).alias(f\"{column}_duration\")\n",
    "    ]).drop(f\"{column}_parts\")\n",
    "\n",
    "split_table = hms_to_duration(split_table, \"Intervals\")\n",
    "\n",
    "split_table = split_table.with_columns([\n",
    "    (pl.concat_str([pl.col(\"New_Day_Start_Time\"), pl.lit(\" \"), pl.col(\"Start_Time\")]).str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"Datetime_Start_Time\")),\n",
    "    (pl.concat_str([pl.col(\"New_Day_End_Time\"), pl.lit(\" \"), pl.col(\"End_Time\")]).str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"Datetime_End_Time\"))\n",
    "])\n",
    "split_table = split_table.filter(pl.col(\"Datetime_Start_Time\") < pl.col(\"Datetime_End_Time\"))\n",
    "# split_table = split_table['Year','Month','Week Begin','Date_Converted','Start Date','Employee Name','OracleID','IEX ID','Wave','LOB','Agent State','Intervals','Datetime_Fluctuate_Start_Shift','Datetime_Fluctuate_End_Shift','Fluctuate Shift','Datetime_Start_Time','Datetime_End_Time']\n",
    "split_table = split_table.group_by(['Year','Month','Week_Monday','Date_Converted','Start Date','OracleID','IEX ID','Employee Name','Email Id','LOB','Wave','Datetime_Fluctuate_Start_Shift','Datetime_Fluctuate_End_Shift','Fluctuate Shift','Intervals','Agent State']).agg(\n",
    "    [\n",
    "        pl.col('Datetime_Start_Time').min().cast(pl.Datetime),\n",
    "        pl.col('Datetime_End_Time').max().cast(pl.Datetime) \n",
    "    ])\n",
    "split_table = split_table.with_columns(\n",
    "    ((pl.col(\"Datetime_End_Time\") - pl.col(\"Datetime_Start_Time\")).dt.total_seconds() / 3600).alias('Duration')\n",
    ")\n",
    "print(split_table)\n",
    "RTA_EXTEND = split_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = RTA_REPORT.columns\n",
    "# data_types = [str(RTA_REPORT[col].dtype) for col in columns]\n",
    "\n",
    "# schema_df = pd.DataFrame({\n",
    "#     'Tên Cột': columns,\n",
    "#     'Kiểu Dữ Liệu': data_types\n",
    "# })\n",
    "\n",
    "# schema_df.to_excel('output.xlsx', index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Week_Monday, group in RTA_REPORT.group_by('Week_Monday'):\n",
    "    Week_Monday_str = Week_Monday[0]\n",
    "    file_name = f'{Week_Monday_str}.xlsx'\n",
    "    file_path = os.path.join(folder_paths[\"rta_output\"], file_name)\n",
    "    group.write_excel(file_path)\n",
    "\n",
    "for Week_Monday, group in RTA_EXTEND.group_by('Week_Monday'):\n",
    "    Week_Monday_str = Week_Monday[0]\n",
    "    file_name = f'{Week_Monday_str}.xlsx'\n",
    "    file_path = os.path.join(folder_paths[\"rta_intervals_output\"], file_name)\n",
    "    group.write_excel(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
