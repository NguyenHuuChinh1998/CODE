{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "import openpyxl\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import csv\n",
    "import shutil\n",
    "import pyautogui\n",
    "from PIL import ImageStat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VARIABLE P2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_start_date_P2 = '2023-11-20'\n",
    "qa_end_date_P2 = '2023-11-26'\n",
    "mod_end_date_P2 = '2023-11-25'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUTOMATICALLY DOWNLOAD QA DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def move(x,y, duration):\n",
    "#     return pyautogui.moveTo(x,y, duration = 1)\n",
    "# def click(x,y,n):\n",
    "#     return pyautogui.click(x,y, duration = 1,clicks=n)\n",
    "# def sleep(sec):\n",
    "#     return time.sleep(sec)\n",
    "# def write(text):\n",
    "#     return pyautogui.write(text)\n",
    "# def calculate_distance(pos1, pos2):\n",
    "#     return ((pos1[0] - pos2[0]) ** 2 + (pos1[1] - pos2[1]) ** 2) ** 0.5\n",
    "# def check_png(picture_name_or_location, n, dur=0.8):\n",
    "#     if isinstance(picture_name_or_location, str):\n",
    "#         location = pyautogui.locateOnScreen(picture_name_or_location, confidence=0.92)\n",
    "#     else:\n",
    "#         location = picture_name_or_location\n",
    "    \n",
    "#     l, t, w, h = location\n",
    "#     center_x = l + w/2\n",
    "#     center_y = t + h/2\n",
    "#     pyautogui.moveTo(center_x, center_y, duration=dur)\n",
    "#     pyautogui.click(center_x, center_y, duration=dur, clicks=n)     \n",
    "# def check_png_right(picture_name, n =1, dur = 1):\n",
    "#     queue_locate = pyautogui.locateOnScreen( picture_name, confidence=0.92)\n",
    "#     l = queue_locate[0]\n",
    "#     t = queue_locate[1]\n",
    "#     w = queue_locate[2]\n",
    "#     h = queue_locate[3]\n",
    "#     pyautogui.moveTo(l+w-1, t+h/2,duration = dur)\n",
    "#     pyautogui.click(l+w-1, t+h/2,duration = dur, clicks = n)  \n",
    "# def get_cell_center(x_start, y_start, cell_width, cell_height, row, col):\n",
    "#     center_x = x_start + col * cell_width + cell_width / 2\n",
    "#     center_y = y_start + row * cell_height + cell_height / 2\n",
    "#     return (center_x, center_y)\n",
    "# EMPTY_COLOR = (255, 255, 255)\n",
    "# def is_empty_cell(x, y, cell_width,cell_height,tolerance=10):\n",
    "#     cell_img = pyautogui.screenshot(region=(x, y, cell_width, cell_height))\n",
    "#     avg_color = ImageStat.Stat(cell_img).mean\n",
    "#     return all(abs(a-b) < tolerance for a, b in zip(avg_color, EMPTY_COLOR))\n",
    "\n",
    "# img_dict = {\n",
    "#     'png': '{}.png',\n",
    "\n",
    "#     #daily export\n",
    "#     'TCS':'TCS.png',\n",
    "#     'zoom':'zoom.png',\n",
    "#     'download':'download.png',\n",
    "#     'f5':'f5.png',\n",
    "#     'filter':'filter.png',\n",
    "#     'export':'export.png',\n",
    "#     'start_export':'start_export.png',\n",
    "    \n",
    "#     #daily download\n",
    "#     'tcs_downloading': 'tcs_downloading.png',\n",
    "#     'download_click_to_link': 'download_click_to_link.png',\n",
    "#     '3_dot': '3_dot.png',\n",
    "#     'download_as': 'download_as.png',\n",
    "#     'download_sheets_as': 'download_sheets_as.png',\n",
    "#     'csv': 'csv.png',\n",
    "#     'tcs_page': 'tcs_page.png',\n",
    "#     'my_export': 'my_export.png',\n",
    "#     'status': 'status.png',\n",
    "\n",
    "#     #daily export sample size\n",
    "#     'batch_export':'batch_export.png',\n",
    "#     'select_all':'sellect_all.png',\n",
    "#     'rock_export':'rock_export.png',\n",
    "#     'rock_export_2':'rock_export_2.png',\n",
    "#     'unname_file':'unname_file.png',\n",
    "#     'confirm_name':'confirm_name.png',\n",
    "    \n",
    "#     #daily export diff case\n",
    "#     'batch_export_case_statistic':'batch_export_case_statistic.png',\n",
    "#     'sellect_all':'sellect_all.png',\n",
    "#     'rock_export':'rock_export.png',\n",
    "#     'tick_no':'tick_no.png',\n",
    "#     'rock_export_2':'rock_export_2.png',\n",
    "#     'unname_file':'unname_file.png',\n",
    "#     'confirm_name':'confirm_name.png',\n",
    "#     'move_to_page':'move_to_page.png',\n",
    "    \n",
    "#     'tcs_close':'tcs_close.png',\n",
    "#     'rock':'rock.png',\n",
    "#     'rock_my_export':'rock_my_export.png',\n",
    "#     'rock_my_export_2':'rock_my_export_2.png',\n",
    "#     'rock_to_page':'rock_to_page.png',\n",
    "#     'rock_download':'rock_download.png',\n",
    "#     'case_statistic':'case_statistic.png',\n",
    "#     'qa_live_slice_appeal_audit':'qa_live_slice_appeal_audit.png',\n",
    "\n",
    "#     'lark_form_annouce':'lark_form_annouce.png',\n",
    "#     'lark_form':'lark_form.png'\n",
    "\n",
    "# }\n",
    "\n",
    "# #Download TCS daily \n",
    "# time.sleep(1)\n",
    "# check_png(img_dict['TCS'],1)\n",
    "# time.sleep(1)\n",
    "# check_png(img_dict['download'],1)\n",
    "# x_start = 45  #55\n",
    "# y_start = 363\n",
    "# cell_width = 200\n",
    "# cell_height = 45\n",
    "# num_rows = 6\n",
    "# num_cols = 1\n",
    "# export_times = 0\n",
    "# time.sleep(10)\n",
    "# for row in range(num_rows):\n",
    "#     for col in range(num_cols):\n",
    "#         center_x, center_y = get_cell_center(x_start, y_start, cell_width, cell_height, row, col)\n",
    "#         if is_empty_cell(center_x, center_y,cell_width,cell_height):\n",
    "#             continue\n",
    "#         pyautogui.move(center_x, center_y, duration=1)\n",
    "#         pyautogui.click(center_x, center_y)\n",
    "#         time.sleep(8)\n",
    "#         try:\n",
    "#             check_png(img_dict['export'], 1)\n",
    "#             time.sleep(2)\n",
    "#             if pyautogui.locateOnScreen(img_dict['start_export']):\n",
    "#                 check_png(img_dict['start_export'], 1)\n",
    "#                 export_times += 1  \n",
    "#                 print(str(export_times))\n",
    "#         except:\n",
    "#             pass\n",
    "#         move(456, 21, 1)\n",
    "#         click(456, 21, 1)\n",
    "#         # move(13, 13, 1)\n",
    "#         # click(13, 13, 1)\n",
    "#         time.sleep(1)\n",
    "\n",
    "# #Download sample size daily\n",
    "# x_start = 643\n",
    "# y_start = 363\n",
    "# cell_width = 200\n",
    "# cell_height = 45\n",
    "# num_rows = 8\n",
    "# num_cols = 1\n",
    "# days_passed = 2\n",
    "# now = datetime.now()\n",
    "# for row in range(num_rows):\n",
    "#     for col in range(num_cols):\n",
    "#         current_date = (now - timedelta(days=days_passed)).strftime(\"%Y_%m_%d\")\n",
    "#         center_x, center_y = get_cell_center(x_start, y_start, cell_width, cell_height, row, col)\n",
    "#         time.sleep(2)\n",
    "#         if is_empty_cell(center_x, center_y, cell_width, cell_height):\n",
    "#             continue    \n",
    "#         pyautogui.move(center_x, center_y, duration=1)\n",
    "#         pyautogui.click(center_x, center_y)\n",
    "#         time.sleep(8)\n",
    "#         try:\n",
    "#             check_png(img_dict['batch_export'], 1)\n",
    "#             time.sleep(2)\n",
    "#             check_png(img_dict['select_all'], 1)\n",
    "#             time.sleep(2)\n",
    "#             check_png(img_dict['rock_export'], 1)\n",
    "#             time.sleep(5)\n",
    "#             check_png(img_dict['rock_export_2'], 1)\n",
    "#             time.sleep(2)\n",
    "#             check_png(img_dict['unname_file'], 2)\n",
    "#             pyautogui.typewrite(current_date)\n",
    "#             check_png(img_dict['confirm_name'], 1)\n",
    "#         except: pass\n",
    "#         move(456, 21, 1)\n",
    "#         click(456, 21, 1)\n",
    "#         # move(13, 13, 1)\n",
    "#         # click(13, 13, 1)\n",
    "#         time.sleep(1)\n",
    "#         days_passed += 1\n",
    "# time.sleep(2)\n",
    "\n",
    "# #Download Rock case statistic daily\n",
    "# coord_list_rock_case_daily= [(550, 391)]\t\t\t\t\t\n",
    "# now = datetime.now()\n",
    "# for i, coord in enumerate(coord_list_rock_case_daily):\n",
    "#     current_date = (now - timedelta(days=i+1)).strftime(\"%Y_%m_%d\")\n",
    "#     pyautogui.moveTo(coord[0], coord[1], duration=2)\n",
    "#     pyautogui.click(coord[0], coord[1])\n",
    "#     time.sleep(5)\n",
    "#     try:\n",
    "#         check_png(img_dict['batch_export_case_statistic'],1)\n",
    "#         time.sleep(1)\n",
    "#         check_png(img_dict['sellect_all'],1)\n",
    "#         time.sleep(1)\n",
    "#         check_png(img_dict['rock_export'],1)\n",
    "#         time.sleep(2)\n",
    "#         pyautogui.scroll(2000)\n",
    "#         time.sleep(2)\n",
    "#         check_png(img_dict['tick_no'],1)\n",
    "#         pyautogui.scroll(-2000)\n",
    "#         time.sleep(2)\n",
    "#         check_png(img_dict['rock_export_2'],1)\n",
    "#         time.sleep(1)\n",
    "#         check_png(img_dict['unname_file'],2)\n",
    "#         time.sleep(1)\n",
    "#         pyautogui.typewrite(current_date)\n",
    "#         check_png(img_dict['confirm_name'],1)\n",
    "#         time.sleep(1)\n",
    "#     except: pass\n",
    "#     move(456, 21, 1)\n",
    "#     click(456, 21, 1)\n",
    "#     # move(13, 13, 1)\n",
    "#     # click(13, 13, 1)\n",
    "#     time.sleep(2)\n",
    "\n",
    "# # Extract TCS rawdata to folder \n",
    "# check_png(img_dict['tcs_downloading'],1)\n",
    "# time.sleep(6)\n",
    "# check_png(img_dict['my_export'],1)\n",
    "# time.sleep(2)\n",
    "# flag = False\n",
    "# click_count = 0\n",
    "# while click_count < export_times:\n",
    "#     click_count = 0\n",
    "#     for i in range(1, 4):\n",
    "#         time.sleep(2)\n",
    "#         check_png(img_dict['png'].format(i), 1)\n",
    "#         time.sleep(5)\n",
    "#         checkBoxNumber = pyautogui.locateAllOnScreen(img_dict['download_click_to_link'], confidence=0.91)\n",
    "#         for each_pos in checkBoxNumber:\n",
    "#             time.sleep(0.1)\n",
    "#             check_png(each_pos,1)\n",
    "#             time.sleep(8)\n",
    "#             try:\n",
    "#                 dots = list(pyautogui.locateAllOnScreen(img_dict['3_dot'], confidence=0.90))\n",
    "#                 mouse_position = pyautogui.position()\n",
    "#                 closest_dot = min(dots, key=lambda pos: calculate_distance(mouse_position, (pos.left+pos.width/2, pos.top+pos.height/2)))\n",
    "#                 check_png(closest_dot, 1)\n",
    "#                 check_png(img_dict['download_as'],1)\n",
    "#                 check_png(img_dict['download_sheets_as'],1)\n",
    "#                 check_png(img_dict['csv'],1)\n",
    "#             except: pass\n",
    "#             move(456, 21, 1)\n",
    "#             click(456, 21, 1)\n",
    "#             # move(13,13,1)\n",
    "#             # click(13,13,1)\n",
    "#             time.sleep(2)\n",
    "#             click_count += 1\n",
    "#             print(\"Click to links: \", click_count)\n",
    "#             if click_count >= export_times:\n",
    "#                 flag = True\n",
    "#                 break\n",
    "#         if flag:\n",
    "#             break\n",
    "\n",
    "# # Extract Rock rawdata to folder \n",
    "# time.sleep(2)\n",
    "# check_png(img_dict['rock'],1)\n",
    "# time.sleep(4)\n",
    "# check_png(img_dict['rock_my_export'],1)\n",
    "# time.sleep(10)\n",
    "# pyautogui.scroll(-180)\n",
    "# time.sleep(5)\n",
    "# checkBoxNumber_rock = pyautogui.locateAllOnScreen(img_dict['rock_download'], confidence=0.91)\n",
    "# count = 0\n",
    "# for each_pos_rock in checkBoxNumber_rock:\n",
    "#     time.sleep(0.1)\n",
    "#     check_png(each_pos_rock,1)\n",
    "#     time.sleep(2)\n",
    "#     move(456, 21, 1)\n",
    "#     click(456, 21, 1)\n",
    "#     count += 1\n",
    "#     if count == 8:\n",
    "#         break\n",
    "# move(103, 554, 1)\n",
    "# click(103, 554, 1)\n",
    "# click(103, 554, 1)\n",
    "# time.sleep(2)\n",
    "# check_png(img_dict['rock_my_export_2'],1)\n",
    "# time.sleep(2)\n",
    "# first_rock_position = pyautogui.locateOnScreen(img_dict['rock_download'], confidence=0.90)\n",
    "# check_png(first_rock_position, 1)\n",
    "# time.sleep(2)\n",
    "# move(456, 21, 1)\n",
    "# click(456, 21, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defination function\n",
    "def convert_datetime(a):\n",
    "    my_format=\"%Y-%m-%d %H:%M:%S\"\n",
    "    date_formats = [\"%Y-%m-%d %H:%M:%S.%f\", \n",
    "                    \"%Y-%m-%d %H:%M:%S\", \n",
    "                    \"%m/%d/%Y %H:%M\",\n",
    "                    \"%m-%d-%Y %H:%M\", \n",
    "                    \"%m/%d/%Y\", \n",
    "                    \"%Y-%m-%d\", \n",
    "                    \"%H:%M.%f\",\n",
    "                    \"%m/%d/%Y %H:%M:%S\"] \n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            return datetime.strptime(str(a), fmt).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    try:\n",
    "        return (datetime(1899, 12, 30) + timedelta(days=float(a))).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    except ValueError:\n",
    "        pass\n",
    "    if not re.match(\"\\d+:\\d+:\\d+(\\.\\d+)?\", str(a)):\n",
    "        return str(a)\n",
    "    return  \n",
    "def convert_date(a):\n",
    "    date_formats = [\"%Y-%m-%d %H:%M:%S.%f\", \n",
    "                    \"%Y-%m-%d %H:%M:%S\", \n",
    "                    \"%m/%d/%Y %H:%M\",\n",
    "                    \"%m-%d-%Y %H:%M\", \n",
    "                    \"%m/%d/%Y\", \n",
    "                    \"%Y-%m-%d\", \n",
    "                    \"%H:%M.%f\",\n",
    "                    \"%m/%d/%Y %H:%M:%S\"] \n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            return datetime.strptime(str(a), fmt).strftime(\"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    try:\n",
    "        return (datetime(1899, 12, 30) + timedelta(days=float(a))).strftime(\"%Y-%m-%d\")\n",
    "    except ValueError:\n",
    "        pass\n",
    "    if not re.match(\"\\d+:\\d+:\\d+(\\.\\d+)?\", str(a)):\n",
    "        return str(a)\n",
    "    return  \n",
    "def convert_to_datetime(struct_time):\n",
    "    return datetime(*struct_time[:6])\n",
    "def input_data(data_dir):\n",
    "    list_files = []\n",
    "    for filename in pathlib.Path(data_dir).glob('**/*.*'):\n",
    "        file_suffixes = filename.suffixes\n",
    "        if file_suffixes[-1].lower() in ['.xlsx', '.csv']:\n",
    "            if file_suffixes[-1].lower() == '.xlsx':\n",
    "                export_time = os.path.getmtime(filename)\n",
    "                export_time_datetime = convert_to_datetime(time.localtime(export_time))\n",
    "                filename_with_export_time = filename.with_suffix('.xlsx')\n",
    "                dfs = pd.read_excel(filename, sheet_name=None, skiprows=0, na_values=None, dtype={'Moderation Queue ID':object,'Object ID':object,'Task id':object,'Sampling Task ID':object})\n",
    "                for sheet_name, df in dfs.items():\n",
    "                    df['sheet_name'] = sheet_name\n",
    "                    df['Export time'] = export_time_datetime\n",
    "                    list_files.append(df)\n",
    "            elif file_suffixes[-1].lower() == '.csv':\n",
    "                export_time = os.path.getmtime(filename)\n",
    "                export_time_datetime = convert_to_datetime(time.localtime(export_time))\n",
    "                df = pd.read_csv(filename, skiprows=0, na_values=None, dtype={'Moderation Queue ID':object,'Object ID':object,'Task id':object,'Sampling Task ID':object,'task_id':object})\n",
    "                df['sheet_name'] = 'CSV'\n",
    "                df['Export time'] = export_time_datetime\n",
    "                list_files.append(df)\n",
    "    df_list = pd.concat(list_files, axis=0, ignore_index=True)\n",
    "    return df_list\n",
    "def merge_csv_files(folder_path):\n",
    "    file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "    dfs = []\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, dtype={'task_id': object,'project_id':object,'object_id':object})\n",
    "            df[['project_id','task_id','object_id']] = df[['project_id','task_id','object_id']].replace(to_replace=r'id=',value= '',regex=True).astype(str)\n",
    "            df['source_file'] = os.path.basename(file_path)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "    merged_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    return merged_df\n",
    "def rename_files(data_dir, column_name, string_value):\n",
    "    for filename in pathlib.Path(data_dir).glob('*.csv'):\n",
    "        if string_value and string_value in filename.name:\n",
    "            continue\n",
    "        with open(filename, 'r', encoding='utf-8') as csv_file:\n",
    "            rows = csv.reader(csv_file, delimiter=',')\n",
    "            headers = next(rows)\n",
    "            sheet = {col: [] for col in headers}\n",
    "            for row in rows:\n",
    "                for i, col in enumerate(headers):\n",
    "                    sheet[col].append(row[i])\n",
    "        if not sheet[column_name] or len(sheet[column_name]) < 2:\n",
    "            continue\n",
    "        date_list = []\n",
    "        for date_str in sheet[column_name]:\n",
    "            date = datetime.datetime.strptime(date_str.split()[0], '%Y-%m-%d').date()\n",
    "            date_list.append(date)\n",
    "        date_list.sort(reverse=True)\n",
    "        start_date = date_list[-1].strftime('%Y-%m-%d')\n",
    "        end_date = date_list[0].strftime('%Y-%m-%d')\n",
    "        title = sheet['title'][0].strip()\n",
    "        new_file_name = f\"{title}_{start_date}_{end_date}_temp.csv\"\n",
    "        new_filepath = os.path.join(data_dir, new_file_name)\n",
    "        new_filepath = os.path.normpath(new_filepath)\n",
    "\n",
    "        if os.path.exists(new_filepath):\n",
    "            os.remove(new_filepath)\n",
    "\n",
    "        os.replace(filename, new_filepath)\n",
    "        os.replace(new_filepath, os.path.join(data_dir, f\"{title}_{start_date}_{end_date}.csv\"))\n",
    "def check_file(file_path, target_string, search_column):\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            data = pd.read_csv(file_path)\n",
    "            if search_column in data.columns:\n",
    "                    if data[search_column].str.strip().eq(target_string.strip()).any():\n",
    "                        return True\n",
    "                    elif data[search_column].astype(str).str.strip().str.contains(target_string).any():\n",
    "                        return True\n",
    "\n",
    "        elif file_path.endswith(('.xls', '.xlsx')):\n",
    "            data_sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "            for sheet_name, data in data_sheets.items():\n",
    "                if search_column in data.columns:\n",
    "                    if data[search_column].str.strip().eq(target_string.strip()).any():\n",
    "                        return True\n",
    "                    elif data[search_column].astype(str).str.strip().str.contains(target_string).any():\n",
    "                        return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "def rename_files_peoplestatistic(data_dir):\n",
    "    for filename in pathlib.Path(data_dir).glob('*.xlsx'):\n",
    "        if \"People daily d\" in filename.name:\n",
    "            continue\n",
    "        with pd.ExcelFile(filename) as xlsx:\n",
    "            df = pd.read_excel(xlsx, sheet_name=None, skiprows=0)\n",
    "        sheet_name = next(iter(df))\n",
    "        sheet = df[sheet_name]\n",
    "        date_str = sheet.at[1, 'Selected duration'].split('~')[0].strip()[:10]\n",
    "        date = datetime.datetime.strptime(date_str, '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "        new_file_name = \"People daily d \" + date + \".xlsx\"\n",
    "        new_filepath = os.path.join(filename.parent, new_file_name)\n",
    "        \n",
    "        i = 1\n",
    "        while os.path.exists(new_filepath):\n",
    "            os.remove(new_filepath)\n",
    "            \n",
    "            new_file_name = f\"People daily d {date}.xlsx\"\n",
    "            new_filepath = os.path.join(filename.parent, new_file_name)\n",
    "            i += 1\n",
    "        \n",
    "        os.rename(filename, new_filepath)\n",
    "def rename_files_casestatistic(data_dir):\n",
    "    for filename in pathlib.Path(data_dir).glob('*.xlsx'):\n",
    "        if \"Rock rawdata daily d\" in filename.name:\n",
    "            continue\n",
    "        with pd.ExcelFile(filename) as xlsx:\n",
    "            df = pd.read_excel(xlsx, sheet_name=None, skiprows=0)\n",
    "        sheet_name = next(iter(df))\n",
    "        sheet = df[sheet_name]\n",
    "        print(filename.name)\n",
    "        date_str = sheet.at[1, 'Selected duration'].split('~')[0].strip()[:10]\n",
    "        date = datetime.datetime.strptime(date_str, '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "        new_file_name = \"Rock rawdata daily d \" + date + \".xlsx\"\n",
    "        new_filepath = os.path.join(filename.parent, new_file_name)\n",
    "        \n",
    "        i = 1\n",
    "        while os.path.exists(new_filepath):\n",
    "            os.remove(new_filepath)\n",
    "            \n",
    "            new_file_name = f\"Rock rawdata daily d {date}.xlsx\"\n",
    "            new_filepath = os.path.join(filename.parent, new_file_name)\n",
    "            i += 1\n",
    "        \n",
    "        os.rename(filename, new_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start date</th>\n",
       "      <th>End date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>appeal_diff_case</th>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>2023-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appeal_dailyreport</th>\n",
       "      <td>2023-11-25</td>\n",
       "      <td>2023-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regular_diff_case</th>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>2023-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regular_false_case</th>\n",
       "      <td>2023-12-02</td>\n",
       "      <td>2023-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rock_dailyreport</th>\n",
       "      <td>2023-11-25</td>\n",
       "      <td>2023-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCR_diff_case</th>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>2023-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCR_false_case</th>\n",
       "      <td>2023-11-25</td>\n",
       "      <td>2023-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sideprj_shoptab</th>\n",
       "      <td>2023-12-03</td>\n",
       "      <td>2023-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sideprj_ugads</th>\n",
       "      <td>2023-12-03</td>\n",
       "      <td>2023-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sideprj_dailyreport</th>\n",
       "      <td>2023-11-25</td>\n",
       "      <td>2023-12-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Start date    End date\n",
       "appeal_diff_case     2023-11-27  2023-12-03\n",
       "appeal_dailyreport   2023-11-25  2023-12-02\n",
       "regular_diff_case    2023-11-27  2023-12-03\n",
       "regular_false_case   2023-12-02  2023-12-02\n",
       "rock_dailyreport     2023-11-25  2023-12-02\n",
       "PCR_diff_case        2023-11-27  2023-12-03\n",
       "PCR_false_case       2023-11-25  2023-12-02\n",
       "sideprj_shoptab      2023-12-03  2023-12-03\n",
       "sideprj_ugads        2023-12-03  2023-12-03\n",
       "sideprj_dailyreport  2023-11-25  2023-12-02"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Variable date\n",
    "import datetime\n",
    "\n",
    "# Đọc tệp excel\n",
    "Read_Excel_datavariable = openpyxl.load_workbook('Support_dashboard.xlsx')\n",
    "Read_Data_Variable = Read_Excel_datavariable['Variable']\n",
    "\n",
    "Read_Date_variable = pd.ExcelFile('Support_dashboard.xlsx')\n",
    "Date_variable = pd.read_excel(Read_Date_variable, sheet_name='Variable', dtype={'Start date': object, 'End date': object})\n",
    "\n",
    "current_date = datetime.datetime.now().date()\n",
    "one_day_ago = current_date - datetime.timedelta(days=1)\n",
    "two_days_ago = current_date - datetime.timedelta(days=2)\n",
    "eight_days_ago = current_date - datetime.timedelta(days=9)\n",
    "\n",
    "if current_date.weekday() == 0:\n",
    "    monday_cycle_temp = pd.to_datetime(current_date - datetime.timedelta(days=7)).date()\n",
    "else:\n",
    "    monday_cycle_temp = pd.to_datetime(current_date - datetime.timedelta(days=current_date.weekday())).date()\n",
    "\n",
    "monday_cycle = monday_cycle_temp.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "date_dict = {\n",
    "    'appeal_diff_case': [monday_cycle, one_day_ago],\n",
    "    'appeal_dailyreport': [eight_days_ago, two_days_ago],\n",
    "    'regular_diff_case': [monday_cycle, one_day_ago],\n",
    "    'regular_false_case': [two_days_ago, two_days_ago],\n",
    "    'rock_dailyreport': [eight_days_ago, two_days_ago],\n",
    "    'PCR_diff_case': [monday_cycle, one_day_ago],\n",
    "    'PCR_false_case': [eight_days_ago, two_days_ago],\n",
    "    'sideprj_shoptab': [one_day_ago, one_day_ago],\n",
    "    'sideprj_ugads': [one_day_ago, one_day_ago],\n",
    "    'sideprj_dailyreport': [eight_days_ago, two_days_ago]\n",
    "}\n",
    "from datetime import datetime, timedelta\n",
    "# Create DataFrame from dict\n",
    "date_df = pd.DataFrame.from_dict(date_dict, orient='index', columns=['Start date', 'End date'])\n",
    "date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supporting Dashboard\n",
    "\n",
    "#Variable\n",
    "Date_variable = date_df.copy()\n",
    "Start_Date_False_Regular = datetime.strptime(str(Date_variable.loc['PCR_false_case', 'Start date']), '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "End_Date_False_Regular = datetime.strptime(str(Date_variable.loc['PCR_false_case', 'End date']), '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "\n",
    "Start_Date_Diff_Regular = datetime.strptime(str(Date_variable.loc['regular_diff_case', 'Start date']), '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "End_Date_Diff_Regular = datetime.strptime(str(Date_variable.loc['regular_diff_case', 'End date']), '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "\n",
    "Start_Date_False_Appeal = datetime.strptime(str(Date_variable.loc['appeal_diff_case', 'Start date']), '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "End_Date_False_Appeal = datetime.strptime(str(Date_variable.loc['appeal_diff_case', 'End date']), '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "\n",
    "Start_Date_DailyRP_Appeal = datetime.strptime(str(Date_variable.loc['appeal_dailyreport', 'Start date']), '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "End_Date_DailyRP_Appeal = datetime.strptime(str(Date_variable.loc['appeal_dailyreport', 'End date']), '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "\n",
    "Start_Date_False_PCR = datetime.strptime(str(Date_variable.loc['PCR_false_case', 'Start date']), '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "End_Date_False_PCR = datetime.strptime(str(Date_variable.loc['PCR_false_case', 'End date']), '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "\n",
    "Start_Date_Diff_PCR = datetime.strptime(str(Date_variable.loc['PCR_diff_case', 'Start date']), '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "End_Date_Diff_PRC = datetime.strptime(str(Date_variable.loc['PCR_diff_case', 'End date']), '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "\n",
    "Start_Date_Daily_Report_People_Statistic = datetime.strptime(str(Date_variable.loc['rock_dailyreport', 'Start date']), '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "End_Date_Daily_Report_People_Statistic = datetime.strptime(str(Date_variable.loc['rock_dailyreport', 'End date']), '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "#Queue\n",
    "queue_list = pd.read_excel('Support_dashboard.xlsx',sheet_name='Queue List',dtype={'MOD QUEUE ID':object,'QA QUEUE ID':object}).values\n",
    "queue_list = pd.DataFrame(queue_list)\n",
    "headers = ['BUSINESS','COMPOUND','CATEGORY','MOD QUEUE ID','MOD QUEUE NAME',\n",
    "           'QA QUEUE ID','QA QUEUE NAME','QUEUE GROUP','STATUS','Priority','Latency','AHT (secs)','Policy_type','Classify']\n",
    "queue_list.columns = headers\n",
    "\n",
    "#Rename Column Name Rock-Case statistic\n",
    "reColumn_name = pd.read_excel('Support_dashboard.xlsx',sheet_name='Column_format')\n",
    "\n",
    "#Move\n",
    "Move_files = pd.read_excel('Support_dashboard.xlsx',sheet_name='Move_Files',dtype={'String':object})\n",
    "\n",
    "#Policy Errors\n",
    "policy_errors_list = pd.read_excel('Support_dashboard.xlsx',sheet_name='Policy Errors').values\n",
    "policy_errors_list = [item for sublist in policy_errors_list for item in sublist]\n",
    "\n",
    "aliases = {\n",
    "    \"Not Approve\": ['\"audit_status\":400','\"status\":400','\":400,\"','400','disapproval', 'disapproved','not approval','not approve' ,'notapproval','disapprove','not approved','violation','reject'],\n",
    "    \"Approve\": ['\"audit_status\":200','approve','\"status\":200' ,'approval', 'approved','200','general']\n",
    "}\n",
    "aliases_final_decision = {\"Failed appeal\":['giveupappealing','failedappeal','failed appeal','mod wrong','mods wrong'] ,\n",
    "            \"Edge case\":['edgecase','edge case', 'borderline'],\n",
    "            \"Appeal successfully\":['appealsuccessfully','appeal successfully','qa wrong','qas wrong','mod correct'],\n",
    "            }\n",
    "#Data full alternation\n",
    "data_full_alternation = pd.read_excel('linemanager_full.xlsx')\n",
    "data_full_alternation = data_full_alternation[data_full_alternation['Role'].str.contains('Operator')]\n",
    "data_full_alternation = data_full_alternation[['EffectDate','Email','LineManager']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start date</th>\n",
       "      <th>End date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>appeal_diff_case</th>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>2023-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appeal_dailyreport</th>\n",
       "      <td>2023-11-25</td>\n",
       "      <td>2023-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regular_diff_case</th>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>2023-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regular_false_case</th>\n",
       "      <td>2023-12-02</td>\n",
       "      <td>2023-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rock_dailyreport</th>\n",
       "      <td>2023-11-25</td>\n",
       "      <td>2023-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCR_diff_case</th>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>2023-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCR_false_case</th>\n",
       "      <td>2023-11-25</td>\n",
       "      <td>2023-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sideprj_shoptab</th>\n",
       "      <td>2023-12-03</td>\n",
       "      <td>2023-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sideprj_ugads</th>\n",
       "      <td>2023-12-03</td>\n",
       "      <td>2023-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sideprj_dailyreport</th>\n",
       "      <td>2023-11-25</td>\n",
       "      <td>2023-12-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Start date    End date\n",
       "appeal_diff_case     2023-11-27  2023-12-03\n",
       "appeal_dailyreport   2023-11-25  2023-12-02\n",
       "regular_diff_case    2023-11-27  2023-12-03\n",
       "regular_false_case   2023-12-02  2023-12-02\n",
       "rock_dailyreport     2023-11-25  2023-12-02\n",
       "PCR_diff_case        2023-11-27  2023-12-03\n",
       "PCR_false_case       2023-11-25  2023-12-02\n",
       "sideprj_shoptab      2023-12-03  2023-12-03\n",
       "sideprj_ugads        2023-12-03  2023-12-03\n",
       "sideprj_dailyreport  2023-11-25  2023-12-02"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Date_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_and_strings = Move_files\n",
    "download_dir = 'Download daily files'\n",
    "\n",
    "if download_dir:\n",
    "    current_hour = datetime.now().hour\n",
    "\n",
    "    for _, row in paths_and_strings.iterrows():\n",
    "        target_dir = row['Direction']\n",
    "        target_string = row['String']\n",
    "        search_column = row['Find_column']\n",
    "        for file_name in os.listdir(download_dir):\n",
    "            if file_name.endswith('.csv') or file_name.endswith('.xlsx'):\n",
    "                file_path = os.path.join(download_dir, file_name)\n",
    "                file_hour = datetime.fromtimestamp(os.path.getmtime(file_path)).hour\n",
    "                if check_file(file_path, target_string, search_column):\n",
    "                    shutil.move(file_path, os.path.join(target_dir, file_name))\n",
    "else:\n",
    "    print('Download folder is empty. Skip moving files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RENAME FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "rename_files('Appeal daily','2_resolve_time','QA')\n",
    "rename_files('Product comment report/R1','1_resolve_time','VN')\n",
    "rename_files('Product comment report/R2','1_resolve_time','VN')\n",
    "rename_files('No_Anchor/R1','1_resolve_time','VN')\n",
    "rename_files('No_Anchor/R2','1_resolve_time','VN')\n",
    "# # rename_files_casestatistic('Rock daily/Daily')\n",
    "rename_files_peoplestatistic('People Daily')\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RENAME COLUMN NAME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openpyxl import load_workbook\n",
    "\n",
    "# def change_header_values(directory, reColumn_name):\n",
    "#     df = reColumn_name.copy()\n",
    "#     columns_to_use = df['Column_to_use'].tolist()\n",
    "#     columns_not_to_use = df['Column_not_to_use'].tolist()\n",
    "#     for filename in os.listdir(directory):\n",
    "#         if filename.endswith(\".xlsx\"):\n",
    "#             file_path = os.path.join(directory, filename)\n",
    "#             wb = load_workbook(file_path)\n",
    "#             for sheet_name in wb.sheetnames:\n",
    "#                 sheet = wb[sheet_name]\n",
    "#                 for row in sheet.iter_rows(min_row=1, max_row=1, values_only=True):\n",
    "#                     for i, cell_value in enumerate(row):\n",
    "#                         if cell_value in columns_not_to_use:\n",
    "#                             column_index = i + 1\n",
    "#                             sheet.cell(row=1, column=column_index, value=columns_to_use[columns_not_to_use.index(cell_value)])\n",
    "#             wb.save(file_path)                   \n",
    "# change_header_values('D:/qa_daily_automation/Rock daily/Daily',reColumn_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA ROCK CLEANING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input from Rock\n",
    "Rock_append_rawdata = input_data('Rock daily/Daily')\n",
    "Rock_append = Rock_append_rawdata.sort_values(by=['Export time'],ascending=True)\n",
    "Rock_append = Rock_append.drop_duplicates(subset='Sampling Task ID',keep='last')\n",
    "\n",
    "Rock_append = Rock_append[['Export time','Sampling Queue','Moderation Queue ID','Sampling Task ID','claimant','Moderation Time',\n",
    "                           'Object ID', 'Initial Moderation Result','Respondent','Moderation Time.1','Task id', \n",
    "                           'Task link','Sampling respondent Result','process result','process reason','Arbitrator',\n",
    "                           'Existing tags','process reason.2','Final Arbitrator','process result.4',\n",
    "                           'process result.1', 'process result.2', 'process result.3', 'process result.5','Correct Moderated Results','process reason.4','beginning time',\n",
    "                           'Appealing Result','Error towards']]\n",
    "Rock_append['Sampling Queue'] = Rock_append['Sampling Queue'].str.replace('QA 【UG-2R-GnE】TTS-vi-VN', 'QA [UG-2R-GnE] TTS-vi-VN')\n",
    "Rock_append = Rock_append.dropna(subset=['Moderation Time.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\4294748022.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_process_result[columns_to_replace] = data_process_result[columns_to_replace].astype(str).replace('---', np.nan)\n",
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\4294748022.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_process_result['Final process result'] = data_process_result.apply(r, axis=1)\n",
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\4294748022.py:51: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  Rock_append.loc[mask, columns] = Rock_append.loc[mask, columns].applymap(check_UG_Ads_result)\n",
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\4294748022.py:158: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'FALSE' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  Rock_append.loc[Rock_append['Mod Result'] != Rock_append['QA Result'],'Diff case']=\"FALSE\"\n",
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\4294748022.py:208: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'False Positive' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  Final_Rock_append.loc[(Final_Rock_append['QA Result'] == 'Approve') & (Final_Rock_append['Final process result'] == 'Failed appeal'),'Final False Error']=\"False Positive\"\n"
     ]
    }
   ],
   "source": [
    "# Xử lý process_result\n",
    "data_process_result = Rock_append[['Sampling Task ID','process result','process result.1','process result.2','process result.3','process result.4','process result.5']]\n",
    "columns_to_replace = ['process result.1', 'process result.2', 'process result.3', 'process result.4', 'process result.5']\n",
    "data_process_result[columns_to_replace] = data_process_result[columns_to_replace].astype(str).replace('---', np.nan)\n",
    "data_process_result.set_index('Sampling Task ID',inplace=True)\n",
    "def r(row):\n",
    "    last_index = row.last_valid_index()\n",
    "    return row[last_index] if last_index else np.nan\n",
    "data_process_result['Final process result'] = data_process_result.apply(r, axis=1)\n",
    "data_process_result.reset_index('Sampling Task ID',inplace=True)\n",
    "data_process_result = data_process_result[['Sampling Task ID','Final process result']]\n",
    "for k, v in aliases_final_decision.items():\n",
    "    pat = '|'.join(v)\n",
    "    data_process_result.loc[data_process_result['Final process result'].str.contains(pat, case=False),'Final process result'] = k\n",
    "data_process_result['Final process result'] = data_process_result['Final process result'].astype(str).replace('give up appealing ', 'Failed appeal')\n",
    "\n",
    "# Xử lý QA/Mod result của Product comment queue\n",
    "Rock_append[['Moderated Results','Moderation Result','Respondent Result']] = Rock_append[['Correct Moderated Results','Initial Moderation Result','Sampling respondent Result']].replace(to_replace=[r'^auditStatus: ',r'^rate: '],value= \"Audit Result: \",regex=True).astype(str)\n",
    "pcr_result = ['remove', 'normal', 'selfvisible']\n",
    "columns = ['Moderated Results', 'Moderation Result', 'Respondent Result']\n",
    "def match_and_concatenate(cell):\n",
    "    cell = str(cell).lower()\n",
    "    cell = cell.replace('_x000d_', '')\n",
    "    found_words = []\n",
    "    audit_exp = re.search(r'(.*auditreason:)(.*)(\\n|$)', cell)\n",
    "    reject_exp = re.search(r'(.*rejectlabel:)(.*)(\\n|$)', cell)\n",
    "    if audit_exp:\n",
    "        cell_split = audit_exp.group(2).strip()\n",
    "    elif reject_exp:\n",
    "        cell_split = reject_exp.group(2).strip()\n",
    "    else:\n",
    "        cell_split = cell\n",
    "    cell_split = re.sub(r'[\\[\\]\"]', '', cell_split)\n",
    "    matches = re.findall('|'.join(pcr_result), cell_split, re.IGNORECASE)\n",
    "    found_words.extend(matches)\n",
    "    return \",\".join(found_words) if found_words else None\n",
    "for column in ['Moderated Results', 'Moderation Result', 'Respondent Result']:\n",
    "    Rock_append.loc[Rock_append['Sampling Queue'].isin(['QA VN LL Product Comment', 'QA VN LL Product Comment Report']), column] = Rock_append.loc[Rock_append['Sampling Queue'].isin(['QA VN LL Product Comment', 'QA VN LL Product Comment Report']), column].apply(match_and_concatenate)\n",
    "\n",
    "# Xử lý QA/Mod result của UG_Ads queue\n",
    "def check_UG_Ads_result(row):\n",
    "    keywords = ['general', 'violation']\n",
    "    if any(keyword in row.lower() for keyword in keywords):\n",
    "        if 'general' in row.lower():\n",
    "            return 'Approve'\n",
    "        elif 'violation' in row.lower():\n",
    "            return 'Not Approve'\n",
    "    return row\n",
    "columns = ['Moderated Results', 'Moderation Result', 'Respondent Result']\n",
    "mask = Rock_append['Sampling Queue'].isin(['QA [UG-2R-GnE] TTS-vi-VN'])\n",
    "Rock_append.loc[mask, columns] = Rock_append.loc[mask, columns].applymap(check_UG_Ads_result)\n",
    "\n",
    "# Xử lý QA/Mod result của Regular + Appeal\n",
    "\n",
    "if (Rock_append['Sampling Queue'] == 'QA [VN] Seller Gov X Feige Chat').any():\n",
    "    columns_to_replace = ['Correct Moderated Results', 'Sampling respondent Result', 'Initial Moderation Result']\n",
    "    for column in columns_to_replace:\n",
    "        Rock_append[column] = Rock_append[column].str.replace('audit_reject: ', 'audit result: ')\n",
    "\n",
    "Rock_append['Correct Moderated Results'] = Rock_append['Correct Moderated Results'].str.lower().replace(to_replace=['audit_status','status','reject label'],value='audit result',regex=True)\n",
    "Rock_append['Sampling respondent Result'] = Rock_append['Sampling respondent Result'].str.lower().replace(to_replace=['audit_status','status','reject label'],value='audit result',regex=True)\n",
    "Rock_append['Initial Moderation Result'] = Rock_append['Initial Moderation Result'].str.lower().replace(to_replace=['audit_status','status','reject label'],value='audit result',regex=True)\n",
    "def process_cell(cell, aliases):\n",
    "    results = []\n",
    "    audit_results = re.findall(r'audit result: (.+)', cell, re.IGNORECASE)\n",
    "    for audit_result in audit_results:\n",
    "        for k, v in aliases.items():\n",
    "            pat = '|'.join(v)\n",
    "            if re.search(pat, audit_result, re.IGNORECASE):\n",
    "                results.append(k)\n",
    "    return ', '.join(results)\n",
    "\n",
    "Rock_append['Final Result'] = Rock_append['Correct Moderated Results'].apply(lambda x: process_cell(str(x), aliases))\n",
    "Rock_append['QA Result'] = Rock_append['Sampling respondent Result'].apply(lambda x: process_cell(str(x), aliases))\n",
    "Rock_append['Mod Result'] = Rock_append['Initial Moderation Result'].apply(lambda x: process_cell(str(x), aliases))\n",
    "Rock_append.loc[Rock_append['Final Result'] == '', 'Final Result'] = Rock_append.loc[Rock_append['Final Result'] == '', 'Moderated Results']\n",
    "Rock_append.loc[Rock_append['QA Result'] == '', 'QA Result'] = Rock_append.loc[Rock_append['QA Result'] == '', 'Respondent Result']\n",
    "Rock_append.loc[Rock_append['Mod Result'] == '', 'Mod Result'] = Rock_append.loc[Rock_append['Mod Result'] == '', 'Moderation Result']\n",
    "def process_cell_2(cell):\n",
    "    if cell is None:\n",
    "        return '---'\n",
    "    elif 'Not' in cell:\n",
    "        return 'Not Approve'\n",
    "    elif any(substring in cell for substring in ['normal', 'remove', 'selfvisible']):\n",
    "        return cell\n",
    "    else:\n",
    "        return 'Approve'\n",
    "Rock_append['Final Result'] = Rock_append['Final Result'].apply(process_cell_2)\n",
    "Rock_append['QA Result'] = Rock_append['QA Result'].apply(process_cell_2)\n",
    "Rock_append['Mod Result'] = Rock_append['Mod Result'].apply(process_cell_2)\n",
    "\n",
    "# Xử lý QA/Mod reason\n",
    "Rock_append['Correct Moderated Results'] = Rock_append['Correct Moderated Results'].str.lower()\n",
    "Rock_append['Sampling respondent Result'] = Rock_append['Sampling respondent Result'].str.lower()\n",
    "Rock_append['Initial Moderation Result'] = Rock_append['Initial Moderation Result'].str.lower()\n",
    "Policy_errors_1 = policy_errors_list.copy()\n",
    "Policy_errors_2 = list(map(lambda x: x.replace(\" \", \"_\") + \"0005\", Policy_errors_1))\n",
    "# Policy_errors_3 = list(map(lambda x: \"shoptab_\" + x.replace(\" \", \"_\"), Policy_errors_1))\n",
    "# Policy_errors_4 = list(map(lambda x: x.replace(\" \", \"_\") + \"_shoptab\", Policy_errors_1))\n",
    "Policy_errors = Policy_errors_1 + Policy_errors_2\n",
    "cols_to_check = [ 'Initial Moderation Result','Sampling respondent Result','Correct Moderated Results']\n",
    "cols_to_return = ['Mod Reason', 'QA Reason', 'Correct Policy']\n",
    "queue_policy_type = queue_list[['QA QUEUE NAME','Policy_type']]\n",
    "Rock_append = pd.merge(Rock_append,queue_policy_type, how='inner', left_on='Sampling Queue', right_on='QA QUEUE NAME')\n",
    "for col_idx, col in enumerate(cols_to_check):\n",
    "    Rock_append[cols_to_return[col_idx]] = ''\n",
    "    for i, row in Rock_append.iterrows():\n",
    "        cell_value = row[col]\n",
    "        policy_type = row['Policy_type']\n",
    "        if isinstance(cell_value, str):\n",
    "            if 'form_info' in cell_value:\n",
    "                form_info_index = cell_value.index('form_info')\n",
    "                cell_value = cell_value[form_info_index:]\n",
    "                match_list_1 = [match for match in Policy_errors_1 if match in cell_value]\n",
    "                match_list_2 = [match for match in Policy_errors_2 if match in cell_value]\n",
    "                if match_list_1 or match_list_2:\n",
    "                    match_list = list(OrderedDict.fromkeys(match_list_1 + match_list_2))\n",
    "                    final_reason = \", \".join(match_list).strip()\n",
    "                    Rock_append.at[i, cols_to_return[col_idx]] = final_reason\n",
    "            else:\n",
    "                if policy_type == 'Multi-choice':\n",
    "                    match_list_1 = [match for match in Policy_errors_1 if match in cell_value]\n",
    "                    match_list_2 = [match for match in Policy_errors_2 if match in cell_value]\n",
    "                    if match_list_1 or match_list_2:\n",
    "                        match_list = list(OrderedDict.fromkeys(match_list_1 + match_list_2))\n",
    "                        final_reason = \", \".join(match_list).strip()\n",
    "                        Rock_append.at[i, cols_to_return[col_idx]] = final_reason\n",
    "                elif policy_type == 'Single-choice':\n",
    "                    match_list_1 = []\n",
    "                    for match in Policy_errors_1:\n",
    "                        if match in cell_value:\n",
    "                            match_list_1.append(match)\n",
    "                            break\n",
    "                    if match_list_1:\n",
    "                        final_reason = match_list_1[0]\n",
    "                        Rock_append.at[i, cols_to_return[col_idx]] = final_reason\n",
    "        else:\n",
    "            continue\n",
    "Rock_append['Final policy errors'] = Rock_append[['Mod Reason', 'QA Reason', 'Correct Policy']].apply(lambda x: x[x != ''].loc[x[x != ''].last_valid_index()] if x[x != ''].last_valid_index() else '', axis=1)\n",
    "columns_to_process = ['Final policy errors', 'QA Reason', 'Mod Reason']\n",
    "for column in columns_to_process:\n",
    "    Rock_append[column] = Rock_append[column].replace(\"0005\", \"\", regex=True)\\\n",
    "        .replace(\"_\", \" \", regex=True)\\\n",
    "        .replace(\"counterfeit abnorma price product\", \"abnormal price\", regex=True)\\\n",
    "        .replace(\"incorrect category high risk\", \"incorrect category-high risk\", regex=True)\n",
    "\n",
    "Rock_append['Final policy errors'] = Rock_append['Final policy errors'].apply(lambda x: \", \".join(set(x.split(\", \"))))\n",
    "Rock_append['QA Reason'] = Rock_append['QA Reason'].apply(lambda x: \", \".join(set(x.split(\", \"))))\n",
    "Rock_append['Mod Reason'] = Rock_append['Mod Reason'].apply(lambda x: \", \".join(set(x.split(\", \"))))\n",
    "\n",
    "# Splitting the RCA labels\n",
    "try:\n",
    "    Rock_append[['RCA lvl1','RCA lvl2','RCA lvl3']] = Rock_append['Existing tags'].str.split(' > ', expand=True, n=2)\n",
    "except:\n",
    "    Rock_append[['RCA lvl1','RCA lvl2','RCA lvl3']] = np.nan\n",
    "\n",
    "#Add diff case\n",
    "Rock_append.loc[Rock_append['Mod Result'] != Rock_append['QA Result'],'Diff case']=\"FALSE\"\n",
    "Rock_append.loc[Rock_append['Diff case'].isnull(),'Diff case']=\"TRUE\"\n",
    "\n",
    "#Convert_time\n",
    "Rock_append['Moderation Time'] = Rock_append['Moderation Time'].apply(convert_datetime)\n",
    "Rock_append['Moderation Time'] = pd.to_datetime(Rock_append['Moderation Time'])\n",
    "Rock_append['Moderation Time.1'] = Rock_append['Moderation Time.1'].apply(convert_datetime)\n",
    "Rock_append['Moderation Time.1'] = pd.to_datetime(Rock_append['Moderation Time.1'])\n",
    "\n",
    "Final_Rock_append = Rock_append.copy()\n",
    "Final_Rock_append['Sampling Task ID'] = Rock_append['Sampling Task ID'].astype('int64').astype(str)\n",
    "Final_Rock_append = pd.merge(Final_Rock_append,data_process_result,how='inner',on=['Sampling Task ID'])\n",
    "Final_Rock_append[['Object ID','Sampling Task ID','Moderation Queue ID']] = Final_Rock_append[['Object ID','Sampling Task ID','Moderation Queue ID']].replace(to_replace=r'id=',value= '',regex=True).astype(str)\n",
    "\n",
    "#AHT\n",
    "Rock_append_aht = input_data('AHT Rock')\n",
    "Rock_append_aht.drop_duplicates(subset=['task_id'], inplace=True)\n",
    "Rock_append_aht[['task_id','1_duration']]\n",
    "Rock_append_aht = Rock_append_aht[['task_id','1_duration']]\n",
    "Final_Rock_append = pd.merge(Final_Rock_append,Rock_append_aht,how='left',left_on='Task id',right_on='task_id')\n",
    "\n",
    "#FN FP of the Product comment report\n",
    "def convert_to_number(df, mod_number, qa_number):\n",
    "    priority = {\n",
    "        \"remove\": 3,\n",
    "        \"selfvisible\": 2,\n",
    "        \"normal\": 1,\n",
    "        \"Approve\": 0,\n",
    "        \"Not Approve\": 0,\n",
    "        \"---\":0}\n",
    "    df['mod_convert_number'] = 0\n",
    "    df['qa_convert_number'] = 0\n",
    "    if df['Sampling Queue'].isin([\"QA VN LL Product Comment\", \"QA VN LL Product Comment Report\"]).any():\n",
    "        df['mod_convert_number'] = df[mod_number].apply(lambda x: int(''.join(str(priority[v]) for v in x.split(','))))\n",
    "        df['qa_convert_number'] = df[qa_number].apply(lambda x: int(''.join(str(priority[v]) for v in x.split(','))))\n",
    "    return df\n",
    "def add_zeros(x, max_length):\n",
    "    x = str(x)\n",
    "    while len(x) < max_length:\n",
    "        x += '0'\n",
    "    return int(x)\n",
    "def add_zeros_to_shorter(df):\n",
    "    max_length = max(df['mod_convert_number'].astype(str).apply(len).max(), df['qa_convert_number'].astype(str).apply(len).max())\n",
    "    df['mod_convert_number'] = df['mod_convert_number'].apply(add_zeros, args=(max_length,))\n",
    "    df['qa_convert_number'] = df['qa_convert_number'].apply(add_zeros, args=(max_length,))\n",
    "    return df\n",
    "Final_Rock_append = convert_to_number(Final_Rock_append, 'Mod Result','QA Result')\n",
    "Final_Rock_append = add_zeros_to_shorter(Final_Rock_append)\n",
    "\n",
    "# Xử lý FN/FP của Regular/Appeal\n",
    "Final_Rock_append.loc[(Final_Rock_append['QA Result'] == 'Approve') & (Final_Rock_append['Final process result'] == 'Failed appeal'),'Final False Error']=\"False Positive\"\n",
    "Final_Rock_append.loc[Final_Rock_append['Final False Error'].isnull(),'Final False Error']=\"False Negative\"\n",
    "Final_Rock_append.loc[(Final_Rock_append['Mod Result'] == 'Approve') & (Final_Rock_append['Final process result'] == 'Appeal successfully'),'Final False Error']=\"False Positive\"\n",
    "Final_Rock_append.loc[Final_Rock_append['Final False Error'].isnull(),'Final False Error']=\"False Negative\"\n",
    "Final_Rock_append.loc[(Final_Rock_append['QA Result'] == 'Approve') & (Final_Rock_append['Final process result'] == 'Edge case'),'Final False Error']=\"False Positive\"\n",
    "Final_Rock_append.loc[Final_Rock_append['Final False Error'].isnull(),'Final False Error']=\"False Negative\"\n",
    "Final_Rock_append.loc[(Final_Rock_append['qa_convert_number'] != 0)&(Final_Rock_append['mod_convert_number'] > Final_Rock_append['qa_convert_number'])&(Final_Rock_append['Final process result'] == 'Failed appeal'),'Final False Error']=\"False Positive\"\n",
    "Final_Rock_append.loc[(Final_Rock_append['qa_convert_number'] != 0)&(Final_Rock_append['mod_convert_number'] < Final_Rock_append['qa_convert_number'])&(Final_Rock_append['Final process result'] == 'Failed appeal'),'Final False Error']=\"False Negative\"\n",
    "Final_Rock_append.loc[(Final_Rock_append['qa_convert_number'] != 0)&(Final_Rock_append['mod_convert_number'] > Final_Rock_append['qa_convert_number'])&(Final_Rock_append['Final process result'] == 'Edge case'),'Final False Error']=\"False Positive\"\n",
    "Final_Rock_append.loc[(Final_Rock_append['qa_convert_number'] != 0)&(Final_Rock_append['mod_convert_number'] < Final_Rock_append['qa_convert_number'])&(Final_Rock_append['Final process result'] == 'Edge case'),'Final False Error']=\"False Negative\"\n",
    "Final_Rock_append.loc[(Final_Rock_append['qa_convert_number'] != 0)&(Final_Rock_append['mod_convert_number'] > Final_Rock_append['qa_convert_number'])&(Final_Rock_append['Final process result'] == 'Appeal successfully'),'Final False Error']=\"False Negative\"\n",
    "Final_Rock_append.loc[(Final_Rock_append['qa_convert_number'] != 0)&(Final_Rock_append['mod_convert_number'] < Final_Rock_append['qa_convert_number'])&(Final_Rock_append['Final process result'] == 'Appeal successfully'),'Final False Error']=\"False Positive\"\n",
    "Final_Rock_append.loc[Final_Rock_append['process result'] == '---','Final False Error']=\"---\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n",
      "58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\639928903.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  False_case_rock['Moderation Time'] = pd.to_datetime(False_case_rock['Moderation Time'].apply(convert_date))\n"
     ]
    }
   ],
   "source": [
    "# Output from Rock statistic\n",
    "# Output diff case from Rock statistic\n",
    "list_error = ['Edge case', 'Failed appeal', 'Appeal successfully']\n",
    "Final_Rock_append['Final process result'] = Final_Rock_append.apply(lambda row: row['Error towards'] if row['Error towards'] in list_error else row['Final process result'], axis=1)\n",
    "Final_Rock_append = Final_Rock_append[['Sampling Queue','claimant','Moderation Time','Object ID','Mod Result', \n",
    "                   'Mod Reason','Respondent','Moderation Time.1','Sampling Task ID', 'QA Result', 'QA Reason','Diff case','process result',\n",
    "                   'process reason','Arbitrator','Final process result','Final False Error','Final policy errors','RCA lvl1','RCA lvl2','RCA lvl3','process reason.2',\n",
    "                   'Final Arbitrator','Final Result','process result.4','process reason.4', '1_duration']].sort_values(ascending=True,by='Moderation Time.1')\n",
    "def filter_time( df,  from_date, to_date):\n",
    "    Final_Rock_append = df[(df['Moderation Time.1'].dt.strftime('%Y-%m-%d') >= from_date) & (df['Moderation Time.1'].dt.strftime('%Y-%m-%d') <= to_date)]\n",
    "    Final_Rock_append = Final_Rock_append[Final_Rock_append['claimant'].str.contains('@trans-cosmos.com.vn')]\n",
    "    Final_Rock_append.to_excel(f\"Rock_Rawdata_Daily/Rock merged d {from_date}{to_date}.xlsx\", index=False)\n",
    "    return Final_Rock_append\n",
    "Regular_diff = filter_time(Final_Rock_append, Start_Date_Diff_Regular, End_Date_Diff_Regular)\n",
    "print(Regular_diff['Sampling Queue'].count())\n",
    "print(Regular_diff[~Regular_diff['Final process result'].isin(['Failed appeal', 'Edge case','Appeal successfully'])]['Sampling Task ID'].count())\n",
    "def filter_time( df,  from_date, to_date, max_mod_date):\n",
    "    Final_Rock_append = df[(df['Moderation Time.1'].dt.strftime('%Y-%m-%d') >= from_date) & (df['Moderation Time.1'].dt.strftime('%Y-%m-%d') <= to_date) & (df['Moderation Time'].dt.strftime('%Y-%m-%d') <= max_mod_date)]\n",
    "    Final_Rock_append = Final_Rock_append[Final_Rock_append['claimant'].str.contains('@trans-cosmos.com.vn')]\n",
    "    Final_Rock_append.to_excel(f\"Rock_Rawdata_Daily/Rock merged d {from_date}{to_date}.xlsx\", index=False)\n",
    "    return Final_Rock_append\n",
    "Regular_diff_P2 = filter_time(Final_Rock_append, qa_start_date_P2, qa_end_date_P2, mod_end_date_P2)\n",
    "\n",
    "# Output sample size from Rock statistic\n",
    "Rock_merged = input_data('Rock_Rawdata_Daily')\n",
    "Rock_merged['Export time'].sort_values(ascending=True)\n",
    "Rock_merged = Rock_merged.drop_duplicates(subset='Sampling Task ID',keep='last')\n",
    "\n",
    "def filter_time( df,  from_date, to_date):\n",
    "\n",
    "    False_case_rock = df[(df['Moderation Time'].dt.strftime('%Y-%m-%d') >= from_date) & (df['Moderation Time'].dt.strftime('%Y-%m-%d') <= to_date)]\n",
    "    False_case_rock['Moderation Time'] = pd.to_datetime(False_case_rock['Moderation Time'].apply(convert_date))\n",
    "    # False_case_rock = False_case_rock[False_case_rock['Final process result'] == \"Failed appeal\"]\n",
    "    False_case_rock = False_case_rock.groupby(by=['Moderation Time','Sampling Queue','claimant'], as_index=False).agg({'Sampling Task ID':'count'})\n",
    "    False_case_rock = False_case_rock.rename(columns={'Sampling Task ID':'No. Diff cases',\n",
    "                                                      'Moderation Time':'Moderation time',\n",
    "                                                      'Sampling Queue':'Sampling Queue',\n",
    "                                                      'claimant':'Moderator name'})\n",
    "    False_case_rock.to_excel(f\"Diff cases {from_date}{to_date}.xlsx\", index=False)\n",
    "\n",
    "    return False_case_rock, from_date, to_date\n",
    "# Fill in moderation time (TODAY - 2)\n",
    "False_rock, start_date, end_date = filter_time(Rock_merged, Start_Date_False_Regular, End_Date_False_Regular)\n",
    "\n",
    "# Lưu lại các dataframe tái sử dụng\n",
    "all_sheets_regular = {'Diff_case_Regular':Regular_diff,\n",
    "                      'False_case_Regular':False_rock}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**APPEAL & 2 ROUND QUEUES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\2837956653.py:45: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'FALSE' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  qa_raw.loc[qa_raw['Mod Result'] != qa_raw['QA Result'],'Diff case']=\"FALSE\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-27 2023-12-03\n"
     ]
    }
   ],
   "source": [
    "#Input from TCS\n",
    "qa_raw = merge_csv_files('Appeal daily')\n",
    "qa_raw['1_verifier'] = qa_raw['1_verifier'].str.replace('robot_estimate@','')\n",
    "# mod_raw = merge_csv_files('D:/Appeal mod')\n",
    "# mod_raw = mod_raw[['object_id','1_verifier']]\n",
    "qa_raw = qa_raw[qa_raw['2_resolve_time'].notnull()]\n",
    "# qa_raw = pd.merge(qa_raw,mod_raw,how='left',on='object_id')\n",
    "# qa_raw = qa_raw.drop('1_verifier_x', axis=1).rename(columns={'1_verifier_y':'1_verifier'})\n",
    "qa_raw['1_verify_data'] = qa_raw['1_verify_data'].str.replace('audit_reject', '')\n",
    "qa_raw['2_verify_data'] = qa_raw['2_verify_data'].str.replace('audit_reject', '')\n",
    "qa_raw['1_reason'] = qa_raw['1_reason'].str.lower()\n",
    "qa_raw['2_reason'] = qa_raw['2_reason'].str.lower()\n",
    "\n",
    "def check_string(s):\n",
    "    if isinstance(s, str):\n",
    "        s_lower = s.lower()  \n",
    "        for approval_status, substrings in aliases.items():\n",
    "            if any(substring in s_lower for substring in substrings):\n",
    "                return approval_status\n",
    "    return s \n",
    "qa_raw['Mod Result'] = qa_raw['1_verify_data'].apply(check_string)\n",
    "qa_raw['QA Result'] = qa_raw['2_verify_data'].apply(check_string)\n",
    "def d(row):\n",
    "    last_index = row.last_valid_index()\n",
    "    return row[last_index] if last_index else np.nan\n",
    "#Final Policy Errors\n",
    "qa_raw['Policy Error'] = qa_raw[['1_reason','2_reason']].apply(d, axis=1)\n",
    "Policy_errors = policy_errors_list.copy()\n",
    "qa_raw['Policy Error'] = qa_raw['Policy Error'].str.lower()\n",
    "Policy_errors_pattern = \"|\".join(Policy_errors )\n",
    "def pattern_searcher(search_str:str, search_list:str):\n",
    "    search_obj = re.search(search_list, search_str)\n",
    "    if search_obj :\n",
    "        return_str = search_str[search_obj.start(): search_obj.end()]\n",
    "    else:\n",
    "        return_str = np.nan\n",
    "    return return_str\n",
    "qa_raw['Policy Error'] = qa_raw['Policy Error'].astype(str).apply(lambda y: pattern_searcher(search_str=y, search_list=Policy_errors_pattern))\n",
    "### convert time\n",
    "qa_raw['2_resolve_time'] = qa_raw['2_resolve_time'].apply(convert_datetime)\n",
    "qa_raw['1_resolve_time'] = qa_raw['1_resolve_time'].apply(convert_datetime)\n",
    "qa_raw['2_resolve_time'] = pd.to_datetime(qa_raw['2_resolve_time'])\n",
    "qa_raw['1_resolve_time'] = pd.to_datetime(qa_raw['1_resolve_time'])\n",
    "#Add diff case\n",
    "qa_raw.loc[qa_raw['Mod Result'] != qa_raw['QA Result'],'Diff case']=\"FALSE\"\n",
    "qa_raw.loc[qa_raw['Diff case'].isnull(),'Diff case']=\"TRUE\"\n",
    "#Remove dup cases\n",
    "qa_raw = qa_raw.drop_duplicates(subset='task_id',keep='first')\n",
    "# cleaning\n",
    "def filter_time( df,  from_date, to_date):\n",
    "    appeal_rawdata = df[(df['2_resolve_time'].dt.strftime('%Y-%m-%d') >= from_date) & (df['2_resolve_time'].dt.strftime('%Y-%m-%d') <= to_date) ]\n",
    "    filer_tcv = appeal_rawdata['1_verifier'].str.contains('@trans-cosmos.com.vn | *',na=False)\n",
    "    appeal_rawdata = appeal_rawdata[filer_tcv].dropna(axis=0, subset = ['1_verifier'])\n",
    "    appeal_rawdata = appeal_rawdata[['title','project_id', '1_verifier','1_resolve_time','object_id','Mod Result','1_reason','1_duration','2_verifier','2_resolve_time','task_id','tcs_link','QA Result','2_reason','Diff case','Policy Error']]\n",
    "    #appeal_rawdata = appeal_rawdata[appeal_rawdata['Diff case']==\"FALSE\"]\n",
    "    return appeal_rawdata, from_date,to_date\n",
    "Appeal_diff_case,Final_appeal_rawdata_from_date,Final_appeal_rawdata_to_date = filter_time(qa_raw, Start_Date_False_Appeal,End_Date_False_Appeal)\n",
    "Appeal_diff_case_P2,Final_appeal_rawdata_from_date_P2,Final_appeal_rawdata_to_date_P2 = filter_time(qa_raw, qa_start_date_P2,qa_end_date_P2)\n",
    "print(Final_appeal_rawdata_from_date, Final_appeal_rawdata_to_date)\n",
    "Appeal_diff_case['title'].value_counts()\n",
    "# sample size and diff case\n",
    "def filter_time_2( df, from_date, to_date):\n",
    "    qa_raw['1_resolve_time'] = pd.to_datetime(qa_raw['1_resolve_time'])\n",
    "    appeal_rawdata_2 = df[(df['1_resolve_time'].dt.strftime('%Y-%m-%d') >= from_date) & (df['1_resolve_time'].dt.strftime('%Y-%m-%d') <= to_date) ]\n",
    "    filer_tcv = appeal_rawdata_2['1_verifier'].str.contains('@trans-cosmos.com.vn',na=False)\n",
    "    appeal_rawdata_2 = appeal_rawdata_2[filer_tcv].dropna(axis=0, subset = ['1_verifier'])\n",
    "    appeal_rawdata_2 = appeal_rawdata_2[['title','project_id', '1_verifier','1_resolve_time','object_id','Mod Result','1_reason','1_duration','2_verifier','2_resolve_time','task_id','tcs_link','QA Result','2_reason']]\n",
    "    return appeal_rawdata_2\n",
    "Final_appeal_rawdata_2 = filter_time_2(qa_raw, Start_Date_DailyRP_Appeal,End_Date_DailyRP_Appeal)\n",
    "Final_appeal_rawdata_2['1_resolve_time'] = pd.to_datetime(Final_appeal_rawdata_2['1_resolve_time'].apply(convert_date))\n",
    "Sample_size_appeal = Final_appeal_rawdata_2.groupby(by=['1_resolve_time','title','1_verifier'],as_index=False).agg({'task_id':'count'})\n",
    "Sample_size_appeal = Sample_size_appeal.rename(columns={'1_resolve_time':'Moderation time', \n",
    "                                          'title':'Sampling Queue', \n",
    "                                           '1_verifier':'Moderator name', \n",
    "                                          'task_id':'No. of Samples'})\n",
    "\n",
    "all_sheets_appeal = {'Diff_case_Appeal': Appeal_diff_case,\n",
    "                     'Sample_size_Appeal': Sample_size_appeal}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRODUCT COMMENT REPORT (MERGE R1&R2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['task_id_qa', 'project_id_qa', 'title_qa', 'object_id', 'status_qa',\n",
      "       'create_time_qa', 'closed_time_qa', '1_verifier_qa', '1_assign_time_qa',\n",
      "       '1_resolve_time_qa', '1_duration_qa', '1_verify_data_qa',\n",
      "       '1_auditResult_qa', '1_auditStatus_qa', '1_rejectLabel_qa',\n",
      "       'tcs_link_qa', 'is_simulation_qa', 'Unnamed: 17_qa', 'Unnamed: 18_qa',\n",
      "       'Unnamed: 19_qa', 'source_file_qa', 'task_id_mod', 'project_id_mod',\n",
      "       'title_mod', 'status_mod', 'create_time_mod', 'closed_time_mod',\n",
      "       '1_verifier_mod', '1_assign_time_mod', '1_resolve_time_mod',\n",
      "       '1_duration_mod', '1_verify_data_mod', '1_auditResult_mod',\n",
      "       '1_auditStatus_mod', '1_rejectLabel_mod', 'tcs_link_mod',\n",
      "       'is_simulation_mod', 'Unnamed: 17_mod', 'Unnamed: 18_mod',\n",
      "       'Unnamed: 19_mod', 'source_file_mod', 'Unnamed: 16'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "qa_rawdata = merge_csv_files('Product comment report/R2')\n",
    "qa_rawdata = qa_rawdata.drop_duplicates()\n",
    "qa_rawdata['1_resolve_time'] = pd.to_datetime(qa_rawdata['1_resolve_time'].apply(convert_datetime))\n",
    "def filter_time( df,  from_date, to_date):\n",
    "    Final_false_case = df[(df['1_resolve_time'].dt.strftime('%Y-%m-%d') >= from_date) & (df['1_resolve_time'].dt.strftime('%Y-%m-%d') <= to_date)]\n",
    "    return Final_false_case\n",
    "qa_rawdata = filter_time(qa_rawdata,'2023-11-10','2023-12-31')\n",
    "\n",
    "mod_rawdata = merge_csv_files('Product comment report/R1')\n",
    "mod_rawdata = mod_rawdata.drop_duplicates()\n",
    "mod_rawdata['1_resolve_time'] = pd.to_datetime(mod_rawdata['1_resolve_time'].apply(convert_datetime))\n",
    "def filter_time( df,  from_date, to_date):\n",
    "    Final_false_case = df[(df['1_resolve_time'].dt.strftime('%Y-%m-%d') >= from_date) & (df['1_resolve_time'].dt.strftime('%Y-%m-%d') <= to_date)]\n",
    "    return Final_false_case\n",
    "mod_rawdata = filter_time(mod_rawdata,'2023-11-10','2023-12-31')\n",
    "\n",
    "mod_rawdata['title'] = mod_rawdata['title'].replace('VN LL Product Comment Report', 'VN Product Comment Report')\n",
    "\n",
    "merged_df = pd.merge(qa_rawdata, mod_rawdata, how='left', \n",
    "                     left_on=['object_id'], \n",
    "                     right_on=['object_id'],\n",
    "                     suffixes=('_qa', '_mod'))\n",
    "merged_df.dropna(subset='1_rejectLabel_mod',inplace=True)\n",
    "print(merged_df.columns)\n",
    "merged_df[['Moderation Result','Respondent Result']] = merged_df[['1_verify_data_mod','1_verifier_qa']].replace(to_replace=r'^auditStatus: ',value= \"Audit Result:: \",regex=True).astype(str)\n",
    "pcr_result = ['remove', 'normal', 'selfvisible']\n",
    "columns = ['Moderation Result', 'Respondent Result']\n",
    "def match_and_concatenate(cell):\n",
    "    cell = str(cell).lower()\n",
    "    cell = cell.replace('_x000d_', '')\n",
    "    found_words = []\n",
    "    audit_exp = re.search(r'(.*auditreason:)(.*)(\\n|$)', cell)\n",
    "    reject_exp = re.search(r'(.*rejectlabel:)(.*)(\\n|$)', cell)\n",
    "    if audit_exp:\n",
    "        cell_split = audit_exp.group(2).strip()\n",
    "    elif reject_exp:\n",
    "        cell_split = reject_exp.group(2).strip()\n",
    "    else:\n",
    "        cell_split = cell\n",
    "    cell_split = re.sub(r'[\\[\\]\"]', '', cell_split)\n",
    "    matches = re.findall('|'.join(pcr_result), cell_split, re.IGNORECASE)\n",
    "    found_words.extend(matches)\n",
    "    return \",\".join(found_words) if found_words else None\n",
    "for column in ['Moderation Result', 'Respondent Result']:\n",
    "    merged_df.loc[merged_df['title_qa'].isin(['QA VN LL Product Comment', 'QA VN LL Product Comment Report']), column] = merged_df.loc[merged_df['title_qa'].isin(['QA VN LL Product Comment', 'QA VN LL Product Comment Report']), column].apply(match_and_concatenate)\n",
    "merged_df['Mod Result'] = merged_df['1_rejectLabel_mod'].apply(match_and_concatenate)\n",
    "merged_df['BPO QA Result'] = merged_df['1_rejectLabel_qa'].apply(match_and_concatenate)\n",
    "merged_df = merged_df[['title_qa','project_id_qa','1_verifier_mod','1_resolve_time_mod','object_id','Mod Result','1_auditResult_mod','1_duration_mod','1_verifier_qa','1_resolve_time_qa','task_id_qa','tcs_link_qa',\n",
    "                       'BPO QA Result','1_auditResult_qa']].rename(columns={'title_qa':'Sampling Queue','project_id_qa':'Queue ID','1_verifier_mod':'Moderator name','1_resolve_time_mod':'Moderation time','object_id':'Object ID','1_auditResult_mod':'Mod Reason',\n",
    "                                                                           '1_duration_mod':'Mod AHT','1_verifier_qa':'BPO QA','1_resolve_time_qa':'BPO QA Date','task_id_qa':'BPO QA Task ID','tcs_link_qa':'BPO QA Link','1_auditResult_qa':'BPO QA Reason'})\n",
    "merged_df[['Object ID','Queue ID','BPO QA Task ID']] = merged_df[['Object ID','Queue ID','BPO QA Task ID']].replace(to_replace=r'id=',value= '',regex=True).astype(str)\n",
    "Policy_errors_1 = policy_errors_list.copy()\n",
    "Policy_errors_2 = list(map(lambda x: x.replace(\" \", \"_\") + \"0005\",Policy_errors_1))\n",
    "Policy_errors = Policy_errors_1 + Policy_errors_2\n",
    "merged_df['Mod Reason'] = merged_df['Mod Reason'].str.lower()\n",
    "merged_df['BPO QA Reason'] = merged_df['BPO QA Reason'].str.lower()\n",
    "Policy_errors_pattern = \"|\".join(Policy_errors )\n",
    "def pattern_searcher(search_str:str, search_list:str):\n",
    "    search_obj = re.search(search_list, search_str)\n",
    "    if search_obj :\n",
    "        return_str = search_str[search_obj.start(): search_obj.end()]\n",
    "    else:\n",
    "        return_str = np.nan\n",
    "    return return_str\n",
    "merged_df['Mod Reason'] = merged_df['Mod Reason'].astype(str).apply(lambda y: pattern_searcher(search_str=y, search_list=Policy_errors_pattern))\n",
    "merged_df['BPO QA Reason'] = merged_df['BPO QA Reason'].astype(str).apply(lambda z: pattern_searcher(search_str=z, search_list=Policy_errors_pattern))\n",
    "def d(row):\n",
    "    last_index = row.last_valid_index()\n",
    "    return row[last_index] if last_index else np.nan\n",
    "#Final Policy Errors\n",
    "merged_df['Final policy errors'] = merged_df[['Mod Reason','BPO QA Reason']].apply(d, axis=1)\n",
    "#Add diff case\n",
    "merged_df['Diff case'] = np.where(merged_df['Mod Result'] != merged_df['BPO QA Result'], \"FALSE\", \"TRUE\")\n",
    "merged_df['BPO QA Date'] = pd.to_datetime(merged_df['BPO QA Date'])\n",
    "merged_df['Moderation time'] = pd.to_datetime(merged_df['Moderation time'])\n",
    "merged_df['QA Week'] = merged_df['BPO QA Date'].dt.isocalendar().week - (merged_df['BPO QA Date'].dt.weekday == 0).astype(int)\n",
    "merged_df['Mod Week'] = merged_df['Moderation time'].dt.isocalendar().week - (merged_df['Moderation time'].dt.weekday == 0).astype(int)\n",
    "Final_false_case = merged_df[(merged_df['BPO QA Date'] >= merged_df['Moderation time']) &\n",
    "                        (merged_df['QA Week'] >= merged_df['Mod Week'])]\n",
    "Final_false_case = Final_false_case.drop(columns=['QA Week', 'Mod Week'])\n",
    "Final_false_case = Final_false_case.sort_values(by='BPO QA Date', ascending=True).drop_duplicates(subset='BPO QA Task ID', keep='last')\n",
    "def filter_time( df,  from_date, to_date):\n",
    "    Final_false_case = df[(df['BPO QA Date'].dt.strftime('%Y-%m-%d') >= from_date) & (df['BPO QA Date'].dt.strftime('%Y-%m-%d') <= to_date)]\n",
    "    return Final_false_case\n",
    "\n",
    "Regular_diff_pcr = filter_time(Final_false_case, Start_Date_Diff_PCR, End_Date_Diff_PRC)\n",
    "Regular_diff_pcr = Regular_diff_pcr[Regular_diff_pcr['Diff case']=='FALSE']\n",
    "Regular_diff_pcr_P2 = filter_time(Final_false_case, qa_start_date_P2, qa_end_date_P2)\n",
    "Regular_diff_pcr_P2 = Regular_diff_pcr_P2[Regular_diff_pcr_P2['Diff case']=='FALSE']\n",
    "\n",
    "all_sheets_PCR = {'Diff_case_PCR':Regular_diff_pcr}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NON-ANCHOR/LIVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\2689602678.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Moderation time'] = pd.to_datetime(df['Moderation time'].apply(convert_date))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Moderation time</th>\n",
       "      <th>Sampling Queue</th>\n",
       "      <th>Moderator name</th>\n",
       "      <th>No. of Samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-11-26</td>\n",
       "      <td>QA VN LL Live No-Anchor Commercial Content</td>\n",
       "      <td>an.nt@trans-cosmos.com.vn</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-26</td>\n",
       "      <td>QA VN LL Live No-Anchor Commercial Content</td>\n",
       "      <td>duy.nv@trans-cosmos.com.vn</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-11-26</td>\n",
       "      <td>QA VN LL Live No-Anchor Commercial Content</td>\n",
       "      <td>duy.tb@trans-cosmos.com.vn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-11-26</td>\n",
       "      <td>QA VN LL Live No-Anchor Commercial Content</td>\n",
       "      <td>hien.dtt2@trans-cosmos.com.vn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-11-26</td>\n",
       "      <td>QA VN LL Live No-Anchor Commercial Content</td>\n",
       "      <td>khanh.nhk@trans-cosmos.com.vn</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>2023-12-02</td>\n",
       "      <td>QA VN LL No-Anchor Video</td>\n",
       "      <td>truc.th@trans-cosmos.com.vn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>2023-12-02</td>\n",
       "      <td>QA VN LL No-Anchor Video</td>\n",
       "      <td>tu.nd@trans-cosmos.com.vn</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>2023-12-02</td>\n",
       "      <td>QA VN LL No-Anchor Video</td>\n",
       "      <td>tung.lt@trans-cosmos.com.vn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>2023-12-02</td>\n",
       "      <td>QA VN LL No-Anchor Video</td>\n",
       "      <td>xuan.ht2@trans-cosmos.com.vn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>2023-12-02</td>\n",
       "      <td>QA VN LL No-Anchor Video</td>\n",
       "      <td>y.tn2@trans-cosmos.com.vn</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>292 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Moderation time                              Sampling Queue  \\\n",
       "0        2023-11-26  QA VN LL Live No-Anchor Commercial Content   \n",
       "1        2023-11-26  QA VN LL Live No-Anchor Commercial Content   \n",
       "2        2023-11-26  QA VN LL Live No-Anchor Commercial Content   \n",
       "3        2023-11-26  QA VN LL Live No-Anchor Commercial Content   \n",
       "4        2023-11-26  QA VN LL Live No-Anchor Commercial Content   \n",
       "..              ...                                         ...   \n",
       "287      2023-12-02                    QA VN LL No-Anchor Video   \n",
       "288      2023-12-02                    QA VN LL No-Anchor Video   \n",
       "289      2023-12-02                    QA VN LL No-Anchor Video   \n",
       "290      2023-12-02                    QA VN LL No-Anchor Video   \n",
       "291      2023-12-02                    QA VN LL No-Anchor Video   \n",
       "\n",
       "                    Moderator name  No. of Samples  \n",
       "0        an.nt@trans-cosmos.com.vn               3  \n",
       "1       duy.nv@trans-cosmos.com.vn               2  \n",
       "2       duy.tb@trans-cosmos.com.vn               1  \n",
       "3    hien.dtt2@trans-cosmos.com.vn               1  \n",
       "4    khanh.nhk@trans-cosmos.com.vn               3  \n",
       "..                             ...             ...  \n",
       "287    truc.th@trans-cosmos.com.vn               1  \n",
       "288      tu.nd@trans-cosmos.com.vn               2  \n",
       "289    tung.lt@trans-cosmos.com.vn               1  \n",
       "290   xuan.ht2@trans-cosmos.com.vn               1  \n",
       "291      y.tn2@trans-cosmos.com.vn               2  \n",
       "\n",
       "[292 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_rawdata_no_anchor = merge_csv_files('No_Anchor/R2')\n",
    "qa_rawdata_no_anchor = qa_rawdata_no_anchor.drop_duplicates()\n",
    "qa_rawdata_no_anchor['1_resolve_time'] = pd.to_datetime(qa_rawdata_no_anchor['1_resolve_time'].apply(convert_datetime))\n",
    "mod_rawdata_no_anchor = merge_csv_files('No_Anchor/R1')\n",
    "mod_rawdata_no_anchor = mod_rawdata_no_anchor.drop_duplicates()\n",
    "mod_rawdata_no_anchor['1_resolve_time'] = pd.to_datetime(mod_rawdata_no_anchor['1_resolve_time'].apply(convert_datetime))\n",
    "\n",
    "merged_df_no_anchor = pd.merge(qa_rawdata_no_anchor, mod_rawdata_no_anchor, how='left', \n",
    "                     left_on=['object_id'], \n",
    "                     right_on=['object_id'],\n",
    "                     suffixes=('_qa', '_mod'))\n",
    "def get_audit_result(cell):\n",
    "    try:\n",
    "        json_data = json.loads(cell)\n",
    "        for item in json_data.get('mainForm', []):\n",
    "            if 'Audit Result' in item.get('name', ''):\n",
    "                return item.get('value', '').strip()\n",
    "    except:\n",
    "        pass\n",
    "    return ''\n",
    "def check_approval_status(cell, aliases):\n",
    "    for approval_status, substrings in aliases.items():\n",
    "        if any(substring in cell.lower() for substring in substrings):\n",
    "            return approval_status\n",
    "    return ''\n",
    "merged_df_no_anchor['Mod Result'] = merged_df_no_anchor['1_verify_data_mod'].apply(get_audit_result)\n",
    "merged_df_no_anchor['BPO QA Result'] = merged_df_no_anchor['1_verify_data_qa'].apply(get_audit_result)\n",
    "merged_df_no_anchor['Mod Result'] = merged_df_no_anchor['Mod Result'].apply(lambda x: check_approval_status(x, aliases))\n",
    "merged_df_no_anchor['BPO QA Result'] = merged_df_no_anchor['BPO QA Result'].apply(lambda x: check_approval_status(x, aliases))\n",
    "Policy_errors = policy_errors_list.copy()\n",
    "merged_df_no_anchor['Mod Reason'] = merged_df_no_anchor['1_standard_output_mod'].str.lower().replace('_', '')\n",
    "merged_df_no_anchor['BPO QA Reason'] = merged_df_no_anchor['1_standard_output_qa'].str.lower().replace('_', '')\n",
    "def pattern_searcher(search_str:str, search_list:str):\n",
    "    search_obj = re.search(search_list, search_str)\n",
    "    if search_obj :\n",
    "        return_str = search_str[search_obj.start(): search_obj.end()]\n",
    "    else:\n",
    "        return_str = np.nan\n",
    "    return return_str\n",
    "merged_df_no_anchor['Mod Reason'] = merged_df_no_anchor['Mod Reason'].astype(str).apply(lambda y: pattern_searcher(search_str=y, search_list=Policy_errors_pattern))\n",
    "merged_df_no_anchor['BPO QA Reason'] = merged_df_no_anchor['BPO QA Reason'].astype(str).apply(lambda y: pattern_searcher(search_str=y, search_list=Policy_errors_pattern))\n",
    "\n",
    "def d(row):\n",
    "    last_index = row.last_valid_index()\n",
    "    return row[last_index] if last_index else np.nan\n",
    "#Final Policy Errors\n",
    "merged_df_no_anchor['Policy Error'] = merged_df_no_anchor[['Mod Reason','BPO QA Reason']].apply(d, axis=1)\n",
    "#Add diff case\n",
    "merged_df_no_anchor['Diff case'] = np.where(merged_df_no_anchor['Mod Result'] != merged_df_no_anchor['BPO QA Result'], \"FALSE\", \"TRUE\")\n",
    "merged_df_no_anchor['BPO QA Date'] = pd.to_datetime(merged_df_no_anchor['1_resolve_time_qa'].apply(convert_date))\n",
    "merged_df_no_anchor['Moderation time'] = pd.to_datetime(merged_df_no_anchor['1_resolve_time_mod'].apply(convert_date))\n",
    "merged_df_no_anchor['QA Week'] = merged_df_no_anchor['BPO QA Date'].dt.isocalendar().week - (merged_df_no_anchor['BPO QA Date'].dt.weekday == 0).astype(int)\n",
    "merged_df_no_anchor['Mod Week'] = merged_df_no_anchor['Moderation time'].dt.isocalendar().week - (merged_df_no_anchor['Moderation time'].dt.weekday == 0).astype(int)\n",
    "merged_df_no_anchor = merged_df_no_anchor[(merged_df_no_anchor['BPO QA Date'] >= merged_df_no_anchor['Moderation time']) &\n",
    "                        (merged_df_no_anchor['QA Week'] >= merged_df_no_anchor['Mod Week'])]\n",
    "merged_df_no_anchor = merged_df_no_anchor.drop(columns=['QA Week', 'Mod Week'])\n",
    "merged_df_no_anchor = merged_df_no_anchor.sort_values(by='BPO QA Date', ascending=True).drop_duplicates(subset='task_id_qa', keep='last')\n",
    "merged_df_no_anchor = merged_df_no_anchor[['title_qa','project_id_qa','1_verifier_mod','1_resolve_time_mod','object_id','Mod Result','Mod Reason','1_duration_mod','1_verifier_qa','1_resolve_time_qa','task_id_qa','tcs_link_qa','BPO QA Result','BPO QA Reason','Diff case','Policy Error']]\\\n",
    "    .rename(columns={'title_qa':'Sampling Queue',\n",
    "                     'project_id_qa':'Queue ID',\n",
    "                     '1_verifier_mod':'Moderator name',\n",
    "                     '1_resolve_time_mod':'Moderation time',\n",
    "                     'object_id':'Object ID',\n",
    "                     '1_duration_mod':'Mod AHT',\n",
    "                     '1_verifier_qa':'BPO QA',\n",
    "                     '1_resolve_time_qa':'BPO QA Date',\n",
    "                     'task_id_qa':'BPO QA Task ID',\n",
    "                     'tcs_link_qa':'BPO QA Link'})\n",
    "def filter_time( df,  from_date, to_date):\n",
    "    merged_df_no_anchor = df[(df['BPO QA Date'].dt.strftime('%Y-%m-%d') >= from_date) & (df['BPO QA Date'].dt.strftime('%Y-%m-%d') <= to_date)]\n",
    "    return merged_df_no_anchor\n",
    "\n",
    "no_anchor_diff = filter_time(merged_df_no_anchor, Start_Date_Diff_PCR, End_Date_Diff_PRC)\n",
    "no_anchor_diff_P2 = filter_time(merged_df_no_anchor, qa_start_date_P2, qa_end_date_P2)\n",
    "\n",
    "# sample size and diff case\n",
    "def filter_time_2( df, from_date, to_date):\n",
    "    df['Moderation time'] = pd.to_datetime(df['Moderation time'].apply(convert_date))\n",
    "    no_anchor_diff = df[(df['Moderation time'].dt.strftime('%Y-%m-%d') >= from_date) & (df['Moderation time'].dt.strftime('%Y-%m-%d') <= to_date) ]\n",
    "    filer_tcv = no_anchor_diff['Moderator name'].str.contains('@trans-cosmos.com.vn',na=False)\n",
    "    no_anchor_diff = no_anchor_diff[filer_tcv].dropna(axis=0, subset = ['Moderator name'])\n",
    "    return no_anchor_diff\n",
    "no_anchor_diff_samples = filter_time_2(no_anchor_diff, Start_Date_DailyRP_Appeal,End_Date_DailyRP_Appeal)\n",
    "Sample_size_no_anchor = no_anchor_diff_samples.groupby(by=['Moderation time','Sampling Queue','Moderator name'],as_index=False).agg({'BPO QA Task ID':'count'})\n",
    "Sample_size_no_anchor = Sample_size_no_anchor.rename(columns={'1_resolve_time':'Moderation time', \n",
    "                                                              'Sampling Queue':'Sampling Queue', \n",
    "                                                              'Moderator name':'Moderator name', \n",
    "                                                              'BPO QA Task ID':'No. of Samples'})\n",
    "Sample_size_no_anchor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DIFF CASES - ALL QUEUES - OUTPUT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\1404980400.py:124: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'https://tcs-sg.bytelemon.com/worktable/7282116362783261186/?mode=scan&task_ids=7305903631767831046' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[index, 'BPO QA Link'] = bpo_qa_link\n",
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\1404980400.py:124: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'https://tcs-sg.bytelemon.com/worktable/7288255941693243906/?mode=scan&task_ids=7303338801718510081' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[index, 'BPO QA Link'] = bpo_qa_link\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Queue Name\n",
       "QA VN LL No-Anchor Video                       273\n",
       "QA VN LL Live No-Anchor Commercial Content     210\n",
       "QA VN LL Product Initial Review                 32\n",
       "[QA] VN LL Anchor Video 02                      26\n",
       "QA VN LL Live Counterfeit Key Frame             22\n",
       "[QA] VN LL Live 02                              15\n",
       "QA VN LL Product Buffer                         13\n",
       "QA VN LL Anchor Video Counterfeit Key Frame     13\n",
       "QA VN LL Product Comment                         9\n",
       "[QA] VN LL Live Report 02                        9\n",
       "[QA] VN LL Anchor Video High VV 02               9\n",
       "[QA] VN LL Product Counterfeit 02                9\n",
       "QA VN LL Product Comment Report                  8\n",
       "QA [VN]Anchor Video Appeal Audit New             7\n",
       "QA VN Product IPR initial review                 7\n",
       "QA VN LL Shop Decoration Risk Review             6\n",
       "[QA]VN CB Product Buffer                         6\n",
       "[QA] VN LL Product High VV 02                    5\n",
       "QA VN VAT Number Check                           5\n",
       "QA [VN] Shoptab Content cover image              4\n",
       "QA [VN_CB]Shop Product Appeal Audit              4\n",
       "QA [VN]Live Slice Appeal Audit                   4\n",
       "[QA]VN LL Seller On Boarding                     4\n",
       "QA VN LL Product Report 02                       4\n",
       "QA VN LL Trademark New                           3\n",
       "QA [VN]Shop Product Appeal Audit                 3\n",
       "[QA] VN CB Product Initial Review 02             3\n",
       "QA VN LL Shop Logo                               3\n",
       "[QA] VN CB Product Report 02                     3\n",
       "[QA] VN CB Product High VV 02                    2\n",
       "QA [VN][Product Counterfeit]Appeal Audit         1\n",
       "[QA] VN LL Anchor Video Report 02                1\n",
       "QA [UG-2R-GnE] TTS-vi-VN                         1\n",
       "QA VN LL Open Platform                           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rename_columns_regular(df, new_column_names):\n",
    "    df = df.rename(columns=new_column_names)\n",
    "    return df\n",
    "def rename_columns_appeal(df, new_column_names):\n",
    "    df = df.rename(columns=new_column_names)\n",
    "    return df\n",
    "def rename_columns_pcr(df, new_column_names):\n",
    "    df = df.rename(columns=new_column_names)\n",
    "    return df\n",
    "def rename_columns_no_anchor(df, new_column_names):\n",
    "    df = df.rename(columns=new_column_names)\n",
    "    return df\n",
    "\n",
    "regular_column_names = {\n",
    "    'Sampling Queue': 'Queue Name','claimant': 'mod',\n",
    "    'Moderation Time': 'mod_date',\n",
    "    'Object ID': 'object id',\n",
    "    'Mod Result': 'mod_result',\n",
    "    'Mod Reason': 'mod_reason',\n",
    "    '1_duration': 'mod_aht',\n",
    "    'Respondent': 'BPO QA',\n",
    "    'Moderation Time.1': 'BPO QA Date',\n",
    "    'Sampling Task ID': 'BPO QA Task ID',\n",
    "    'QA Result': 'BPO QA Result',\n",
    "    'QA Reason': 'BPO QA Reason',\n",
    "    'Diff case': 'Diff case',\n",
    "    'process result': 'To be appeal (Y/N)',\n",
    "    'process reason': '[TL Remarks]',\n",
    "    'Arbitrator': 'BPO QA Handler',\n",
    "    'Final process result': 'Final Decision',\n",
    "    'Final False Error': 'Final False Error',\n",
    "    'Final policy errors': 'Policy Error',\n",
    "    'RCA lvl1': 'Error Category 1',\n",
    "    'RCA lvl2': 'Error Category 2',\n",
    "    'RCA lvl3': 'Error Category 3',\n",
    "    'process reason.2': 'Remark',\n",
    "    'Final Arbitrator': '[BD] QA Handler',\n",
    "    'Final Result': 'BD QA Approve/ Reject',\n",
    "    'process result.4': 'BD QA Decision',\n",
    "    'process reason.4': '[BD] Remarks'\n",
    "    }\n",
    "appeal_column_names = {\n",
    "    'title':'Queue Name', \n",
    "    'project_id':'Queue ID', \n",
    "    '1_verifier':'mod', \n",
    "    '1_resolve_time':'mod_date', \n",
    "    'object_id':'object id',\n",
    "    'Mod Result':'mod_result', \n",
    "    '1_reason':'mod_reason', \n",
    "    '1_duration':'mod_aht', \n",
    "    '2_verifier':'BPO QA', \n",
    "    '2_resolve_time':'BPO QA Date',\n",
    "    'task_id':'BPO QA Task ID', \n",
    "    'tcs_link':'BPO QA Link', \n",
    "    'QA Result':'BPO QA Result', \n",
    "    '2_reason':'BPO QA Reason', \n",
    "    'Diff case':'Diff case'\n",
    "    }\n",
    "pcr_column_names= {\n",
    "    'Sampling Queue':'Queue Name',                                                           \n",
    "    'Queue ID':'Queue ID', \n",
    "    'Moderator name':'mod', \n",
    "    'Moderation time':'mod_date', \n",
    "    'Object ID':'object id',\n",
    "    'Mod Result':'mod_result', \n",
    "    'Mod Reason':'mod_reason', \n",
    "    'Mod AHT':'mod_aht', \n",
    "    'BPO QA':'BPO QA', \n",
    "    'BPO QA Date':'BPO QA Date',\n",
    "    'BPO QA Task ID':'BPO QA Task ID', \n",
    "    'BPO QA Link':'BPO QA Link', \n",
    "    'BPO QA Result':'BPO QA Result', \n",
    "    'BPO QA Reason':'BPO QA Reason', \n",
    "    'Diff case':'Diff case',\n",
    "    'Final policy errors':'Policy Error'\n",
    "    }\n",
    "no_anchor_column_names= {\n",
    "    'Sampling Queue':'Queue Name',                                                           \n",
    "    'Queue ID':'Queue ID', \n",
    "    'Moderator name':'mod', \n",
    "    'Moderation time':'mod_date', \n",
    "    'Object ID':'object id',\n",
    "    'Mod Result':'mod_result', \n",
    "    'Mod Reason':'mod_reason', \n",
    "    'Mod AHT':'mod_aht', \n",
    "    'BPO QA':'BPO QA', \n",
    "    'BPO QA Date':'BPO QA Date',\n",
    "    'BPO QA Task ID':'BPO QA Task ID', \n",
    "    'BPO QA Link':'BPO QA Link', \n",
    "    'BPO QA Result':'BPO QA Result', \n",
    "    'BPO QA Reason':'BPO QA Reason', \n",
    "    'Diff case':'Diff case',\n",
    "    'Final policy errors':'Policy Error'\n",
    "}\n",
    "\n",
    "Regular_diff = rename_columns_regular(Regular_diff, regular_column_names)\n",
    "Regular_diff_P2 = rename_columns_regular(Regular_diff_P2, regular_column_names)\n",
    "Appeal_diff_case = rename_columns_appeal(Appeal_diff_case, appeal_column_names)\n",
    "Appeal_diff_case_P2 = rename_columns_appeal(Appeal_diff_case_P2, appeal_column_names)\n",
    "Regular_diff_pcr = rename_columns_pcr(Regular_diff_pcr, pcr_column_names)\n",
    "Regular_diff_pcr_P2 = rename_columns_pcr(Regular_diff_pcr_P2, pcr_column_names)\n",
    "no_anchor_diff = rename_columns_no_anchor(no_anchor_diff, no_anchor_column_names)\n",
    "no_anchor_diff_P2 = rename_columns_no_anchor(no_anchor_diff_P2, no_anchor_column_names)\n",
    "\n",
    "queue_list_1 = queue_list[['QA QUEUE NAME','COMPOUND','QA QUEUE ID']].rename(columns={'QA QUEUE NAME':'Queue Name','COMPOUND':'Compound'})\n",
    "\n",
    "### Nối các dataframe\n",
    "rock_diff_case = pd.concat([no_anchor_diff,Appeal_diff_case,Regular_diff_pcr,Regular_diff], axis=0)\n",
    "rock_diff_case = rock_diff_case.drop_duplicates(subset='BPO QA Task ID',keep='last')\n",
    "rock_diff_case_P2 = pd.concat([no_anchor_diff_P2,Appeal_diff_case_P2,Regular_diff_pcr_P2,Regular_diff_P2], axis=0)\n",
    "rock_diff_case_P2 = rock_diff_case_P2.drop_duplicates(subset='BPO QA Task ID',keep='last')\n",
    "\n",
    "###\n",
    "\n",
    "def process_rock_diff_case(df):\n",
    "    df = df.drop(columns=['Queue ID', 'BPO QA Link'])\n",
    "    df = pd.merge(df, queue_list_1, how='left', on='Queue Name')\n",
    "    df = df.rename(columns={'QA QUEUE ID': 'Queue ID'})\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        qa_queue_id = row['Queue ID']\n",
    "        bpo_qa_task_id = row['BPO QA Task ID']\n",
    "        bpo_qa_link = \"https://tcs-sg.bytelemon.com/worktable/\" + str(qa_queue_id) + \"/?mode=scan&task_ids=\" + str(bpo_qa_task_id)\n",
    "        df.at[index, 'BPO QA Link'] = bpo_qa_link\n",
    "    df['mod_date_2'] = pd.to_datetime(df['mod_date'].apply(convert_date))\n",
    "    df = pd.merge(df, data_full_alternation, how='left', left_on=['mod_date_2', 'mod'], right_on=['EffectDate', 'Email'])\n",
    "    df = df.assign(Screenshot=pd.Series())\n",
    "    df = df.assign(Final_False_Error=pd.Series())\n",
    "    df = df.assign(Wrong_Tagging=pd.Series())\n",
    "    df = df[['Compound', 'Queue Name', 'Queue ID', 'mod', 'LineManager', 'mod_date', 'object id', 'mod_result',\n",
    "             'mod_reason', 'mod_aht', 'BPO QA', 'BPO QA Date', 'BPO QA Task ID', 'BPO QA Link',\n",
    "             'BPO QA Result', 'BPO QA Reason', 'Diff case', 'To be appeal (Y/N)',\n",
    "             '[TL Remarks]', 'BPO QA Handler', 'Final Decision', 'Final False Error',\n",
    "             'Policy Error', 'Error Category 1', 'Error Category 2',\n",
    "             'Error Category 3', 'Remark', 'Screenshot', '[BD] QA Handler',\n",
    "             'BD QA Approve/ Reject', 'BD QA Decision', 'Final_False_Error', 'Wrong_Tagging', '[BD] Remarks']]\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "rock_diff_case = process_rock_diff_case(rock_diff_case)\n",
    "rock_diff_case_P2 = process_rock_diff_case(rock_diff_case_P2)\n",
    "rock_diff_case['Queue Name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAMPLES OF ALL QUEUES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\314166423.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  People_Round.rename(columns={'People':'Moderator name'},inplace=True)\n",
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\314166423.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  People_Round[['Moderation time','End date']] = People_Round['Selected duration'].str.split('~', expand=True, n=1)\n",
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\314166423.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  People_Round[['Moderation time','End date']] = People_Round['Selected duration'].str.split('~', expand=True, n=1)\n",
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\314166423.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  People_Round['Moderation time'] = pd.to_datetime(People_Round['Moderation time'], format=\"%Y-%m-%d %H:%M:%S\" ).dt.date\n",
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\314166423.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  People_Round['End date'] = pd.to_datetime(People_Round['End date'], format=\"%Y-%m-%d %H:%M:%S\" ).dt.date\n",
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\314166423.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_size_filter['Moderation time'] = pd.to_datetime(sample_size_filter['Moderation time'])\n"
     ]
    }
   ],
   "source": [
    "People_statistic_append = input_data('People daily')\n",
    "People_statistic_append= People_statistic_append[People_statistic_append['People']!=\"汇总/All\"]\n",
    "People_statistic_append['Export time'].sort_values(ascending=True)\n",
    "People_statistic_append = People_statistic_append.drop_duplicates(subset=['Selected duration', 'Claimant queue', 'Sampling Queue', 'People','No. of Samples'],keep='last')\n",
    "People_Round = People_statistic_append[['Selected duration',\n",
    "                                      'Sampling Queue',\n",
    "                                      'People',\n",
    "                                      'No. of Samples']]\n",
    "People_Round.rename(columns={'People':'Moderator name'},inplace=True)\n",
    "#Split duration\n",
    "People_Round[['Moderation time','End date']] = People_Round['Selected duration'].str.split('~', expand=True, n=1)\n",
    "People_Round['Moderation time'] = pd.to_datetime(People_Round['Moderation time'], format=\"%Y-%m-%d %H:%M:%S\" ).dt.date\n",
    "People_Round['End date'] = pd.to_datetime(People_Round['End date'], format=\"%Y-%m-%d %H:%M:%S\" ).dt.date\n",
    "#Sample size\n",
    "sample_size_filter = People_Round[['Moderation time','Sampling Queue','Moderator name','No. of Samples']]\n",
    "sample_size_filter['Moderation time'] = pd.to_datetime(sample_size_filter['Moderation time'])\n",
    "def filter_time( df, from_date, to_date):\n",
    "    sample_size_filter = df[(df['Moderation time'].dt.strftime('%Y-%m-%d') >= from_date) & (df['Moderation time'].dt.strftime('%Y-%m-%d') <= to_date)]\n",
    "    sample_size_filter = sample_size_filter.groupby(by=['Moderation time','Sampling Queue','Moderator name'], as_index=False).agg({'No. of Samples':'sum'})\n",
    "    sample_size_filter = sample_size_filter.drop_duplicates(subset=['Moderation time', 'Sampling Queue', 'Moderator name'],keep='first')\n",
    "    sample_size_filter = sample_size_filter[['Moderation time','Sampling Queue','Moderator name','No. of Samples']]\n",
    "    return sample_size_filter, from_date, to_date\n",
    "# Fill in moderation time\n",
    "sample_size_filter,from_date, to_date = filter_time(sample_size_filter, Start_Date_Daily_Report_People_Statistic,End_Date_Daily_Report_People_Statistic)\n",
    "# sample_size_filter,from_date, to_date = filter_time(sample_size_filter,'2023-10-26','2023-11-25')\n",
    "\n",
    "all_sheets_regular = {'Sample_size_Regular':sample_size_filter}\n",
    "Sample_size_merge = pd.concat([Sample_size_appeal.reset_index(drop=True), sample_size_filter.reset_index(drop=True),Sample_size_no_anchor.reset_index(drop=True)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\3254305301.py:1: FutureWarning: The provided callable <function sum at 0x0000022CE8B63EC0> is currently using DataFrameGroupBy.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass 'sum' instead.\n",
      "  Sample_size_merge_pv = pd.pivot_table(Sample_size_merge,values='No. of Samples',index=['Sampling Queue'],columns='Moderation time',aggfunc=np.sum)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Moderation time</th>\n",
       "      <th>2023-11-25</th>\n",
       "      <th>2023-11-26</th>\n",
       "      <th>2023-11-27</th>\n",
       "      <th>2023-11-28</th>\n",
       "      <th>2023-11-29</th>\n",
       "      <th>2023-11-30</th>\n",
       "      <th>2023-12-01</th>\n",
       "      <th>2023-12-02</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sampling Queue</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QA VN CB Shop Decoration Risk Review</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA VN CB Trademark New</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA VN LL Anchor Video Counterfeit Key Frame</th>\n",
       "      <td>112.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA VN LL Anchor Video Pirated Content</th>\n",
       "      <td>35.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA VN LL Live Counterfeit Key Frame</th>\n",
       "      <td>433.0</td>\n",
       "      <td>471.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>426.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA VN LL Live No-Anchor Commercial Content</th>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA VN LL Live Pirated Content</th>\n",
       "      <td>112.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA VN LL No-Anchor Video</th>\n",
       "      <td>NaN</td>\n",
       "      <td>41.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA VN LL Open Platform</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA VN LL Product Buffer</th>\n",
       "      <td>114.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA VN LL Product Comment</th>\n",
       "      <td>109.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA VN LL Product Comment Report</th>\n",
       "      <td>116.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA VN LL Product Initial Review</th>\n",
       "      <td>291.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA VN LL Product Report 02</th>\n",
       "      <td>116.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA VN LL Shop Decoration Risk Review</th>\n",
       "      <td>52.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA VN LL Shop Logo</th>\n",
       "      <td>108.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA VN LL Trademark New</th>\n",
       "      <td>113.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA VN Product IPR initial review</th>\n",
       "      <td>53.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA VN VAT Number Check</th>\n",
       "      <td>117.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA [CB][VN]-Shoptab Product Image Dedicated</th>\n",
       "      <td>57.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA [L2L][VN]-Shoptab Product Image Dedicated</th>\n",
       "      <td>54.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA [VN] Appeal Anchor Video Pirated Content</th>\n",
       "      <td>57.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA [VN] Seller Gov X Feige Chat</th>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA [VN] Shoptab Content cover image</th>\n",
       "      <td>86.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA [VN]Anchor Video Appeal Audit New</th>\n",
       "      <td>112.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA [VN]Live Slice Appeal Audit</th>\n",
       "      <td>112.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA [VN]Shop Product Appeal Audit</th>\n",
       "      <td>117.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA [VN]TTSPC Enrollment Review</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA [VN][Live Counterfeit]Appeal Audit</th>\n",
       "      <td>63.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA [VN][Live Pirated]Appeal Audit</th>\n",
       "      <td>81.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA [VN][Product Counterfeit]Appeal Audit</th>\n",
       "      <td>109.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA [VN][Video Counterfeit]Appeal Audit</th>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA [VN_CB]Shop Product Appeal Audit</th>\n",
       "      <td>111.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QA 【UG-2R-GnE】TTS-vi-VN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[QA] VN CB Product High VV 02</th>\n",
       "      <td>117.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[QA] VN CB Product Initial Review 02</th>\n",
       "      <td>112.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[QA] VN CB Product Report 02</th>\n",
       "      <td>30.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[QA] VN LL Anchor Video 02</th>\n",
       "      <td>294.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[QA] VN LL Anchor Video High VV 02</th>\n",
       "      <td>113.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[QA] VN LL Anchor Video Report 02</th>\n",
       "      <td>107.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[QA] VN LL Live 02</th>\n",
       "      <td>113.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[QA] VN LL Live Report 02</th>\n",
       "      <td>114.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[QA] VN LL Product Counterfeit 02</th>\n",
       "      <td>112.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[QA] VN LL Product High VV 02</th>\n",
       "      <td>112.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[QA]VN CB Product Buffer</th>\n",
       "      <td>114.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[QA]VN LL Seller On Boarding</th>\n",
       "      <td>114.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Moderation time                               2023-11-25  2023-11-26  \\\n",
       "Sampling Queue                                                         \n",
       "QA VN CB Shop Decoration Risk Review                 1.0         NaN   \n",
       "QA VN CB Trademark New                               4.0         1.0   \n",
       "QA VN LL Anchor Video Counterfeit Key Frame        112.0       115.0   \n",
       "QA VN LL Anchor Video Pirated Content               35.0        36.0   \n",
       "QA VN LL Live Counterfeit Key Frame                433.0       471.0   \n",
       "QA VN LL Live No-Anchor Commercial Content           NaN        33.0   \n",
       "QA VN LL Live Pirated Content                      112.0       112.0   \n",
       "QA VN LL No-Anchor Video                             NaN        41.0   \n",
       "QA VN LL Open Platform                               NaN         2.0   \n",
       "QA VN LL Product Buffer                            114.0        17.0   \n",
       "QA VN LL Product Comment                           109.0       113.0   \n",
       "QA VN LL Product Comment Report                    116.0       127.0   \n",
       "QA VN LL Product Initial Review                    291.0       296.0   \n",
       "QA VN LL Product Report 02                         116.0       110.0   \n",
       "QA VN LL Shop Decoration Risk Review                52.0        37.0   \n",
       "QA VN LL Shop Logo                                 108.0       117.0   \n",
       "QA VN LL Trademark New                             113.0        42.0   \n",
       "QA VN Product IPR initial review                    53.0        51.0   \n",
       "QA VN VAT Number Check                             117.0       111.0   \n",
       "QA [CB][VN]-Shoptab Product Image Dedicated         57.0        57.0   \n",
       "QA [L2L][VN]-Shoptab Product Image Dedicated        54.0        57.0   \n",
       "QA [VN] Appeal Anchor Video Pirated Content         57.0        58.0   \n",
       "QA [VN] Seller Gov X Feige Chat                     38.0        38.0   \n",
       "QA [VN] Shoptab Content cover image                 86.0        92.0   \n",
       "QA [VN]Anchor Video Appeal Audit New               112.0       109.0   \n",
       "QA [VN]Live Slice Appeal Audit                     112.0       116.0   \n",
       "QA [VN]Shop Product Appeal Audit                   117.0       109.0   \n",
       "QA [VN]TTSPC Enrollment Review                       NaN         NaN   \n",
       "QA [VN][Live Counterfeit]Appeal Audit               63.0        30.0   \n",
       "QA [VN][Live Pirated]Appeal Audit                   81.0        82.0   \n",
       "QA [VN][Product Counterfeit]Appeal Audit           109.0        95.0   \n",
       "QA [VN][Video Counterfeit]Appeal Audit              34.0        25.0   \n",
       "QA [VN_CB]Shop Product Appeal Audit                111.0        11.0   \n",
       "QA 【UG-2R-GnE】TTS-vi-VN                              NaN         NaN   \n",
       "[QA] VN CB Product High VV 02                      117.0       108.0   \n",
       "[QA] VN CB Product Initial Review 02               112.0       114.0   \n",
       "[QA] VN CB Product Report 02                        30.0        28.0   \n",
       "[QA] VN LL Anchor Video 02                         294.0       292.0   \n",
       "[QA] VN LL Anchor Video High VV 02                 113.0       121.0   \n",
       "[QA] VN LL Anchor Video Report 02                  107.0       115.0   \n",
       "[QA] VN LL Live 02                                 113.0       116.0   \n",
       "[QA] VN LL Live Report 02                          114.0       119.0   \n",
       "[QA] VN LL Product Counterfeit 02                  112.0       115.0   \n",
       "[QA] VN LL Product High VV 02                      112.0       116.0   \n",
       "[QA]VN CB Product Buffer                           114.0       114.0   \n",
       "[QA]VN LL Seller On Boarding                       114.0       111.0   \n",
       "\n",
       "Moderation time                               2023-11-27  2023-11-28  \\\n",
       "Sampling Queue                                                         \n",
       "QA VN CB Shop Decoration Risk Review                 3.0         4.0   \n",
       "QA VN CB Trademark New                              16.0         6.0   \n",
       "QA VN LL Anchor Video Counterfeit Key Frame        115.0       114.0   \n",
       "QA VN LL Anchor Video Pirated Content               31.0        35.0   \n",
       "QA VN LL Live Counterfeit Key Frame                444.0       462.0   \n",
       "QA VN LL Live No-Anchor Commercial Content          29.0        29.0   \n",
       "QA VN LL Live Pirated Content                      117.0       112.0   \n",
       "QA VN LL No-Anchor Video                            39.0        40.0   \n",
       "QA VN LL Open Platform                               3.0         2.0   \n",
       "QA VN LL Product Buffer                            121.0       103.0   \n",
       "QA VN LL Product Comment                           114.0       112.0   \n",
       "QA VN LL Product Comment Report                    103.0       116.0   \n",
       "QA VN LL Product Initial Review                    290.0       290.0   \n",
       "QA VN LL Product Report 02                         123.0       108.0   \n",
       "QA VN LL Shop Decoration Risk Review                62.0        68.0   \n",
       "QA VN LL Shop Logo                                 116.0       113.0   \n",
       "QA VN LL Trademark New                             113.0       114.0   \n",
       "QA VN Product IPR initial review                    48.0        53.0   \n",
       "QA VN VAT Number Check                             114.0       112.0   \n",
       "QA [CB][VN]-Shoptab Product Image Dedicated         57.0        64.0   \n",
       "QA [L2L][VN]-Shoptab Product Image Dedicated        57.0        57.0   \n",
       "QA [VN] Appeal Anchor Video Pirated Content         55.0        60.0   \n",
       "QA [VN] Seller Gov X Feige Chat                     38.0        38.0   \n",
       "QA [VN] Shoptab Content cover image                 88.0        96.0   \n",
       "QA [VN]Anchor Video Appeal Audit New               112.0       116.0   \n",
       "QA [VN]Live Slice Appeal Audit                     110.0       118.0   \n",
       "QA [VN]Shop Product Appeal Audit                   114.0       113.0   \n",
       "QA [VN]TTSPC Enrollment Review                       4.0         1.0   \n",
       "QA [VN][Live Counterfeit]Appeal Audit              102.0        42.0   \n",
       "QA [VN][Live Pirated]Appeal Audit                   96.0       151.0   \n",
       "QA [VN][Product Counterfeit]Appeal Audit           120.0       115.0   \n",
       "QA [VN][Video Counterfeit]Appeal Audit              28.0        20.0   \n",
       "QA [VN_CB]Shop Product Appeal Audit                 93.0       120.0   \n",
       "QA 【UG-2R-GnE】TTS-vi-VN                              NaN         NaN   \n",
       "[QA] VN CB Product High VV 02                      123.0       132.0   \n",
       "[QA] VN CB Product Initial Review 02               118.0       110.0   \n",
       "[QA] VN CB Product Report 02                        25.0        30.0   \n",
       "[QA] VN LL Anchor Video 02                         296.0       296.0   \n",
       "[QA] VN LL Anchor Video High VV 02                 104.0       120.0   \n",
       "[QA] VN LL Anchor Video Report 02                  119.0       117.0   \n",
       "[QA] VN LL Live 02                                 112.0       114.0   \n",
       "[QA] VN LL Live Report 02                          107.0       115.0   \n",
       "[QA] VN LL Product Counterfeit 02                  114.0       118.0   \n",
       "[QA] VN LL Product High VV 02                      119.0       110.0   \n",
       "[QA]VN CB Product Buffer                           129.0        99.0   \n",
       "[QA]VN LL Seller On Boarding                       114.0       113.0   \n",
       "\n",
       "Moderation time                               2023-11-29  2023-11-30  \\\n",
       "Sampling Queue                                                         \n",
       "QA VN CB Shop Decoration Risk Review                 6.0         2.0   \n",
       "QA VN CB Trademark New                              14.0        24.0   \n",
       "QA VN LL Anchor Video Counterfeit Key Frame        111.0       118.0   \n",
       "QA VN LL Anchor Video Pirated Content               34.0        33.0   \n",
       "QA VN LL Live Counterfeit Key Frame                440.0       476.0   \n",
       "QA VN LL Live No-Anchor Commercial Content          30.0        30.0   \n",
       "QA VN LL Live Pirated Content                      113.0       117.0   \n",
       "QA VN LL No-Anchor Video                            37.0        40.0   \n",
       "QA VN LL Open Platform                               3.0         3.0   \n",
       "QA VN LL Product Buffer                            114.0       116.0   \n",
       "QA VN LL Product Comment                           118.0       113.0   \n",
       "QA VN LL Product Comment Report                    107.0       116.0   \n",
       "QA VN LL Product Initial Review                    296.0       286.0   \n",
       "QA VN LL Product Report 02                         116.0       116.0   \n",
       "QA VN LL Shop Decoration Risk Review                57.0       116.0   \n",
       "QA VN LL Shop Logo                                 115.0       115.0   \n",
       "QA VN LL Trademark New                             112.0       118.0   \n",
       "QA VN Product IPR initial review                    52.0        52.0   \n",
       "QA VN VAT Number Check                             114.0       117.0   \n",
       "QA [CB][VN]-Shoptab Product Image Dedicated         66.0        41.0   \n",
       "QA [L2L][VN]-Shoptab Product Image Dedicated        58.0        61.0   \n",
       "QA [VN] Appeal Anchor Video Pirated Content         53.0        60.0   \n",
       "QA [VN] Seller Gov X Feige Chat                     38.0        38.0   \n",
       "QA [VN] Shoptab Content cover image                 87.0        90.0   \n",
       "QA [VN]Anchor Video Appeal Audit New               113.0       114.0   \n",
       "QA [VN]Live Slice Appeal Audit                     110.0       109.0   \n",
       "QA [VN]Shop Product Appeal Audit                   115.0       106.0   \n",
       "QA [VN]TTSPC Enrollment Review                       2.0         NaN   \n",
       "QA [VN][Live Counterfeit]Appeal Audit               39.0        38.0   \n",
       "QA [VN][Live Pirated]Appeal Audit                  105.0        87.0   \n",
       "QA [VN][Product Counterfeit]Appeal Audit           117.0        78.0   \n",
       "QA [VN][Video Counterfeit]Appeal Audit              39.0        31.0   \n",
       "QA [VN_CB]Shop Product Appeal Audit                131.0        84.0   \n",
       "QA 【UG-2R-GnE】TTS-vi-VN                              NaN         1.0   \n",
       "[QA] VN CB Product High VV 02                       90.0       148.0   \n",
       "[QA] VN CB Product Initial Review 02               114.0       117.0   \n",
       "[QA] VN CB Product Report 02                        41.0        39.0   \n",
       "[QA] VN LL Anchor Video 02                         290.0       294.0   \n",
       "[QA] VN LL Anchor Video High VV 02                 115.0       106.0   \n",
       "[QA] VN LL Anchor Video Report 02                  105.0       119.0   \n",
       "[QA] VN LL Live 02                                 115.0       108.0   \n",
       "[QA] VN LL Live Report 02                          116.0       121.0   \n",
       "[QA] VN LL Product Counterfeit 02                  115.0       106.0   \n",
       "[QA] VN LL Product High VV 02                      118.0       111.0   \n",
       "[QA]VN CB Product Buffer                           134.0        94.0   \n",
       "[QA]VN LL Seller On Boarding                       117.0       115.0   \n",
       "\n",
       "Moderation time                               2023-12-01  2023-12-02  \n",
       "Sampling Queue                                                        \n",
       "QA VN CB Shop Decoration Risk Review                 5.0         6.0  \n",
       "QA VN CB Trademark New                              34.0        19.0  \n",
       "QA VN LL Anchor Video Counterfeit Key Frame        113.0       111.0  \n",
       "QA VN LL Anchor Video Pirated Content               36.0        34.0  \n",
       "QA VN LL Live Counterfeit Key Frame                451.0       426.0  \n",
       "QA VN LL Live No-Anchor Commercial Content          33.0        26.0  \n",
       "QA VN LL Live Pirated Content                      108.0       113.0  \n",
       "QA VN LL No-Anchor Video                            39.0        37.0  \n",
       "QA VN LL Open Platform                               2.0         NaN  \n",
       "QA VN LL Product Buffer                            112.0       114.0  \n",
       "QA VN LL Product Comment                           120.0       105.0  \n",
       "QA VN LL Product Comment Report                    112.0       108.0  \n",
       "QA VN LL Product Initial Review                    298.0       281.0  \n",
       "QA VN LL Product Report 02                         116.0       106.0  \n",
       "QA VN LL Shop Decoration Risk Review               115.0        68.0  \n",
       "QA VN LL Shop Logo                                 117.0       104.0  \n",
       "QA VN LL Trademark New                             113.0        88.0  \n",
       "QA VN Product IPR initial review                    52.0        52.0  \n",
       "QA VN VAT Number Check                             111.0       115.0  \n",
       "QA [CB][VN]-Shoptab Product Image Dedicated         57.0        57.0  \n",
       "QA [L2L][VN]-Shoptab Product Image Dedicated        52.0        57.0  \n",
       "QA [VN] Appeal Anchor Video Pirated Content         57.0        53.0  \n",
       "QA [VN] Seller Gov X Feige Chat                     38.0         NaN  \n",
       "QA [VN] Shoptab Content cover image                 89.0        89.0  \n",
       "QA [VN]Anchor Video Appeal Audit New               115.0       110.0  \n",
       "QA [VN]Live Slice Appeal Audit                     122.0       104.0  \n",
       "QA [VN]Shop Product Appeal Audit                   123.0       105.0  \n",
       "QA [VN]TTSPC Enrollment Review                       2.0         NaN  \n",
       "QA [VN][Live Counterfeit]Appeal Audit               42.0        28.0  \n",
       "QA [VN][Live Pirated]Appeal Audit                   84.0        53.0  \n",
       "QA [VN][Product Counterfeit]Appeal Audit           124.0       104.0  \n",
       "QA [VN][Video Counterfeit]Appeal Audit              28.0        19.0  \n",
       "QA [VN_CB]Shop Product Appeal Audit                108.0        65.0  \n",
       "QA 【UG-2R-GnE】TTS-vi-VN                              7.0         NaN  \n",
       "[QA] VN CB Product High VV 02                       81.0       110.0  \n",
       "[QA] VN CB Product Initial Review 02               114.0       113.0  \n",
       "[QA] VN CB Product Report 02                        39.0        27.0  \n",
       "[QA] VN LL Anchor Video 02                         297.0       288.0  \n",
       "[QA] VN LL Anchor Video High VV 02                 114.0       114.0  \n",
       "[QA] VN LL Anchor Video Report 02                  113.0       108.0  \n",
       "[QA] VN LL Live 02                                 130.0        97.0  \n",
       "[QA] VN LL Live Report 02                          101.0       116.0  \n",
       "[QA] VN LL Product Counterfeit 02                  114.0       113.0  \n",
       "[QA] VN LL Product High VV 02                      117.0       109.0  \n",
       "[QA]VN CB Product Buffer                           114.0       114.0  \n",
       "[QA]VN LL Seller On Boarding                       113.0       110.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sample_size_merge_pv = pd.pivot_table(Sample_size_merge,values='No. of Samples',index=['Sampling Queue'],columns='Moderation time',aggfunc=np.sum)\n",
    "Sample_size_merge_pv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUEUE PERFORMANCE FOR DAILY REPORT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\544453923.py:27: FutureWarning: The provided callable <function sum at 0x0000022CE8B63EC0> is currently using DataFrameGroupBy.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass 'sum' instead.\n",
      "  pv_queue_performance = pd.pivot_table(queue_performance,values='Accuracy',index=['Compound','Sampling Queue'],columns='Moderation time',aggfunc=np.sum)\n",
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\544453923.py:29: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  pv_queue_performance = pv_queue_performance.applymap(lambda x: '{:.2%}'.format(x) if isinstance(x, float) else x)\n",
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\544453923.py:37: FutureWarning: The provided callable <function sum at 0x0000022CE8B63EC0> is currently using DataFrameGroupBy.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass 'sum' instead.\n",
      "  ste_df_pv = pd.pivot_table(ste_df,values='WTD',index=['Compound','Sampling Queue'],aggfunc=np.sum)\n",
      "C:\\Users\\v6210227\\AppData\\Local\\Temp\\ipykernel_3216\\544453923.py:41: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  queue_performance_final = queue_performance_final.applymap(lambda x: '{:.2%}'.format(x) if isinstance(x, float) else x)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Compound</th>\n",
       "      <th>Sampling Queue</th>\n",
       "      <th>WTD</th>\n",
       "      <th>2023-11-28 00:00:00</th>\n",
       "      <th>2023-11-29 00:00:00</th>\n",
       "      <th>2023-11-30 00:00:00</th>\n",
       "      <th>2023-12-01 00:00:00</th>\n",
       "      <th>2023-12-02 00:00:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anchor Video</td>\n",
       "      <td>QA VN LL Anchor Video Pirated Content</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anchor Video</td>\n",
       "      <td>QA VN LL No-Anchor Video</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anchor Video</td>\n",
       "      <td>[QA] VN LL Anchor Video 02</td>\n",
       "      <td>99.39%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>99.31%</td>\n",
       "      <td>99.32%</td>\n",
       "      <td>98.65%</td>\n",
       "      <td>99.65%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anchor Video</td>\n",
       "      <td>[QA] VN LL Anchor Video High VV 02</td>\n",
       "      <td>99.65%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>98.25%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anchor Video</td>\n",
       "      <td>[QA] VN LL Anchor Video Report 02</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Appeal</td>\n",
       "      <td>QA [VN] Appeal Anchor Video Pirated Content</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Appeal</td>\n",
       "      <td>QA [VN]Anchor Video Appeal Audit New</td>\n",
       "      <td>99.65%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>98.25%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Appeal</td>\n",
       "      <td>QA [VN]Live Slice Appeal Audit</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Appeal</td>\n",
       "      <td>QA [VN]Shop Product Appeal Audit</td>\n",
       "      <td>99.64%</td>\n",
       "      <td>98.23%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Appeal</td>\n",
       "      <td>QA [VN][Live Counterfeit]Appeal Audit</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Appeal</td>\n",
       "      <td>QA [VN][Live Pirated]Appeal Audit</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Appeal</td>\n",
       "      <td>QA [VN][Product Counterfeit]Appeal Audit</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Appeal</td>\n",
       "      <td>QA [VN][Video Counterfeit]Appeal Audit</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Appeal</td>\n",
       "      <td>QA [VN_CB]Shop Product Appeal Audit</td>\n",
       "      <td>99.61%</td>\n",
       "      <td>99.17%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>98.81%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CB Product</td>\n",
       "      <td>QA VN CB Shop Decoration Risk Review</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CB Product</td>\n",
       "      <td>[QA] VN CB Product High VV 02</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CB Product</td>\n",
       "      <td>[QA] VN CB Product Initial Review 02</td>\n",
       "      <td>99.82%</td>\n",
       "      <td>99.09%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CB Product</td>\n",
       "      <td>[QA] VN CB Product Report 02</td>\n",
       "      <td>99.43%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>97.56%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CB Product</td>\n",
       "      <td>[QA]VN CB Product Buffer</td>\n",
       "      <td>99.82%</td>\n",
       "      <td>98.99%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>IPR</td>\n",
       "      <td>QA VN CB Trademark New</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>IPR</td>\n",
       "      <td>QA VN LL Anchor Video Counterfeit Key Frame</td>\n",
       "      <td>99.65%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>99.10%</td>\n",
       "      <td>99.15%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>IPR</td>\n",
       "      <td>QA VN LL Live Counterfeit Key Frame</td>\n",
       "      <td>99.60%</td>\n",
       "      <td>99.78%</td>\n",
       "      <td>98.86%</td>\n",
       "      <td>99.37%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>IPR</td>\n",
       "      <td>QA VN LL Trademark New</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>IPR</td>\n",
       "      <td>QA VN Product IPR initial review</td>\n",
       "      <td>98.85%</td>\n",
       "      <td>98.11%</td>\n",
       "      <td>98.08%</td>\n",
       "      <td>98.08%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>IPR</td>\n",
       "      <td>[QA] VN LL Product Counterfeit 02</td>\n",
       "      <td>99.82%</td>\n",
       "      <td>99.15%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LL Product</td>\n",
       "      <td>QA VN LL Product Buffer</td>\n",
       "      <td>98.75%</td>\n",
       "      <td>99.03%</td>\n",
       "      <td>99.12%</td>\n",
       "      <td>97.41%</td>\n",
       "      <td>99.11%</td>\n",
       "      <td>99.12%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LL Product</td>\n",
       "      <td>QA VN LL Product Comment</td>\n",
       "      <td>99.47%</td>\n",
       "      <td>99.11%</td>\n",
       "      <td>99.15%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>99.17%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LL Product</td>\n",
       "      <td>QA VN LL Product Comment Report</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LL Product</td>\n",
       "      <td>QA VN LL Product Initial Review</td>\n",
       "      <td>98.90%</td>\n",
       "      <td>97.59%</td>\n",
       "      <td>98.65%</td>\n",
       "      <td>99.30%</td>\n",
       "      <td>99.66%</td>\n",
       "      <td>99.29%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LL Product</td>\n",
       "      <td>QA VN LL Product Report 02</td>\n",
       "      <td>99.64%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>98.28%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>LL Product</td>\n",
       "      <td>QA VN LL Shop Decoration Risk Review</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>LL Product</td>\n",
       "      <td>[QA] VN LL Product High VV 02</td>\n",
       "      <td>99.65%</td>\n",
       "      <td>99.09%</td>\n",
       "      <td>99.15%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Livestream</td>\n",
       "      <td>QA VN LL Live No-Anchor Commercial Content</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Livestream</td>\n",
       "      <td>QA VN LL Live Pirated Content</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Livestream</td>\n",
       "      <td>[QA] VN LL Live 02</td>\n",
       "      <td>99.11%</td>\n",
       "      <td>98.25%</td>\n",
       "      <td>97.39%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Livestream</td>\n",
       "      <td>[QA] VN LL Live Report 02</td>\n",
       "      <td>99.12%</td>\n",
       "      <td>98.26%</td>\n",
       "      <td>98.28%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>99.01%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Seller</td>\n",
       "      <td>QA VN LL Open Platform</td>\n",
       "      <td>90.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>50.00%</td>\n",
       "      <td>---</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Seller</td>\n",
       "      <td>QA VN LL Shop Logo</td>\n",
       "      <td>99.65%</td>\n",
       "      <td>99.12%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>99.04%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Seller</td>\n",
       "      <td>QA VN VAT Number Check</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Seller</td>\n",
       "      <td>QA [VN]TTSPC Enrollment Review</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>---</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>---</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Seller</td>\n",
       "      <td>[QA]VN LL Seller On Boarding</td>\n",
       "      <td>99.65%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>98.26%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Side Project</td>\n",
       "      <td>QA [CB][VN]-Shoptab Product Image Dedicated</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Side Project</td>\n",
       "      <td>QA [L2L][VN]-Shoptab Product Image Dedicated</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Side Project</td>\n",
       "      <td>QA [VN] Seller Gov X Feige Chat</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>---</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Side Project</td>\n",
       "      <td>QA [VN] Shoptab Content cover image</td>\n",
       "      <td>99.56%</td>\n",
       "      <td>97.92%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Compound                                Sampling Queue      WTD  \\\n",
       "0   Anchor Video         QA VN LL Anchor Video Pirated Content  100.00%   \n",
       "1   Anchor Video                      QA VN LL No-Anchor Video  100.00%   \n",
       "2   Anchor Video                    [QA] VN LL Anchor Video 02   99.39%   \n",
       "3   Anchor Video            [QA] VN LL Anchor Video High VV 02   99.65%   \n",
       "4   Anchor Video             [QA] VN LL Anchor Video Report 02  100.00%   \n",
       "5         Appeal   QA [VN] Appeal Anchor Video Pirated Content  100.00%   \n",
       "6         Appeal          QA [VN]Anchor Video Appeal Audit New   99.65%   \n",
       "7         Appeal                QA [VN]Live Slice Appeal Audit  100.00%   \n",
       "8         Appeal              QA [VN]Shop Product Appeal Audit   99.64%   \n",
       "9         Appeal         QA [VN][Live Counterfeit]Appeal Audit  100.00%   \n",
       "10        Appeal             QA [VN][Live Pirated]Appeal Audit  100.00%   \n",
       "11        Appeal      QA [VN][Product Counterfeit]Appeal Audit  100.00%   \n",
       "12        Appeal        QA [VN][Video Counterfeit]Appeal Audit  100.00%   \n",
       "13        Appeal           QA [VN_CB]Shop Product Appeal Audit   99.61%   \n",
       "14    CB Product          QA VN CB Shop Decoration Risk Review  100.00%   \n",
       "15    CB Product                 [QA] VN CB Product High VV 02  100.00%   \n",
       "16    CB Product          [QA] VN CB Product Initial Review 02   99.82%   \n",
       "17    CB Product                  [QA] VN CB Product Report 02   99.43%   \n",
       "18    CB Product                      [QA]VN CB Product Buffer   99.82%   \n",
       "19           IPR                        QA VN CB Trademark New  100.00%   \n",
       "20           IPR   QA VN LL Anchor Video Counterfeit Key Frame   99.65%   \n",
       "21           IPR           QA VN LL Live Counterfeit Key Frame   99.60%   \n",
       "22           IPR                        QA VN LL Trademark New  100.00%   \n",
       "23           IPR              QA VN Product IPR initial review   98.85%   \n",
       "24           IPR             [QA] VN LL Product Counterfeit 02   99.82%   \n",
       "25    LL Product                       QA VN LL Product Buffer   98.75%   \n",
       "26    LL Product                      QA VN LL Product Comment   99.47%   \n",
       "27    LL Product               QA VN LL Product Comment Report  100.00%   \n",
       "28    LL Product               QA VN LL Product Initial Review   98.90%   \n",
       "29    LL Product                    QA VN LL Product Report 02   99.64%   \n",
       "30    LL Product          QA VN LL Shop Decoration Risk Review  100.00%   \n",
       "31    LL Product                 [QA] VN LL Product High VV 02   99.65%   \n",
       "32    Livestream    QA VN LL Live No-Anchor Commercial Content  100.00%   \n",
       "33    Livestream                 QA VN LL Live Pirated Content  100.00%   \n",
       "34    Livestream                            [QA] VN LL Live 02   99.11%   \n",
       "35    Livestream                     [QA] VN LL Live Report 02   99.12%   \n",
       "36        Seller                        QA VN LL Open Platform   90.00%   \n",
       "37        Seller                            QA VN LL Shop Logo   99.65%   \n",
       "38        Seller                        QA VN VAT Number Check  100.00%   \n",
       "39        Seller                QA [VN]TTSPC Enrollment Review  100.00%   \n",
       "40        Seller                  [QA]VN LL Seller On Boarding   99.65%   \n",
       "41  Side Project   QA [CB][VN]-Shoptab Product Image Dedicated  100.00%   \n",
       "42  Side Project  QA [L2L][VN]-Shoptab Product Image Dedicated  100.00%   \n",
       "43  Side Project               QA [VN] Seller Gov X Feige Chat  100.00%   \n",
       "44  Side Project           QA [VN] Shoptab Content cover image   99.56%   \n",
       "\n",
       "   2023-11-28 00:00:00 2023-11-29 00:00:00 2023-11-30 00:00:00  \\\n",
       "0              100.00%             100.00%             100.00%   \n",
       "1              100.00%             100.00%             100.00%   \n",
       "2              100.00%              99.31%              99.32%   \n",
       "3              100.00%             100.00%             100.00%   \n",
       "4              100.00%             100.00%             100.00%   \n",
       "5              100.00%             100.00%             100.00%   \n",
       "6              100.00%             100.00%              98.25%   \n",
       "7              100.00%             100.00%             100.00%   \n",
       "8               98.23%             100.00%             100.00%   \n",
       "9              100.00%             100.00%             100.00%   \n",
       "10             100.00%             100.00%             100.00%   \n",
       "11             100.00%             100.00%             100.00%   \n",
       "12             100.00%             100.00%             100.00%   \n",
       "13              99.17%             100.00%              98.81%   \n",
       "14             100.00%             100.00%             100.00%   \n",
       "15             100.00%             100.00%             100.00%   \n",
       "16              99.09%             100.00%             100.00%   \n",
       "17             100.00%              97.56%             100.00%   \n",
       "18              98.99%             100.00%             100.00%   \n",
       "19             100.00%             100.00%             100.00%   \n",
       "20             100.00%              99.10%              99.15%   \n",
       "21              99.78%              98.86%              99.37%   \n",
       "22             100.00%             100.00%             100.00%   \n",
       "23              98.11%              98.08%              98.08%   \n",
       "24              99.15%             100.00%             100.00%   \n",
       "25              99.03%              99.12%              97.41%   \n",
       "26              99.11%              99.15%             100.00%   \n",
       "27             100.00%             100.00%             100.00%   \n",
       "28              97.59%              98.65%              99.30%   \n",
       "29             100.00%             100.00%             100.00%   \n",
       "30             100.00%             100.00%             100.00%   \n",
       "31              99.09%              99.15%             100.00%   \n",
       "32             100.00%             100.00%             100.00%   \n",
       "33             100.00%             100.00%             100.00%   \n",
       "34              98.25%              97.39%             100.00%   \n",
       "35              98.26%              98.28%             100.00%   \n",
       "36             100.00%             100.00%             100.00%   \n",
       "37              99.12%             100.00%             100.00%   \n",
       "38             100.00%             100.00%             100.00%   \n",
       "39             100.00%             100.00%                 ---   \n",
       "40             100.00%             100.00%              98.26%   \n",
       "41             100.00%             100.00%             100.00%   \n",
       "42             100.00%             100.00%             100.00%   \n",
       "43             100.00%             100.00%             100.00%   \n",
       "44              97.92%             100.00%             100.00%   \n",
       "\n",
       "   2023-12-01 00:00:00 2023-12-02 00:00:00  \n",
       "0              100.00%             100.00%  \n",
       "1              100.00%             100.00%  \n",
       "2               98.65%              99.65%  \n",
       "3               98.25%             100.00%  \n",
       "4              100.00%             100.00%  \n",
       "5              100.00%             100.00%  \n",
       "6              100.00%             100.00%  \n",
       "7              100.00%             100.00%  \n",
       "8              100.00%             100.00%  \n",
       "9              100.00%             100.00%  \n",
       "10             100.00%             100.00%  \n",
       "11             100.00%             100.00%  \n",
       "12             100.00%             100.00%  \n",
       "13             100.00%             100.00%  \n",
       "14             100.00%             100.00%  \n",
       "15             100.00%             100.00%  \n",
       "16             100.00%             100.00%  \n",
       "17             100.00%             100.00%  \n",
       "18             100.00%             100.00%  \n",
       "19             100.00%             100.00%  \n",
       "20             100.00%             100.00%  \n",
       "21             100.00%             100.00%  \n",
       "22             100.00%             100.00%  \n",
       "23             100.00%             100.00%  \n",
       "24             100.00%             100.00%  \n",
       "25              99.11%              99.12%  \n",
       "26              99.17%             100.00%  \n",
       "27             100.00%             100.00%  \n",
       "28              99.66%              99.29%  \n",
       "29              98.28%             100.00%  \n",
       "30             100.00%             100.00%  \n",
       "31             100.00%             100.00%  \n",
       "32             100.00%             100.00%  \n",
       "33             100.00%             100.00%  \n",
       "34             100.00%             100.00%  \n",
       "35              99.01%             100.00%  \n",
       "36              50.00%                 ---  \n",
       "37             100.00%              99.04%  \n",
       "38             100.00%             100.00%  \n",
       "39             100.00%                 ---  \n",
       "40             100.00%             100.00%  \n",
       "41             100.00%             100.00%  \n",
       "42             100.00%             100.00%  \n",
       "43             100.00%                 ---  \n",
       "44             100.00%             100.00%  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rock_diff_case_queue = rock_diff_case.copy()\n",
    "rock_diff_case_queue = rock_diff_case_queue[rock_diff_case_queue['Final Decision']==\"Failed appeal\"]\n",
    "rock_diff_case_queue['Moderation time'] = pd.to_datetime(rock_diff_case_queue['mod_date'].apply(convert_date))\n",
    "rock_diff_case_groupby_queue = rock_diff_case_queue.groupby(by=['Moderation time','Queue Name'], as_index=False).agg({'object id':'count'}).rename(columns={'Queue Name':'Sampling Queue','object id':'Mods wrong'})\n",
    "\n",
    "Sample_size_queue = Sample_size_merge.copy()\n",
    "Sample_size_queue['Moderation time'] = pd.to_datetime(Sample_size_queue['Moderation time'].apply(convert_date))\n",
    "sample_size_groupby_queue = Sample_size_queue.groupby(by=['Moderation time','Sampling Queue'], as_index=False).agg({'No. of Samples':'sum'})\n",
    "\n",
    "queue_performance = pd.merge(sample_size_groupby_queue,rock_diff_case_groupby_queue,how='left',on=['Moderation time','Sampling Queue'])\n",
    "queue_performance = queue_performance.fillna(0)\n",
    "queue_performance['Accuracy'] = round((queue_performance['No. of Samples'] - queue_performance['Mods wrong'])/queue_performance['No. of Samples'],4)\n",
    "queue_performance = pd.merge(queue_performance,queue_list_1,how='left',left_on='Sampling Queue',right_on=['Queue Name'])\n",
    "queue_performance = queue_performance.drop(columns={'Queue Name','QA QUEUE ID'})\n",
    "\n",
    "today = datetime.today()\n",
    "end_date = today - timedelta(days=2)\n",
    "days_7_ago = datetime.today() - timedelta(days=7)\n",
    "\n",
    "queue_performance = queue_performance[\n",
    "    (queue_performance['Moderation time'] >= days_7_ago) &\n",
    "    (queue_performance['Moderation time'] <= today)\n",
    "]\n",
    "\n",
    "queue_performance = queue_performance.sort_values(by=['Moderation time'],ascending=True)\n",
    "\n",
    "pv_queue_performance = pd.pivot_table(queue_performance,values='Accuracy',index=['Compound','Sampling Queue'],columns='Moderation time',aggfunc=np.sum)\n",
    "pv_queue_performance = pv_queue_performance.fillna('---')\n",
    "pv_queue_performance = pv_queue_performance.applymap(lambda x: '{:.2%}'.format(x) if isinstance(x, float) else x)\n",
    "pv_queue_performance = pv_queue_performance.reset_index()\n",
    "pv_queue_performance.columns.name = None\n",
    "pv_queue_performance\n",
    "\n",
    "ste_df = pd.DataFrame()\n",
    "ste_df = queue_performance.groupby(by=['Compound','Sampling Queue'], as_index=False).agg({'No. of Samples':'sum','Mods wrong':'sum'})\n",
    "ste_df['WTD'] = round((ste_df['No. of Samples'] - ste_df['Mods wrong']) / ste_df['No. of Samples'],4)\n",
    "ste_df_pv = pd.pivot_table(ste_df,values='WTD',index=['Compound','Sampling Queue'],aggfunc=np.sum)\n",
    "ste_df_pv = ste_df_pv.reset_index()\n",
    "ste_df_pv = ste_df_pv.fillna('---')\n",
    "queue_performance_final = pd.merge(ste_df_pv,pv_queue_performance,how='outer',on=['Compound','Sampling Queue'])\n",
    "queue_performance_final = queue_performance_final.applymap(lambda x: '{:.2%}'.format(x) if isinstance(x, float) else x)\n",
    "queue_performance_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTPUT QA DAILY DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "all_sheets_appeal = {'Diff_case': rock_diff_case,\n",
    "                     'Sample_size': Sample_size_merge,\n",
    "                     'Diff_case_P2': rock_diff_case_P2,\n",
    "                     'Queue Performance':queue_performance_final}\n",
    "\n",
    "writer = pd.ExcelWriter('QA DAILY OUTPUT/QA DAILY OUTPUT.xlsx', engine='xlsxwriter')\n",
    "for sheet_name in all_sheets_appeal.keys():\n",
    "    all_sheets_appeal[sheet_name].to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Push in Lark\n",
    "# import pyperclip\n",
    "\n",
    "# time.sleep(1)\n",
    "# check_png('lark.png',1)\n",
    "# time.sleep(1)\n",
    "# check_png('qa data lark.png',1)\n",
    "# time.sleep(1)\n",
    "# check_png('add folder.png',1)\n",
    "# time.sleep(1)\n",
    "# check_png('local file.png',1)\n",
    "# time.sleep(1)\n",
    "# move(608, 58, 1)\n",
    "# click(608, 58, 1)\n",
    "# pyautogui.press(\"backspace\")\n",
    "# time.sleep(2)\n",
    "# string_to_paste = r\"E:\\tt\\qa_daily_automation\\QA DAILY OUTPUT\"\n",
    "# pyperclip.copy(string_to_paste)\n",
    "# current_position = pyautogui.position()\n",
    "# pyautogui.moveTo(current_position.x, current_position.y, duration=1)\n",
    "# pyautogui.hotkey(\"ctrl\", \"v\")\n",
    "# time.sleep(1)\n",
    "# pyautogui.press(\"enter\")\n",
    "# time.sleep(1)\n",
    "# check_png('date mod.png',1)\n",
    "# time.sleep(1)\n",
    "# pyautogui.hotkey(\"ctrl\", \"a\")\n",
    "# time.sleep(1)\n",
    "# check_png('open from folder.png',1)\n",
    "# time.sleep(1)\n",
    "# check_png('send.png',1)\n",
    "# time.sleep(1)\n",
    "# pyautogui.typewrite(f\"QA Daily Data [{one_day_ago}]\")\n",
    "# pyautogui.hotkey(\"alt\", \"enter\")\n",
    "# pyautogui.hotkey(\"alt\", \"enter\")\n",
    "# pyautogui.typewrite(\"Update Diff Case's latest date on the link below:\")\n",
    "# pyautogui.hotkey(\"alt\", \"enter\")\n",
    "# pyautogui.typewrite(\"https://bytedance.sg.feishu.cn/sheets/shtlgjWvyvxEXFpFICQz51bEH0c?sheet=UMu07M\")\n",
    "# pyautogui.hotkey(\"alt\", \"enter\")\n",
    "# pyautogui.hotkey(\"alt\", \"enter\")\n",
    "# pyautogui.typewrite(\"Update Samples to the latest date on the link below:\")\n",
    "# pyautogui.hotkey(\"alt\", \"enter\")\n",
    "# pyautogui.typewrite(\"https://trans-cosmos.larksuite.com/sheets/Utn2suW2bhH1udthJdrulJ3bsQg?sheet=3a4b97\")\n",
    "# pyautogui.hotkey(\"alt\", \"enter\")\n",
    "# pyautogui.typewrite(\"Note: updated samples within the last 8 days.\")\n",
    "# pyautogui.hotkey(\"alt\", \"enter\")\n",
    "# time.sleep(2)\n",
    "# pyautogui.press(\"enter\")\n",
    "# check_png('cb_schedule_task.png',1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
