{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "import openpyxl\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import joblib\n",
    "import xlsxwriter\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331\n"
     ]
    }
   ],
   "source": [
    "def convert_to_number(string):\n",
    "    # Định nghĩa độ ưu tiên cho từng giá trị\n",
    "    priority = {\n",
    "        \"remove\": 3,\n",
    "        \"selfvisible\": 2,\n",
    "        \"normal\": 1\n",
    "    }\n",
    "\n",
    "    # Phân tách chuỗi thành danh sách các giá trị\n",
    "    string_list = string.split(\",\")\n",
    "\n",
    "    # Chuyển đổi giá trị thành số và ghép chúng lại\n",
    "    number = \"\"\n",
    "    for value in string_list:\n",
    "        number += str(priority[value])\n",
    "\n",
    "    return int(number)\n",
    "\n",
    "string = \"remove,remove,normal\"\n",
    "number = convert_to_number(string)\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đổi tên file D:/Download daily files\\tcs_export_7266693133944062465_1 - Sheet1.csv thành [VN] OEC Product Model Recall Review_2023-08-06_2023-08-12.csv\n",
      "Đổi tên file D:/Download daily files\\tcs_export_7266693133944062465_2 - Sheet1.csv thành [VN] OEC Product Model Recall Review_2023-08-12_2023-08-12.csv\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'D:/Download daily files\\\\tcs_export_7266693133944062465_3 - Sheet1.csv' -> 'D:/Download daily files\\\\[VN] OEC Product Model Recall Review_2023-08-12_2023-08-12.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39m# Sử dụng hàm rename_csv_files để đổi tên file CSV trong một thư mục\u001b[39;00m\n\u001b[0;32m     27\u001b[0m folder_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mD:/Download daily files\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 28\u001b[0m rename_csv_files(folder_path)\n",
      "Cell \u001b[1;32mIn[9], line 21\u001b[0m, in \u001b[0;36mrename_csv_files\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     18\u001b[0m     new_file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_path, new_file_name)\n\u001b[0;32m     20\u001b[0m     \u001b[39m# Đổi tên file\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     os\u001b[39m.\u001b[39;49mrename(file, new_file_path)\n\u001b[0;32m     22\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mĐổi tên file \u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m thành \u001b[39m\u001b[39m{\u001b[39;00mnew_file_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'D:/Download daily files\\\\tcs_export_7266693133944062465_3 - Sheet1.csv' -> 'D:/Download daily files\\\\[VN] OEC Product Model Recall Review_2023-08-12_2023-08-12.csv'"
     ]
    }
   ],
   "source": [
    "def rename_csv_files(folder_path):\n",
    "    # Lấy đường dẫn của tất cả các file CSV trong thư mục\n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "    for file in csv_files:\n",
    "        # Đọc dữ liệu từ file CSV\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Kiểm tra xem cột \"title\" và \"1_resolve_time\" có tồn tại trong dataframe không\n",
    "        if \"title\" in df.columns and \"1_resolve_time\" in df.columns:\n",
    "            # Lấy giá trị trong cột \"title\" và \"1_resolve_time\"\n",
    "            title = df[\"title\"].iloc[0]\n",
    "            start_date = df[\"1_resolve_time\"].iloc[0].split()[0]\n",
    "            end_date = df[\"1_resolve_time\"].iloc[-1].split()[0]\n",
    "            \n",
    "            # Tạo tên file mới\n",
    "            new_file_name = f\"{title}_{start_date}_{end_date}.csv\"\n",
    "            new_file_path = os.path.join(folder_path, new_file_name)\n",
    "\n",
    "            # Đổi tên file\n",
    "            os.rename(file, new_file_path)\n",
    "            print(f\"Đổi tên file {file} thành {new_file_name}\")\n",
    "        else:\n",
    "            print(f\"Không tìm thấy cột 'title' hoặc '1_resolve_time' trong file {file}.\")\n",
    "\n",
    "# Sử dụng hàm rename_csv_files để đổi tên file CSV trong một thư mục\n",
    "folder_path = \"D:/Download daily files\"\n",
    "rename_csv_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Đọc file excel\n",
    "# df_xls = pd.read_excel('D:/Data clean/Data set - Side project.xlsx', sheet_name=None)\n",
    "\n",
    "# # Lặp qua từng sheet trong file excel\n",
    "# for sheet_name, sheet_df in df_xls.items():\n",
    "#     # Lưu sheet_df thành file excel riêng biệt với tên file là tên sheet\n",
    "#     sheet_df.to_csv(f'D:/Data clean/All Queues - Diff case Rawdata/Side_project/{sheet_name}.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Week\n",
    "# week_list = pd.read_excel('D:/Code/Code/Support_dashboard.xlsx',sheet_name='Week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_datetime(a):\n",
    "#     my_format=\"%Y-%m-%d\"\n",
    "#     try:\n",
    "#         try:\n",
    "#             try:\n",
    "#                 return datetime.strptime(str(a), \"%Y-%m-%d %H:%M:%S\").strftime(my_format)\n",
    "#             except:\n",
    "#                 return datetime.strptime(str(a), \"%m/%d/%Y %H:%M\").strftime(my_format)\n",
    "#         except:\n",
    "#             return datetime.strptime(str(a), \"%m/%d/%Y\").strftime(my_format)\n",
    "#     except:\n",
    "#         return datetime.strptime(str(a), \"%Y-%m-%d\").strftime(my_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def input_data( data_dir  ):\n",
    "#     data_dir = data_dir\n",
    "#     out_dir = '.'\n",
    "\n",
    "#     list_files = []\n",
    "#     for filename in pathlib.Path(data_dir).glob('**/*.xlsx'):\n",
    "\n",
    "#         df=pd.concat(pd.read_excel(filename,sheet_name=None, skiprows=0))\n",
    "#         list_files.append(df)\n",
    "\n",
    "#         # file_name = os.path.basename(filename)\n",
    "#         # week = file_name.split(' (')[0]\n",
    "#         # week += f' ({file_name.split(\"(\")[1].split(\")\")[0]})'\n",
    "    \n",
    "#         # df['Week'] = week\n",
    "\n",
    "#     df1 = pd.concat(list_files,   ignore_index=True)\n",
    "    \n",
    "#     return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_W28 = input_data('D:/Download daily files/')\n",
    "# test_W28= test_W28[test_W28['People']!=\"汇总/All\"]\n",
    "# test_W28 = test_W28[['Selected duration',\n",
    "#                                       'Sampling Queue',\n",
    "#                                       'People',\n",
    "#                                       'No. of Samples']]\n",
    "# test_W28.rename(columns={'People':'Moderator name'},inplace=True)\n",
    "# #Split duration\n",
    "# test_W28[['Moderation time','End date']] = test_W28['Selected duration'].str.split('~', expand=True, n=1)\n",
    "# test_W28['Moderation time'] = pd.to_datetime(test_W28['Moderation time'], format=\"%Y-%m-%d %H:%M:%S\" ).dt.date #%H:%M:%S\n",
    "# test_W28['End date'] = pd.to_datetime(test_W28['End date'], format=\"%Y-%m-%d %H:%M:%S\" ).dt.date #%H:%M:%\n",
    "\n",
    "# test_W28.drop_duplicates()\n",
    "\n",
    "# test_W28.to_excel('D:/Export temp/W28.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1_W8 = input_data('D:/W1_W16')\n",
    "# W1_W8['Moderators Name'] = W1_W8['Moderators Name'].fillna('')\n",
    "# W1_W8['Moderators Name'] = W1_W8['Moderators Name'].str.replace('robot_estimate@','',regex=True)\n",
    "# W1_W8 = W1_W8[W1_W8['Moderators Name'].str.contains('@trans-cosmos.com.vn')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1_W8['BD QA Decision'] = W1_W8['BD QA Decision'].replace('Mods Wrong', 'Failed appeal')\n",
    "# W1_W8['BD QA Decision'] = W1_W8['BD QA Decision'].replace('QAs Wrong', 'Appeal successfully')\n",
    "# W1_W8['BD QA Decision'] = W1_W8['BD QA Decision'].replace('Borderline', 'Edge case')\n",
    "# W1_W8['BD QA Decision'] = W1_W8['BD QA Decision'].replace('Mods & QAs Wrong', 'Failed appeal')\n",
    "# W1_W8['BD QA Decision'] = W1_W8['BD QA Decision'].replace('Pending ', np.nan)\n",
    "\n",
    "# W1_W8['[BPO QA] Decision'] = W1_W8['[BPO QA] Decision'].replace('Mods Wrong', 'Failed appeal')\n",
    "# W1_W8['[BPO QA] Decision'] = W1_W8['[BPO QA] Decision'].replace(['QAs Wrong','QA Wrong'], 'Appeal successfully')\n",
    "# W1_W8['[BPO QA] Decision'] = W1_W8['[BPO QA] Decision'].replace('Borderline', 'Edge case')\n",
    "# W1_W8['[BPO QA] Decision'] = W1_W8['[BPO QA] Decision'].replace('Mods & QAs Wrong', 'Failed appeal')\n",
    "# W1_W8['[BPO QA] Decision'] = W1_W8['[BPO QA] Decision'].replace(['Pending ','QAs Checking','Mods Checking','Pending','QA Checking', datetime.time(0, 0)], np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1_W8['Final Decision'] = W1_W8['BD QA Decision']\n",
    "# W1_W8.loc[W1_W8['Final Decision'].isnull(), 'Final Decision'] = W1_W8.loc[W1_W8['Final Decision'].isnull(), '[BPO QA] Decision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1_W8['Week'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diff_case_W01_W08 = W1_W8[['Week','Queue Name','Task ID','Moderators Name','QA Date','BPO QA name',\n",
    "#                                                'Error Category 1','Error Category 1', 'Error Category 2', 'Error Category 3','Final Decision']]\n",
    "\n",
    "\n",
    "# Diff_case_W01_W08['QA Date'] = pd.to_datetime(Diff_case_W01_W08['QA Date']).dt.strftime(\"%Y-%m-%d\")\n",
    "# Diff_case_W01_W08['QA Date'] = pd.to_datetime(Diff_case_W01_W08['QA Date'])\n",
    "\n",
    "\n",
    "# #sample - Shop UPL \n",
    "# W01_W08_sample_size = Diff_case_W01_W08.groupby(['Week','QA Date','Queue Name','Moderators Name'],as_index=False)['Task ID'].apply(lambda y: y[y != ''].count())\n",
    "# W01_W08_sample_size = W01_W08_sample_size[['Week','QA Date','Queue Name','Moderators Name','Task ID']].rename(columns={'QA Date':'Date',\n",
    "#                                                                                     'Queue Name':'QA QUEUE NAME',\n",
    "#                                                                                     'Moderators Name':'Moderator',\n",
    "#                                                                                     'Task ID':'Sample size'})\n",
    "\n",
    "# #diff case - Shop UPL \n",
    "# W01_W08_mod_wrong = Diff_case_W01_W08.groupby(['Week','QA Date','Queue Name','Moderators Name'],as_index=False)['Final Decision'].apply(lambda y: y[y ==\"Failed appeal\"].count())\n",
    "\n",
    "# W01_W08_mod_wrong.loc[W01_W08_mod_wrong['Queue Name']!='','Team']=\"E-UPL\"\n",
    "# W01_W08_mod_wrong = W01_W08_mod_wrong[['Week','QA Date','Queue Name','Moderators Name','Final Decision']].rename(columns={'QA Date':'Date',\n",
    "#                                                                                     'Queue Name':'QA QUEUE NAME',\n",
    "#                                                                                     'Moderators Name':'Moderator',\n",
    "#                                                                                     'Final Decision':'Mods Wrong'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #MOD - Merge sample size & diff case\n",
    "# merged = pd.merge(W01_W08_sample_size, W01_W08_mod_wrong,how='left',left_on=['Week','Date','QA QUEUE NAME','Moderator'],right_on=['Week','Date','QA QUEUE NAME','Moderator'])\n",
    "\n",
    "# accr_tracker_temp = merged[['Week','Date','QA QUEUE NAME','Moderator','Sample size','Mods Wrong']]\n",
    "# accr_tracker_temp = accr_tracker_temp.groupby(by=['Week','Date','QA QUEUE NAME','Moderator'],as_index=False).agg({'Sample size':'sum','Mods Wrong':'sum'})\n",
    "# accr_tracker_temp = accr_tracker_temp.sort_values('Date',ascending=True)\n",
    "# #accr_tracker_temp.to_excel('D:/Export temp/ACCR W1-W08.xlsx')\n",
    "# accr_tracker_temp = accr_tracker_temp.groupby(by=['Week','Date','Moderator'],as_index=False).agg({'Sample size':'sum','Mods Wrong':'sum'})\n",
    "# data_full_alternation = pd.read_excel('D:/Code/Code/linemanager_full.xlsx')\n",
    "# data_full_alternation = data_full_alternation[['EffectDate','Email','LineManager']]\n",
    "# accr_tracker_temp = pd.merge(accr_tracker_temp,data_full_alternation,how='left',left_on=['Date','Moderator'],right_on=['EffectDate','Email'])\n",
    "# accr_tracker_temp[['LineManager','Week', 'Date', 'Moderator', 'Sample size','Mods Wrong']].to_excel('D:/Export temp/W1_W8.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "\n",
    "# import datetime\n",
    "\n",
    "# def convert_datetime(a):\n",
    "#     date_formats = [\"%Y-%m-%d %H:%M:%S.%f\", \"%Y-%m-%d %H:%M:%S\", \"%m/%d/%Y %H:%M\", \"%m/%d/%Y\", \"%Y-%m-%d\"]\n",
    "#     # Try all way to convert a to a datetime object\n",
    "#     for fmt in date_formats:\n",
    "#         try:\n",
    "#             return datetime.datetime.strptime(str(a), fmt).strftime(\"%Y-%m-%d\")\n",
    "#         except ValueError:\n",
    "#             pass\n",
    "#     # If all trials failed, Assuming Excel's numerical date format (Days since 1900-01-01).\n",
    "#     try:\n",
    "#         if '.' in str(a):\n",
    "#             a = float(a)\n",
    "#             return (datetime.datetime(1899, 12, 30) + datetime.timedelta(days=a)).strftime(\"%Y-%m-%d\")\n",
    "#     except ValueError:\n",
    "#         pass\n",
    "#     # Return a as is if no conversion was possible\n",
    "#     return str(a)\n",
    "\n",
    "# def merge_export_xlsx_files(path, output_path):\n",
    "#     all_files = [file for file in os.listdir(path) if file.endswith(\".xlsx\")]\n",
    "\n",
    "#     list_data = []\n",
    "\n",
    "#     for file in all_files:\n",
    "#         try:\n",
    "#             df = pd.read_excel(path + \"/\" + file)\n",
    "            \n",
    "#             df[\"Week\"] = os.path.splitext(file)[0]\n",
    "#             df['Moderation Date'] = df['Moderation Date'].apply(convert_datetime)\n",
    "#             df['QA Date'] = df['QA Date'].apply(convert_datetime)\n",
    "\n",
    "#             list_data.append(df)\n",
    "#         except Exception as e:\n",
    "#             print(f'File {file} has error: {e}')\n",
    "#             pass\n",
    "\n",
    "#     all_data = pd.concat(list_data, ignore_index=True)\n",
    "\n",
    "#     for week in all_data['Week'].unique():\n",
    "#         all_data[all_data['Week'] == week].to_csv(output_path + \"/\" + week + \".csv\", index=False)\n",
    "\n",
    "\n",
    "# #merge_export_xlsx_files('D:/MOD Performance/Queues/To be QA (backup)', 'D:/Data clean/All Queues - Diff case Rawdata/Diff case')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_csv_files(folder_path):\n",
    "#     file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "#     dfs = []\n",
    "#     for file_path in file_paths:\n",
    "#         try:\n",
    "#             df = pd.read_csv(file_path)\n",
    "#             df['source_file'] = os.path.basename(file_path)\n",
    "\n",
    "#             dfs.append(df)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "  \n",
    "#     merged_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "#     return merged_df\n",
    "\n",
    "# total_sample_W1_W19 = merge_csv_files('D:/Data clean/All Queues - Diff case Rawdata/Diff case')\n",
    "# total_sample_W1_W19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Thư mục chứa các file excel cần nối\n",
    "# dir_path = 'D:/Data clean/OEC and Seller GOV chat/'\n",
    "\n",
    "# # Lặp qua từng file trong thư mục\n",
    "# file_dict = {}\n",
    "# for filename in os.listdir(dir_path):\n",
    "#     if filename.endswith('.xlsx'):\n",
    "#         # Lấy hai ký tự cuối cùng trong tên file\n",
    "#         last_two_chars = filename[-6:-4]\n",
    "        \n",
    "#         # Thêm file name vào từ điển, value là list chứa tên file và hai ký tự cuối cùng của file\n",
    "#         if last_two_chars in file_dict:\n",
    "#             file_dict[last_two_chars].append([filename, last_two_chars])\n",
    "#         else:\n",
    "#             file_dict[last_two_chars] = [[filename, last_two_chars]]\n",
    "            \n",
    "# # Lặp qua từng cặp file có cùng hai ký tự cuối cùng\n",
    "# for key, value in file_dict.items():\n",
    "#     if len(value) == 2:\n",
    "#         df1 = pd.read_excel(os.path.join(dir_path, value[0][0]), engine='openpyxl')\n",
    "#         df2 = pd.read_excel(os.path.join(dir_path, value[1][0]), engine='openpyxl')\n",
    "#         combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "        \n",
    "#         # Ghi kết quả vào file mới\n",
    "#         combined_df.to_excel(os.path.join(dir_path, f\"{value[0][1]}_combined.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Thư mục chứa các file excel cần nối\n",
    "# dir_path = 'D:/Data clean/OEC and Seller GOV chat'\n",
    "\n",
    "# # Lặp qua từng file trong thư mục và đọc vào DataFrame\n",
    "# dfs = []\n",
    "# for file in os.listdir(dir_path):\n",
    "#     if file.endswith('.xlsx'):\n",
    "#         file_path = os.path.join(dir_path, file)\n",
    "#         df = pd.read_excel(file_path, engine='openpyxl',dtype={'Queue ID':object,'object id':object,'BPO Task ID':object})\n",
    "#         dfs.append(df)\n",
    "\n",
    "# # Nối các DataFrame lại với nhau\n",
    "# combined_df = pd.concat(dfs, ignore_index=True)\n",
    "# combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read_Press_to_link = pd.ExcelFile('D:/Code/Code/Support_dashboard.xlsx')\n",
    "# Press_to_link = pd.read_excel(Read_Press_to_link,sheet_name='Links',dtype={'Queue ID':object},keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def input_data( data_dir  ):\n",
    "#     data_dir = data_dir\n",
    "#     out_dir = '.'\n",
    "\n",
    "#     list_files = []\n",
    "#     for filename in pathlib.Path(data_dir).glob('**/*.xlsx'):\n",
    "\n",
    "#         df=pd.concat(pd.read_excel(filename,sheet_name=None, skiprows=0))\n",
    "#         list_files.append(df)\n",
    "\n",
    "#     df1 = pd.concat(list_files,   ignore_index=True)\n",
    "    \n",
    "#     return df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# People_statistic_append = input_data('D:/Download daily files')\n",
    "# People_statistic_append.to_excel('ANCHOR.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
